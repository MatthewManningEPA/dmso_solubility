Index: dmso_model_dev/models/feature_selector.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import builtins\r\nimport copy\r\nimport dataclasses\r\nimport os\r\nimport pprint\r\nimport itertools\r\nimport prettyprint\r\nimport functools\r\nimport logging\r\nimport typing\r\nfrom typing import Any, Set\r\nfrom collections import namedtuple\r\nimport cachetools\r\nimport numpy as np\r\nimport pandas as pd\r\nimport sklearn.utils.validation\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.linear_model import LassoCV, ElasticNetCV\r\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest, SequentialFeatureSelector, SelectFromModel, \\\r\n    VarianceThreshold\r\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, roc_curve, classification_report, \\\r\n    matthews_corrcoef\r\nfrom sklearn.feature_selection import f_classif, chi2, mutual_info_classif\r\nfrom sklearn.ensemble import RandomTreesEmbedding, RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\r\nfrom sklearn.preprocessing import RobustScaler, Normalizer\r\nfrom sklearn.base import TransformerMixin, OneToOneFeatureMixin\r\nfrom sklearn.utils import check_X_y\r\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\r\nfrom dmso_model_dev.data_handling.padel_categorization import load_padel_df\r\nfrom dmso_model_dev.data_handling import FeatureAccessor\r\n\r\nPADEL_GROUPS_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/padel/padel_desc_groups.csv\"\r\nGROUP_COL_NAMES = ('Type', 'Number', 'Descriptor', 'Class')\r\nWEIGHT_NAMES = ('mass', 'charges', 'van der Waals', 'Sanderson', 'electronegativites', 'polarizabilities', 'ionization', 'I-state', )\r\n# Model running options.\r\nlinear_dim_reduction = False\r\nplot_tsne = False\r\nplot_logreg = True\r\nforest = True\r\n'''\r\n@dataclasses.dataclass\r\nclass TrainingOpts:\r\n    run_debug: bool\r\n    corr_meth: str\r\n    sample_wts: pd.Series\r\n    pjobs: int\r\n    data_dir: str | os.PathLike\r\n    stats_dir: str | os.PathLike\r\n'''\r\ndef make_training_opts(opts):\r\n    options = dataclasses.make_dataclass('options', ((k, type(v)) for k, v in opts.items()))(**opts)\r\n    options.__module__ = __name__\r\n    return options\r\n\r\n# TODO: Check assumptions/requirements of estimators.\r\nclass FeatureSelectorTrainer:\r\n    _group_names = ('zero_var', 'nonzero_var', 'sparse', 'dense', 'discrete', 'cont', 'nonneg', 'neg')\r\n\r\n    def __init__(self, data, labels=None, fs=None, options=None, logger=None, subsets=None, *args, **kwargs):\r\n        self.options = self.set_options(options, log_opts=logger)\r\n        if self.options.run_debug:\r\n            self.X = FeatureFrame(df=data.sample(n=2500, axis='index').sample(n=100, axis='columns').copy(deep=True), options=self.options)\r\n            self.options.sample_wts = self.options.sample_wts[self.X.feat_frame.index]\r\n        elif fs and type(fs) is FeatureFrame:\r\n            self.X = fs\r\n        else:\r\n            self._X = FeatureFrame(df=data.copy(deep=True), options=self.options)\r\n        self._y = labels\r\n        self._corr = None\r\n        self._corr_mat = None\r\n        self._condition_num = None\r\n        self._descriptor_groups_df = None\r\n        self._calc_values()\r\n        # self.feature_properties = dict()\r\n        # self.feat_scorer_types = dict()\r\n        self._scalers = dict()\r\n        # self._get_desc_groups(grouping_dir=PADEL_GROUPS_DIR, cols=GROUP_COL_NAMES)\r\n        # x_sample = train_X.sample(axis=1, frac=0.2, random_state=0)\r\n        if not debug:\r\n            if len(self.X.sparse) < MIN_NUM_FEATS['sparse']:\r\n                logger.warning('Too few sparse features: \\n{}'.format(self.X.sparse))\r\n                raise ValueError\r\n            if len(self.X.dense) < MIN_NUM_FEATS['dense']:\r\n                logger.warning('Too few dense features: \\n{}'.format(self.X.dense))\r\n                raise ValueError\r\n        cweights = self.y.copy().replace({1: 1, 0: 10})\r\n        # self.__initialize_data_history__()\r\n\r\n    '''\r\n    def __initialize_data_history__(self):\r\n        for a in self.data_dict:\r\n            self.data_history[a.__name] = [a]\r\n    '''\r\n\r\n    # Set default values for options named tuple.\r\n    def set_options(self, f_opts, log_opts=None):\r\n        global logger\r\n        logging.basicConfig(**log_opts)\r\n        logger = logging.getLogger(name='logger')\r\n        if f_opts['run_debug']:\r\n            logger.setLevel(logging.DEBUG)\r\n        else:\r\n            logger.setLevel(logging.INFO)\r\n        dc = make_training_opts(f_opts)\r\n        return dc\r\n\r\n    def _calc_values(self):\r\n        self._corr = self.X.feat_frame[self.X.nonzero_vars].corrwith(other=self.y, axis='columns',method=self.options.corr_meth)\r\n        # self._set_descriptor_groups_df()\r\n\r\n    def check_feat_num(self):\r\n        for k, v in self.options.asdict().items():\r\n            if 'min' in k and 'feats' in k and len(X.sparse.tolist()) < v:\r\n                logger.error('Number of {} features is smaller than minimum of {}'.format(k, v))\r\n\r\n    @property\r\n    def corr_mat(self):\r\n        return self._corr_mat\r\n\r\n\r\n    # aweights is used to balance covariance estimates for different subsets of observations.\r\n    def calc_cov_mat(self, calc_cov=None, filter_nan=False, sample_wts='auto', *args, **kwargs):\r\n        if calc_cov is not None:\r\n            self._corr_mat = calc_cov\r\n        cov_path = '{}cov.pkl'.format(self.options.stats_dir)\r\n        if type(self.corr_mat) is pd.DataFrame and not self.corr_mat.empty and self.corr_mat.shape[1] == self.X.feat_frame.shape[1]:\r\n            logger.info('Covariance matrix has already been calculated.')\r\n            return self.corr_mat\r\n        if sample_wts == 'auto':\r\n            sample_wts = self.options.sample_wts\r\n        # _corr = self.X.feat_frame[self.X.dense].corr(method=self.options.corr_meth)\r\n        dense_ind = self.X.feat_frame.index.intersection(self.X.dense)\r\n        # print(self.X.feat_frame[self.X.dense].shape, sample_wts.shape)\r\n        _corr = pd.DataFrame(np.cov(self.X.feat_frame, rowvar=False, aweights=sample_wts, ddof=0), index=self.X.feat_frame.columns\r\n                             , columns=self.X.feat_frame.columns)\r\n        assert _corr is not None\r\n        logging.info('Covariance Matrix:\\n{}'.format(_corr))\r\n        logging.info('CoV NaNs: {}'.format(_corr.isna().astype(int).sum().sum()))\r\n        if filter_nan:\r\n            # Remove features with no non-NaN correlations.\r\n            na_corr = _corr.isna().astype(int)\r\n            sum_na = na_corr.sum(axis=0)\r\n            while na_corr[sum_na > 0].size > 0:\r\n                col_na = na_corr[sum_na == 0]\r\n                most_na = na_corr.idxmax()\r\n                logger.info('NA value for cols: {}'.format(col_na))\r\n                # logger.warning('These features have no valid correlations: {}'.format(col_na))\r\n                _corr.drop(columns=col_na, inplace=True).drop(index=col_na, inplace=True)\r\n                na_corr = _corr.isna().astype(int)\r\n                sum_na = na_corr.sum(axis=0)\r\n            # logger.info('Correlation matrix: {}'.format(_corr))\r\n        assert _corr is not None\r\n        logger.debug('Covariance matrix with weighting of {}:\\n{}'.format(self.options.sample_wts, _corr))\r\n        self._corr_mat = _corr\r\n        return _corr\r\n\r\n    @cachetools.cached(cache={})\r\n    def set_cov_pairs(self, cov_thresh=None):\r\n        if not cov_thresh:\r\n            cov_thresh = 0.9\r\n        corr_mat = self.X.corr_mat\r\n        corr_dict = corr_mat.to_dict(orient='series')\r\n        same_list, close_list = list(), list()\r\n        for ind, col in itertools.combinations(corr_dict.keys(), r=2):\r\n            val = corr_mat.loc[ind, col]\r\n            print(ind, loc, val)\r\n            if abs(val) == 1:\r\n                same_list.append((col, ind))\r\n                logger.info('{} and {} with R of {}'.format(ind, col, val))\r\n            elif abs(val) > cov_thresh:\r\n                logger.info('{} and {} are highly correlated with an R of {}'.format(ind, col, val))\r\n                close_list.append([(ind, col), val])\r\n        logger.info(close_list)\r\n        self.X._corr_pairs = same_list\r\n\r\n    @cachetools.cached(cache={})\r\n    def set_condition_num(self):\r\n        if type(self.condition_num) is pd.DataFrame and not self.condition_num.empty:\r\n            logger.warning('Condition number is already calculated for this FeatureFrame.')\r\n            return self.condition_num\r\n        # Eigenvalue method for collinearity.\r\n        eigenvalues = np.linalg.eigvals(self.corr_mat)\r\n        condition_index = np.sqrt(np.max(eigenvalues) / eigenvalues)\r\n        pprint.PrettyPrinter().pprint(condition_index)\r\n        logger.info(f\"Condition Index: {condition_index}\")\r\n        np.save('{}condition_index'.format(self.options.stats_dir), arr=condition_index)\r\n        condition_index = pd.DataFrame(condition_index)\r\n        if condition_index.shape[0] == self.corr_mat.shape[0]:\r\n            condition_index.reindex_like(self.corr_mat.index)\r\n        if condition_index.shape[1] == self.corr_mat.shape[1]:\r\n            condition_index.columns.rename(self.corr_mat.columns)\r\n        self._condition_num = condition_index\r\n\r\n    @cachetools.cached(cache={})\r\n    def multi_collin_feats(self):\r\n        dense_df = self.X.feat_frame[self.X.dense].copy(deep=True)\r\n        logger.debug(self.X.dense)\r\n        # sklearn.utils.assert_all_finite(dense_df)\r\n        vif = pd.DataFrame()\r\n        vif[\"features\"] = dense_df.columns\r\n        vif[\"VIF Factor\"] = [variance_inflation_factor(np.array(dense_df.values.tolist(), dtype=float), i) for i in\r\n                             range(dense_df.shape[1])]\r\n        vif.set_index(keys='features', drop=True, inplace=True)\r\n        logger.info(vif)\r\n        return vif\r\n\r\n    def seq_linear_select(self, df=None, label=None, method='auto', seq_linear_select_scoring='balanced_accuracy',\r\n                          num_parallel=1):\r\n        if not df:\r\n            df = self.X\r\n        if not label:\r\n            label = self.y\r\n        check_X_y(df, label)\r\n        if method == 'elastic' or (method == 'auto' and df.shape[1] < 50):\r\n            seq_linear_select_estimator = ElasticNetCV(n_jobs=num_parallel, random_state=0, max_iter=5000, tol=1e-5,\r\n                                                       cv=3)\r\n        else:\r\n            seq_linear_select_estimator = LassoCV(fit_intercept=False, max_iter=2500, tol=1e-5, n_jobs=num_parallel,\r\n                                                  random_state=0,\r\n                                                  cv='balanced')\r\n        seq_linear_selector = SequentialFeatureSelector(estimator=seq_linear_select_estimator,\r\n                                                        scoring=seq_linear_select_scoring,\r\n                                                        n_jobs=num_parallel)\r\n        return seq_linear_selector.fit(df)\r\n\r\n    # TODO: Delete groups with no members or just one member.\r\n    def _set_descriptor_groups_df(self, grouping_dir=PADEL_GROUPS_DIR, cols=GROUP_COL_NAMES, use3d=False):\r\n        padel_name_df = load_padel_df()\r\n        short_long_zip = zip(padel_name_df['Descriptor name'].tolist(), padel_name_df['Description'].tolist())\r\n        short_long_dict = dict([(a, b) for a, b in short_long_zip])\r\n        if grouping_dir:\r\n            desc_groups_df = pd.read_csv(filepath_or_buffer=grouping_dir, usecols=cols)\r\n            # desc_groups_df.dropna(subset='Descriptor', inplace=True)\r\n            if not use3d:\r\n                # logging.info(desc_groups_df['Descriptor'])\r\n                desc_groups_df.drop(desc_groups_df[desc_groups_df['Class'] == '3D'].index, inplace=True)\r\n            long_dict = dict()\r\n            for _, i, desc_group in desc_groups_df[desc_groups_df['Number'] > 1][['Type', 'Descriptor']].itertuples():\r\n                # logging.warning('{} {}'.format(type(desc_group), desc_group))\r\n                # i, desc_group = vals.values\r\n                ind = str(i)\r\n                logging.info('Desc row: {}: {}'.format(ind, desc_group))\r\n                if desc_group == 'nan' or desc_group == np.NaN or not desc_group or type(desc_group) is float:\r\n                    long_dict.update([(ind, list())])\r\n                    continue\r\n                elif type(desc_group) is str:\r\n                    if ',' not in desc_group:\r\n                        if desc_group in short_long_dict.keys():\r\n                            long_dict.update([(ind, list(desc_group))])\r\n                            continue\r\n                        else:\r\n                            logging.error('COULD NOT FIND DESCRIPTOR IN LIST: {}'.format(desc_group))\r\n                            continue\r\n                    elif ',' in desc_group:\r\n                        key_list = [d.strip() for d in desc_group.split(',')]\r\n                        desc_list = list()\r\n                        if any([len(k) == 1 for k in key_list]):\r\n                            logging.warning('SPLITTING INTO SINGLE CHARACTERS!')\r\n                        else:\r\n                            for d in key_list:\r\n                                if len(d) <= 1:\r\n                                    logging.error('Descriptor splitting for {} gives single characters'.format(key_list))\r\n                                elif d not in short_long_dict.keys():\r\n                                    logging.warning('Descriptor not found in key list!!!: \"{}\"'.format(d))\r\n                                    continue\r\n                                    # key_list.remove(d)\r\n                                else:\r\n                                    desc_list.append((d, short_long_dict[d]))\r\n                                    logger.debug('{} in keys!'.format(d))\r\n                            long_dict = dict(desc_list)\r\n                            if len(desc_list) == 0:\r\n                                logging.warning('Empty descriptor group label: {}: {}'.format(ind, desc_group.split(',')))\r\n                                long_dict.update({ind: list()})\r\n                            elif len(desc_list) == 0 and any([k in desc_group.strip(',') for k in short_long_dict.keys()]):\r\n                                logging.error('DESCRIPTOR LIST NOT BEING SPLIT CORRECTLY! \\n{}VS.\\n{}'.format(desc_group, desc_list))\r\n                                raise ValueError\r\n                            else:\r\n                                long_dict.update({ind: list()})\r\n                    else:\r\n                        logging.error('Desc list {} is string but is not in descriptor DF and does not contain comma delimiter.'.format(key_list))\r\n                        raise ValueError\r\n                elif type(desc_group) is list:\r\n                    long_dict.update((ind, desc_group))\r\n                elif type(desc_group) is tuple or type(desc_group) is set:\r\n                    long_dict.update((ind, list(desc_group)))\r\n                else:\r\n                    logging.error('Unknown type {} for descriptor group {}'.format(type(desc_group), desc_group))\r\n                    raise TypeError\r\n                    # long_dict.update([(ind, [short_long_dict[str(d)] for d in desc_group.split(',') if str(d) in short_long_dict.keys()])])\r\n            logger.info('Input dictionary: {}'.format(long_dict.items()))\r\n            for k, v in long_dict.items():\r\n                if type(v) is not list:\r\n                    long_dict[k] = list(v)\r\n            logger.info('Input dictionary: {}'.format(long_dict.items()))\r\n            long_df = pd.DataFrame.from_dict(long_dict)\r\n            if long_df.empty:\r\n                long_df = pd.DataFrame.from_dict(long_dict, orient='index')\r\n            if long_df.empty:\r\n                long_df = pd.DataFrame.from_records(data=long_dict)\r\n            if long_df.empty:\r\n                long_df = pd.Series(data=long_dict.values(), index=long_dict.keys(), name='Long')\r\n            if long_df.empty:\r\n                logger.warning('Long descriptor DF is empty!')\r\n                raise ValueError\r\n            if type(long_df) is pd.DataFrame and len(long_df.shape) > 1 and long_df.shape[0] < long_df.shape[1]:\r\n                long_df = long_df.T\r\n            else:\r\n                logger.info(type(long_df))\r\n            logger.info('Brand new long_df: {}'.format(long_df))\r\n            try:\r\n                long_df.rename(name=['Long'], inplace=True)\r\n            except TypeError:\r\n                logger.error('Could not rename long_df')\r\n            logger.info('Long DF: \\n{}'.format(long_df.to_string()))\r\n            new_desc_df = desc_groups_df.set_index(keys='Type').join(long_df, how='inner')\r\n            if new_desc_df.empty:\r\n                new_desc_df = desc_groups_df.join(long_df.reset_index(), how='inner', ignore_index=True)\r\n            # desc_groups_df['Long'] = desc_groups_df['Descriptor'].apply(func=lambda x: [[short_long_dict[a.strip()] for a in d if a != 'NaN' and a != np.NaN] for d.split(',') in x if len(d) > 1 else x])\r\n            # desc_groups_df['Long'] = desc_groups_df['Descriptor'].apply(func=lambda x: [short_long_dict[d.rstrip(',')] for d in x.split()])\r\n            logger.info('Col Names: {}'.format(new_desc_df.columns))\r\n            logger.info('Long Name Descriptors: \\n{}'.format(new_desc_df))\r\n            # new_desc_df.sort_values(key=lambda x: len(x), inplace=True, ascending=False)\r\n        else:\r\n            raise FileNotFoundError\r\n        self._descriptor_groups_df = new_desc_df\r\n\r\n    @cachetools.cached(cache={})\r\n    def scale_df(self, df=None, scale_to=None, select_scalers='all', *args, **kwargs):\r\n        df_scaled = dict()\r\n        if not df:\r\n            df = self.X.cont\r\n        if not self.scalers:\r\n            self.scalers = self.X.scalers(scale_to, *args, **kwargs)\r\n        if select_scalers != 'all':\r\n            select_scalars = [key for key in self.scalers.keys() if key in select_scalers]\r\n        else:\r\n            select_scalers = self.scalers\r\n        for name, scaler in self.scalers.items():\r\n            if sklearn.utils.validation.check_is_fitted(scaler):\r\n                df_scaled.update((name, scaler.transform(df)))\r\n            else:\r\n                if scale_to:\r\n                    scaler.fit(scale_to)\r\n                    scaler.transform(df)\r\n                else:\r\n                    scaler.fit_transform(df)\r\n                self.scalers.update((name, self.scalers))\r\n        return df_scaled\r\n\r\n    # Set Sample weight option.\r\n    def isolate_observations(self, fit_to=None, sample_wts='auto', contamination=0.001, rstate=None, n_jobs=-1):\r\n        from sklearn.ensemble import IsolationForest\r\n        if not fit_to:\r\n            fit_to = self.X.feat_frame[self.X.nonzero_vars]\r\n        if sample_wts == 'auto':\r\n            sample_wts = None\r\n        iso = IsolationForest(n_jobs=n_jobs, contamination=contamination, random_state=0)\r\n        if fit_to:\r\n            iso = iso.fit(fit_to, sample_weight=sample_wts)\r\n        return iso\r\n\r\n    @property\r\n    def X(self):\r\n        return self._X\r\n\r\n    @property\r\n    def y(self):\r\n        return self._y\r\n\r\n    @staticmethod\r\n    def get_high_vars(self, n=50):\r\n        scaler = RobustScaler()\r\n        high_thresh = VarianceThreshold()\r\n        high_thresh_pipe = Pipeline(steps=[('high_var_scaler', scaler), ('high_thresh', high_thresh)])\r\n        return high_thresh_pipe\r\n\r\n    @property\r\n    def corr(self):\r\n        return self._corr\r\n\r\n    @property\r\n    def descriptor_groups_df(self):\r\n        return self._descriptor_groups_df\r\n\r\n    @descriptor_groups_df.setter\r\n    def descriptor_groups_df(self, value):\r\n        if self._descriptor_groups_df.shape[0] > 1:\r\n            pass\r\n        self._descriptor_groups_df = value\r\n\r\n    @property\r\n    def scalers(self):\r\n        return self._scalers\r\n\r\n    @cachetools.cached(cache={})\r\n    @scalers.setter\r\n    def scalers(self, fit_to=None, robust_iqr=(0.05, 0.95), unit_robust_iqr=(0.05, 0.95), normalizer_norm='l2', *args,\r\n                **kwargs):\r\n        \"\"\"\r\n        :param fit_to:\r\n        :param robust_iqr:\r\n        :param unit_robust_iqr:\r\n        :param normalizer_norm:\r\n        :param args:\r\n        :param kwargs:\r\n        :return:\r\n        \"\"\"\r\n\r\n        # noinspection PyArgumentEqualDefault\r\n        scaler_dict = dict([\r\n            ('robust', RobustScaler(quantile_range=robust_iqr, unit_variance=False, **kwargs)),\r\n            ('robust_unit', RobustScaler(quantile_range=unit_robust_iqr, unit_variance=True, **kwargs)),\r\n            ('normal', Normalizer(norm=normalizer_norm, **kwargs))])\r\n        if fit_to:\r\n            [val.fit(fit_to.to_numpy()) for val in scaler_dict.values()]\r\n        self._scalers = scaler_dict\r\n\r\ndef iter_feats(feat_groups):\r\n    for name, feat_dict in feat_groups:\r\n        col_group, selector = feat_dict\r\n\r\n\r\nREQUIRED_OPTS = ('tol_discrete', 'tol_sparse', 'sparsity')\r\n\r\n\r\nclass FeatureFrame:\r\n\r\n    def __init__(self, df=None, options=None, *args, **kwargs):\r\n        self._feat_frame = df\r\n        self._original = self.feat_frame.copy(deep=True)\r\n        self._options = options\r\n        self._sparse = None\r\n        self._dense = None\r\n        self._discrete = None\r\n        self._cont = None\r\n        self._neg = None\r\n        self._nonneg = None\r\n        self._feat_vars = None\r\n        self._feat_stat_scorer = None\r\n        self._corr_pairs = None\r\n        self._corr_mat = None\r\n        self._set_attr()\r\n\r\n    @property\r\n    def feat_frame(self):\r\n        return self._feat_frame\r\n\r\n    @property\r\n    def original(self):\r\n        return self._original\r\n\r\n    @property\r\n    def options(self):\r\n        return self._options\r\n\r\n    @options.setter\r\n    def options(self, value):\r\n        self._options = format_options(value)\r\n\r\n    def _set_attr(self):\r\n        self._set_feat_vars()\r\n        self._set_sparse_dense()\r\n        self._set_discrete_cont()\r\n        self._set_neg_nonneg()\r\n        self._set_corr_mat()\r\n        # self.scalers()\r\n        self._set_feat_stat_scorer(None)\r\n\r\n    @property\r\n    def feat_vars(self):\r\n        return self._feat_vars\r\n\r\n    def _set_feat_vars(self, skipna=True, numeric_only=False, *args, **kwargs):\r\n        feat_vars = self.original.var(axis='index', skipna=skipna, numeric_only=numeric_only)\r\n        logger.info('Feature variances: {}'.format(feat_vars))\r\n        self._feat_vars = feat_vars\r\n        if feat_vars.empty or feat_vars.dropna().empty:\r\n            raise ValueError\r\n        return feat_vars\r\n        # self.nonzero_vars((self.feat_vars > 0).index.append(pd.Index([c for c in self.original.columns if c not in self.feat_vars.index])))\r\n        # self.zero_vars(self.original.columns.symmetric_difference(self.nonzero_vars))\r\n\r\n    @property\r\n    def desc_groups(self):\r\n        return self._desc_groups\r\n\r\n    @property\r\n    def feat_stat_scorer(self):\r\n        return self._feat_stat_scorer\r\n\r\n    def _set_feat_stat_scorer(self, value):\r\n        cont_mi_clf = functools.partialmethod(mutual_info_classif, discrete_features=False)\r\n        disc_mi_clf = functools.partialmethod(mutual_info_classif, discrete_features=True)\r\n        return dict(\r\n            [(f_classif, 'real'), (cont_mi_clf, 'real'), (disc_mi_clf, 'discrete'), (chi2, 'nonneg')])\r\n\r\n    def _set_neg_nonneg(self):\r\n        nonneg_cols, neg_cols = list(), list()\r\n        for col, ser in self.feat_frame.items():\r\n            if (ser.values < 0).any():\r\n                neg_cols.append(col)\r\n            else:\r\n                nonneg_cols.append(col)\r\n        self.nonneg = pd.Index(nonneg_cols)\r\n        self.neg = pd.Index(neg_cols)\r\n        logger.debug(self.neg)\r\n        logger.debug(self.nonneg)\r\n\r\n    @property\r\n    def discrete(self):\r\n        return self._discrete\r\n\r\n    @discrete.setter\r\n    def discrete(self, value):\r\n        if type(value) is not pd.Index:\r\n            raise AttributeError\r\n        self._discrete = value\r\n\r\n    @property\r\n    def cont(self):\r\n        return self._cont\r\n\r\n    @cont.setter\r\n    def cont(self, value):\r\n        if type(value) is not pd.Index:\r\n            raise AttributeError\r\n        self._cont = value\r\n\r\n    def _set_discrete_cont(self):\r\n        nonzeros = self.feat_frame[self.nonzero_vars].copy()\r\n        remainder = nonzeros.round(0).sub(nonzeros).abs()\r\n        logger.debug(remainder)\r\n        self.discrete = nonzeros.columns[(remainder < self.options.tol_discrete).all()]\r\n        self.cont = nonzeros.columns.symmetric_difference(self.discrete)\r\n        # logger.info('Discrete: {}', self.discrete)\r\n        # logger.info('Continuous: {}', self.cont)\r\n\r\n    @property\r\n    def corr_mat(self):\r\n        return self._corr_mat\r\n\r\n    # aweights is used to balance covariance estimates for different subsets of observations.\r\n    def _set_corr_mat(self, corr_method='pearson', filter_nan=False, *args, **kwargs):\r\n        _corr = self.feat_frame[self.dense].corr(method=corr_method)\r\n        np.cov()\r\n        if filter_nan:\r\n            # Remove features with no non-NaN correlations.\r\n            na_corr = _corr.isna().astype(int)\r\n            col_na = _corr[na_corr.sum(axis=0) < _corr.shape[1] - 1]\r\n            logger.info('Correlation matrix: {}'.format(_corr))\r\n            logger.info('NA value for cols: {}'.format(col_na))\r\n            logger.warning('These features have no valid correlations: {}'.format(col_na))\r\n            corr_matrix = _corr.loc[col_na, col_na]\r\n        else:\r\n            corr_matrix = _corr\r\n        logger.debug(corr_matrix)\r\n        self._corr_mat = corr_matrix\r\n\r\n    @property\r\n    def corr_pairs(self):\r\n        return self._corr_pairs\r\n\r\n    @corr_pairs.setter\r\n    def corr_pairs(self, corr_method='kendall', *args, **kwargs):\r\n        corr_val = self.feat_frame.corr_mat(corr_method=corr_method, *args, **kwargs)\r\n        logger.info(corr_val, corr_val.shape)\r\n        corr_sorted = corr_val.unstack().sort_values(kind='quicksort', ascending=False)\r\n        logging.info('Sorted Correlation values:\\n{}'.format(corr_sorted[:10]))\r\n        corr_set = set()\r\n        for i in range(0, self.X.dense.shape[1]):\r\n            for j in range(0, i + 1):\r\n                corr_set.add((self.X.dense.columns[i], self.X.dense.columns[j]))\r\n        self._corr_pairs = corr_set\r\n\r\n    @property\r\n    def sparse(self):\r\n        return self._sparse\r\n\r\n    @sparse.setter\r\n    def sparse(self, value):\r\n        if type(value) is not pd.Index:\r\n            raise AttributeError\r\n        self._sparse = value\r\n\r\n    @property\r\n    def zero_vars(self):\r\n        return self._zero_vars\r\n\r\n    @zero_vars.setter\r\n    def zero_vars(self, value):\r\n        if type(value) is not pd.Index:\r\n            raise AttributeError\r\n        self._zero_vars = value\r\n\r\n    @property\r\n    def nonzero_vars(self):\r\n        return self._nonzero_vars\r\n\r\n    @nonzero_vars.setter\r\n    def nonzero_vars(self, value):\r\n        if type(value) is not pd.Index:\r\n            raise AttributeError\r\n        self._nonzero_vars = value\r\n\r\n    @cachetools.cached(cache={})\r\n    def _set_sparse_dense(self, all_freq=True, ignore_nan=False):\r\n        if 0 < self.options.tol_sparse < 1:\r\n            freq_cut = 1 - self.options.tol_sparse\r\n        elif 1 < self.options.tol_sparse < self.X.vary.shape[0]:\r\n            freq_cut = self.options.tol_sparse / self.X.vary.shape[0]\r\n        else:\r\n            raise ValueError\r\n        logger.info('Sparse tolerance: {}'.format(self.options.tol_sparse))\r\n        logger.info('Freq cut: {}'.format(freq_cut))\r\n        sparse_list, dense_list, zero_list, freq_dict = list(), list(), list(), dict()\r\n        for col, ser in self._pd_feat_freq(ignore_nan=ignore_nan):\r\n            if all_freq:\r\n                freq_dict[col] = ser\r\n            sermax = ser.max(skipna=ignore_nan)\r\n            # print('Feature freq max: {}'.format(sermax))\r\n            logging.debug('Feature maximum: {}'.format(sermax))\r\n            if freq_cut < sermax < 1.:\r\n                sparse_list.append(col)\r\n            elif freq_cut > sermax:\r\n                dense_list.append(col)\r\n            else:\r\n                zero_list.append(col)\r\n        self.sparse = pd.Index(sparse_list)\r\n        self.dense = pd.Index(dense_list)\r\n        nonzeros = sparse_list.copy()\r\n        nonzeros.extend(dense_list)\r\n        self.nonzero_vars = pd.Index(nonzeros)\r\n        self.zero_vars = pd.Index(zero_list)\r\n        # self.feature_properties['zero_vars'] = self.X.vary.columns.symmetric_difference(pd.Index(data=nonzeros))\r\n\r\n    @cachetools.cached(cache={})\r\n    def _pd_feat_freq(self, ignore_nan=False):\r\n        # logger.info('Feature variances: ', self.feat_vars)\r\n        for col, ser in self.original.items():\r\n            yield col, ser.value_counts(normalize=True, dropna=ignore_nan)\r\n\r\n\r\n@functools.singledispatch\r\ndef format_options(arg):\r\n    self._options = args\r\n\r\n\r\n@format_options.register(dict)\r\ndef _(arg):\r\n    for o in REQUIRED_OPTS:\r\n        if o not in arg.keys():\r\n            logger.error('Required option {} not found.'.format(o))\r\n            raise KeyError\r\n    return arg\r\n\r\n# selector = sklearn.feature_selection.SelectFromModel(estimator=score_f, max_features=n_feats, prefit=True, threshold='median')\r\n\r\n# padel_groups = _get_desc_groups(X, grouping_dir=PADEL_GROUPS_DIR, cols=GROUP_COL_NAMES)\r\n# feat_group_names = ['Dense', 'Matrix-based', 'E-States', 'Sparse', 'All Features']\r\n# feat_selector_bool = [False, True, True, True, False]\r\n# feat_col_list = [dense, matrix, estate, sparse,  X_train.columns]\r\n# zip(feat_group_names, feat_col_list)\r\n# feat_col_list = [sparse, dense, matrix, estate, X_train.columns]\r\n\r\n\r\n# train_feats = X_train.copy()\r\n# train_labels = train_y.copy()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/models/feature_selector.py b/dmso_model_dev/models/feature_selector.py
--- a/dmso_model_dev/models/feature_selector.py	
+++ b/dmso_model_dev/models/feature_selector.py	
@@ -53,6 +53,7 @@
     return options
 
 # TODO: Check assumptions/requirements of estimators.
+# TODO: Alter when *sample* is run data, to after zero-var drop and before cov calculations.
 class FeatureSelectorTrainer:
     _group_names = ('zero_var', 'nonzero_var', 'sparse', 'dense', 'discrete', 'cont', 'nonneg', 'neg')
 
@@ -64,8 +65,9 @@
         elif fs and type(fs) is FeatureFrame:
             self.X = fs
         else:
-            self._X = FeatureFrame(df=data.copy(deep=True), options=self.options)
-        self._y = labels
+            self.X = FeatureFrame(df=data.copy(deep=True), options=self.options)
+        # self._X.feat_frame = self._X.feat_frame[self.X.nonzero_vars]
+        self._y = labels[self.X.feat_frame.index].astype(int)
         self._corr = None
         self._corr_mat = None
         self._condition_num = None
@@ -76,6 +78,8 @@
         self._scalers = dict()
         # self._get_desc_groups(grouping_dir=PADEL_GROUPS_DIR, cols=GROUP_COL_NAMES)
         # x_sample = train_X.sample(axis=1, frac=0.2, random_state=0)
+        # self.check_feat_num()
+        '''
         if not debug:
             if len(self.X.sparse) < MIN_NUM_FEATS['sparse']:
                 logger.warning('Too few sparse features: \n{}'.format(self.X.sparse))
@@ -83,7 +87,7 @@
             if len(self.X.dense) < MIN_NUM_FEATS['dense']:
                 logger.warning('Too few dense features: \n{}'.format(self.X.dense))
                 raise ValueError
-        cweights = self.y.copy().replace({1: 1, 0: 10})
+        '''
         # self.__initialize_data_history__()
 
     '''
@@ -105,7 +109,8 @@
         return dc
 
     def _calc_values(self):
-        self._corr = self.X.feat_frame[self.X.nonzero_vars].corrwith(other=self.y, axis='columns',method=self.options.corr_meth)
+        nonzeros = self.X.feat_frame[self.X.nonzero_vars]
+        self._corr = nonzeros.corrwith(other=self.y, axis='columns',method=self.options.corr_meth)
         # self._set_descriptor_groups_df()
 
     def check_feat_num(self):
@@ -117,6 +122,9 @@
     def corr_mat(self):
         return self._corr_mat
 
+    @property
+    def condition_num(self):
+        return self._condition_num
 
     # aweights is used to balance covariance estimates for different subsets of observations.
     def calc_cov_mat(self, calc_cov=None, filter_nan=False, sample_wts='auto', *args, **kwargs):
@@ -157,21 +165,25 @@
     @cachetools.cached(cache={})
     def set_cov_pairs(self, cov_thresh=None):
         if not cov_thresh:
-            cov_thresh = 0.9
-        corr_mat = self.X.corr_mat
-        corr_dict = corr_mat.to_dict(orient='series')
+            self.options.cov_thresh
+        if (type(self.corr_mat) is not pd.DataFrame or self.corr_mat.empty) and type(self.corr_mat) is not np.ndarray:
+            self.calc_cov_mat()
+        # corr_dict = self.corr_mat.to_dict()
         same_list, close_list = list(), list()
-        for ind, col in itertools.combinations(corr_dict.keys(), r=2):
-            val = corr_mat.loc[ind, col]
-            print(ind, loc, val)
-            if abs(val) == 1:
-                same_list.append((col, ind))
-                logger.info('{} and {} with R of {}'.format(ind, col, val))
-            elif abs(val) > cov_thresh:
-                logger.info('{} and {} are highly correlated with an R of {}'.format(ind, col, val))
-                close_list.append([(ind, col), val])
-        logger.info(close_list)
-        self.X._corr_pairs = same_list
+        for ind, col in itertools.combinations(self.corr_mat.columns.to_list(), r=2):
+            val = self.corr_mat.loc[ind, col]
+            # print(ind, col, val)
+            if np.isscalar(val):
+                if abs(val) == 1:
+                    same_list.append((col, ind))
+                    logger.info('{} and {} with R of {}'.format(ind, col, val))
+                elif abs(val) > cov_thresh:
+                    logger.info('{} and {} are highly correlated with an R of {}'.format(ind, col, val))
+                    close_list.append([(ind, col), val])
+            else:
+                logger.warning('Value in covariance matrix is not a scalar?!?\n{}'.format(val))
+        self.corr_pairs = close_list
+        return close_list
 
     @cachetools.cached(cache={})
     def set_condition_num(self):
@@ -192,36 +204,65 @@
         self._condition_num = condition_index
 
     @cachetools.cached(cache={})
-    def multi_collin_feats(self):
-        dense_df = self.X.feat_frame[self.X.dense].copy(deep=True)
-        logger.debug(self.X.dense)
-        # sklearn.utils.assert_all_finite(dense_df)
+    def multi_collin_feats(self, subset=None):
+        if not subset:
+            subset = self.X.dense
+        subset_frame = self.X.feat_frame[subset].copy(deep=True)
+        logger.debug(subset)
+        # sklearn.utils.assert_all_finite(subset_frame)
         vif = pd.DataFrame()
-        vif["features"] = dense_df.columns
-        vif["VIF Factor"] = [variance_inflation_factor(np.array(dense_df.values.tolist(), dtype=float), i) for i in
-                             range(dense_df.shape[1])]
-        vif.set_index(keys='features', drop=True, inplace=True)
-        logger.info(vif)
+        vif["features"] = subset_frame.columns.copy(deep=True)
+        vif["VIF Factor"] = [variance_inflation_factor(np.array(subset_frame.values.tolist(), dtype=float), i) for i in
+                             range(subset_frame.shape[1])]
+        vif.sort_values(by='VIF Factor').set_index(keys='features', drop=True, inplace=True)
+        logger.info('Variance Inflation Factor:\n{}'.format(vif.to_string()))
         return vif
 
-    def seq_linear_select(self, df=None, label=None, method='auto', seq_linear_select_scoring='balanced_accuracy',
-                          num_parallel=1):
-        if not df:
-            df = self.X
-        if not label:
+    def seq_linear_select(self, df=None, label=None, subset='auto', method='auto', seq_linear_select_scoring='balanced_accuracy', n_feats='auto'):
+        if type(df) is not pd.DataFrame:
+            df = self.X.feat_frame[self.X.dense]
+        if type(label) is not pd.Series and type(label) is not pd.DataFrame:
             label = self.y
         check_X_y(df, label)
+        from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV
+
         if method == 'elastic' or (method == 'auto' and df.shape[1] < 50):
-            seq_linear_select_estimator = ElasticNetCV(n_jobs=num_parallel, random_state=0, max_iter=5000, tol=1e-5,
+            seq_linear_select_estimator = ElasticNetCV(n_jobs=self.options.pjobs, random_state=0, max_iter=5000, tol=1e-5,
                                                        cv=3)
-        else:
-            seq_linear_select_estimator = LassoCV(fit_intercept=False, max_iter=2500, tol=1e-5, n_jobs=num_parallel,
+        elif type(seq_linear_select_scoring) is str and 'lasso' in seq_linear_select_scoring.lower():
+            seq_linear_select_estimator = LassoCV(fit_intercept=False, max_iter=2500, tol=1e-5, n_jobs=self.options.pjobs,
                                                   random_state=0,
                                                   cv='balanced')
-        seq_linear_selector = SequentialFeatureSelector(estimator=seq_linear_select_estimator,
+        else:
+            seq_linear_select_estimator = method
+        seq_feat_selector = SequentialFeatureSelector(estimator=seq_linear_select_estimator,
                                                         scoring=seq_linear_select_scoring,
-                                                        n_jobs=num_parallel)
-        return seq_linear_selector.fit(df)
+                                                        n_jobs=self.options.pjobs, n_features_to_select=n_feats)
+        return seq_feat_selector
+
+
+    def custom_recursive_eliminator(self,  n_drop):
+        from sklearn.feature_selection import RFE, RFECV
+        from sklearn.feature_selection import mutual_info_classif
+        dropped_feats = list()
+        # First pass
+        vif = self.multi_collin_feats()
+        multicos = vif[vif >= 5.0]
+        dropped_feats.append(multicos.index.tolist())
+        feat_list = list()
+        cov_list = [(i, j, self.corr_mat.to_dict()[j][i]) for i, j in self.X.corr_pairs]
+        [feat_list.extend([i, j]) for i, j in self.X.corr_pairs]
+        feat_counts = pd.Series(feat_list).value_counts()
+        pd.DataFrame(cov_list).to_csv('{}correlated_pairs.csv'.format(self.options.stats_dir), na_rep='NA')
+        return pd.DataFrame(cov_list, columns=['Feat1', 'Feat2', 'CoV']), feat_counts
+        # for feat, count in feat_counts.items():
+
+        # for feat, num in feat_counts.items():
+
+        # cov_list.sort(key=lambda x: x[2], reverse=True)
+        # for i, j, score in cov_list:
+
+
 
     # TODO: Delete groups with no members or just one member.
     def _set_descriptor_groups_df(self, grouping_dir=PADEL_GROUPS_DIR, cols=GROUP_COL_NAMES, use3d=False):
@@ -348,16 +389,20 @@
         return df_scaled
 
     # Set Sample weight option.
-    def isolate_observations(self, fit_to=None, sample_wts='auto', contamination=0.001, rstate=None, n_jobs=-1):
+    def isolate_observations(self, fit_to=None, n_samples='auto', sample_wts='auto', sparsity='dense', n_est=100, contamination=0.001, rstate=None, n_jobs=-1):
         from sklearn.ensemble import IsolationForest
-        if not fit_to:
-            fit_to = self.X.feat_frame[self.X.nonzero_vars]
+        predicted = None
+        if type(fit_to) is not pd.DataFrame:
+            fit_to = self.X.feat_frame
         if sample_wts == 'auto':
-            sample_wts = None
-        iso = IsolationForest(n_jobs=n_jobs, contamination=contamination, random_state=0)
-        if fit_to:
-            iso = iso.fit(fit_to, sample_weight=sample_wts)
-        return iso
+            sample_wts = self.options.sample_wts[fit_to.index]
+        iso = IsolationForest(n_jobs=self.options.pjobs, max_samples=n_samples, n_estimators=int(n_est), contamination=contamination, random_state=0)
+        if sparsity == 'dense':
+            dense_df = fit_to[self._X.dense].astype(np.float32)
+            iso = iso.fit(dense_df, sample_weight=self.options.sample_wts)
+            #iso = iso.fit(fit_to[self._X.sparse].astype(), sample_weight=self.options.sample_wts)
+            predicted = iso.predict(dense_df)
+        return iso, predicted
 
     @property
     def X(self):
@@ -420,15 +465,17 @@
         col_group, selector = feat_dict
 
 
-REQUIRED_OPTS = ('tol_discrete', 'tol_sparse', 'sparsity')
+REQUIRED_OPTS = ()
 
 
 class FeatureFrame:
 
     def __init__(self, df=None, options=None, *args, **kwargs):
-        self._feat_frame = df
-        self._original = self.feat_frame.copy(deep=True)
         self._options = options
+        self._original = df.copy(deep=True)
+        self._feat_frame = None
+        self._nonzero_vars = None
+        self._zero_vars = None
         self._sparse = None
         self._dense = None
         self._discrete = None
@@ -436,6 +483,8 @@
         self._neg = None
         self._nonneg = None
         self._feat_vars = None
+        self._outliers = None
+        self._inliers = None
         self._feat_stat_scorer = None
         self._corr_pairs = None
         self._corr_mat = None
@@ -443,8 +492,16 @@
 
     @property
     def feat_frame(self):
-        return self._feat_frame
+        if self.inliers is not None and not self.inliers.empty:
+            return self._feat_frame.loc[self.inliers]
+        else:
+            return self._feat_frame
 
+    @feat_frame.setter
+    def feat_frame(self, value):
+        self._feat_frame = value
+
+
     @property
     def original(self):
         return self._original
@@ -458,13 +515,32 @@
         self._options = format_options(value)
 
     def _set_attr(self):
+        self.feat_frame = self.check_duplicate_feats(X=self.original)
         self._set_feat_vars()
+        # self.feat_frame = self.original[self.nonzero_vars]
         self._set_sparse_dense()
         self._set_discrete_cont()
         self._set_neg_nonneg()
-        self._set_corr_mat()
+        # self.calc_cov_mat(sample_wts=self.options.sample_wts[self.feat_frame.index])
         # self.scalers()
-        self._set_feat_stat_scorer(None)
+        # self._set_feat_stat_scorer(None)
+
+    # Make results attributes to maintain history of modifications.
+    def check_duplicate_feats(self, X=None):
+        if X is None:
+            X = self.feat_frame
+        dup_col_names = X.columns[X.columns.duplicated(keep=False)]
+        if len(dup_col_names.tolist()) == 0:
+            return None
+        logger.warning('Duplicated features in FeatureFrame:\n{}'.format(dup_col_names.unique()))
+        dict_list = list()
+        renamer_dict = dict()
+        drop_dict, rename_dict = true_duplicates(X, None, subset_names=dup_col_names.unique())
+        [renamer_dict.update(d) for d in dict_list]
+        [logger.info('Dropped indices {} for feature {}'.format(k, v) for k, v in renamer_dict.items())]
+        new_frame = X.copy()
+        [new_frame.drop(index=a, inplace=True) for a in renamer_dict.values()]
+        return new_frame
 
     @property
     def feat_vars(self):
@@ -472,7 +548,9 @@
 
     def _set_feat_vars(self, skipna=True, numeric_only=False, *args, **kwargs):
         feat_vars = self.original.var(axis='index', skipna=skipna, numeric_only=numeric_only)
-        logger.info('Feature variances: {}'.format(feat_vars))
+        self._zero_vars = feat_vars[feat_vars == 0].index
+        self._nonzero_vars = feat_vars[feat_vars != 0].index
+        logger.info('Feature variances:\n{}'.format(feat_vars))
         self._feat_vars = feat_vars
         if feat_vars.empty or feat_vars.dropna().empty:
             raise ValueError
@@ -480,6 +558,14 @@
         # self.nonzero_vars((self.feat_vars > 0).index.append(pd.Index([c for c in self.original.columns if c not in self.feat_vars.index])))
         # self.zero_vars(self.original.columns.symmetric_difference(self.nonzero_vars))
 
+    @property
+    def outliers(self):
+        return self._outliers
+
+    @property
+    def inliers(self):
+        return self._inliers
+
     @property
     def desc_groups(self):
         return self._desc_groups
@@ -510,6 +596,12 @@
     def discrete(self):
         return self._discrete
 
+    @feat_frame.setter
+    def feat_frame(self, value):
+        if type(value) is not pd.DataFrame:
+            raise ValueError
+        self._feat_frame = value
+
     @discrete.setter
     def discrete(self, value):
         if type(value) is not pd.Index:
@@ -527,50 +619,38 @@
         self._cont = value
 
     def _set_discrete_cont(self):
-        nonzeros = self.feat_frame[self.nonzero_vars].copy()
+        nonzeros = self.feat_frame.copy().astype(float)
         remainder = nonzeros.round(0).sub(nonzeros).abs()
-        logger.debug(remainder)
+        logger.debug('Rounding remainders:\n{}'.format(remainder))
         self.discrete = nonzeros.columns[(remainder < self.options.tol_discrete).all()]
         self.cont = nonzeros.columns.symmetric_difference(self.discrete)
+        assert not self.discrete.empty
+        assert not self.cont.empty
+        pd.Index
         # logger.info('Discrete: {}', self.discrete)
         # logger.info('Continuous: {}', self.cont)
 
     @property
-    def corr_mat(self):
-        return self._corr_mat
-
-    # aweights is used to balance covariance estimates for different subsets of observations.
-    def _set_corr_mat(self, corr_method='pearson', filter_nan=False, *args, **kwargs):
-        _corr = self.feat_frame[self.dense].corr(method=corr_method)
-        np.cov()
-        if filter_nan:
-            # Remove features with no non-NaN correlations.
-            na_corr = _corr.isna().astype(int)
-            col_na = _corr[na_corr.sum(axis=0) < _corr.shape[1] - 1]
-            logger.info('Correlation matrix: {}'.format(_corr))
-            logger.info('NA value for cols: {}'.format(col_na))
-            logger.warning('These features have no valid correlations: {}'.format(col_na))
-            corr_matrix = _corr.loc[col_na, col_na]
+    def corr_pairs(self):
+        if self._corr_pairs is None:
+            self._corr_pairs = self.set_corr_pairs(corr_method='kendall')
         else:
-            corr_matrix = _corr
-        logger.debug(corr_matrix)
-        self._corr_mat = corr_matrix
-
-    @property
-    def corr_pairs(self):
-        return self._corr_pairs
+            return self._corr_pairs
 
-    @corr_pairs.setter
-    def corr_pairs(self, corr_method='kendall', *args, **kwargs):
-        corr_val = self.feat_frame.corr_mat(corr_method=corr_method, *args, **kwargs)
-        logger.info(corr_val, corr_val.shape)
+    def set_corr_pairs(self, corr_method='kendall', *args, **kwargs):
+        if self.corr_mat is None:
+            logger.warning('Covariance matrix has not been calculated.')
+            return None
+        else:
+            corr_val = self.feat_frame.corr_mat.copy(deep=True)
+        logger.info('{}\n{}'.format(corr_val, corr_val.shape))
         corr_sorted = corr_val.unstack().sort_values(kind='quicksort', ascending=False)
         logging.info('Sorted Correlation values:\n{}'.format(corr_sorted[:10]))
         corr_set = set()
         for i in range(0, self.X.dense.shape[1]):
             for j in range(0, i + 1):
                 corr_set.add((self.X.dense.columns[i], self.X.dense.columns[j]))
-        self._corr_pairs = corr_set
+        self.set_corr_pairs = corr_set
 
     @property
     def sparse(self):
@@ -610,8 +690,8 @@
             freq_cut = self.options.tol_sparse / self.X.vary.shape[0]
         else:
             raise ValueError
-        logger.info('Sparse tolerance: {}'.format(self.options.tol_sparse))
-        logger.info('Freq cut: {}'.format(freq_cut))
+        # logger.info('Sparse tolerance: {}'.format(self.options.tol_sparse))
+        # logger.info('Freq cut: {}'.format(freq_cut))
         sparse_list, dense_list, zero_list, freq_dict = list(), list(), list(), dict()
         for col, ser in self._pd_feat_freq(ignore_nan=ignore_nan):
             if all_freq:
@@ -636,7 +716,7 @@
     @cachetools.cached(cache={})
     def _pd_feat_freq(self, ignore_nan=False):
         # logger.info('Feature variances: ', self.feat_vars)
-        for col, ser in self.original.items():
+        for col, ser in self.feat_frame.items():
             yield col, ser.value_counts(normalize=True, dropna=ignore_nan)
 
 
@@ -665,3 +745,100 @@
 
 # train_feats = X_train.copy()
 # train_labels = train_y.copy()
+
+
+def rename_duplicates(X, y=None, indices='all', name='auto', sep='_', *args, **kwargs):
+    rename_dict = dict()
+    if indices == 'all' and name == 'auto':
+        raise ValueError('Must specify either indices or name.')
+    elif type(name) is list or type(name) is pd.Index:
+        if type(name) is pd.Index:
+            name_list = name.unique().tolist()
+        else:
+            name_list = list(set(name))
+        return_dict = dict()
+        if 'indices' != 'all':
+            for i, one_name in name_list:
+                return_dict[one_name] = rename_duplicates(X, y, indices=indices[i], name=one_name, sep=sep, *args, **kwargs)
+        else:
+            for one_name in name:
+                return_dict[one_name] = rename_duplicates(X, y, indices=indices, name=one_name, sep=sep, *args, **kwargs)
+        return return_dict
+    elif name == 'auto' and indices != 'all':
+        names = X.columns[indices].unique().tolist()
+        assert len(names) == 1
+        name = names[0]
+    if type(name) is str:
+        matches = X.index.get_indexer_for(target=[name])
+        use_ind = matches
+        if indices != 'all':
+            use_ind = [c for c in matches if c in indices]
+        j = 0
+        for i, ind in enumerate(use_ind):
+            assert X.columns[ind] == name
+            while '{}{}{}'.format(name, sep, j) in X.columns.tolist():
+                j += 1
+            # X.columns[ind] = '{}{}{}'.format(name, sep, j)
+            rename_dict[ind] = copy.deepcopy('{}{}{}'.format(name, sep, j))
+    else:
+        raise ValueError('Keyword "name" must be either string, iterable, or pd.Index')
+    return rename_dict
+
+
+def recursive_vif(X, y, indices, n_feats=2, **kwargs):
+    from sklearn.preprocessing import RobustScaler
+    ind = indices
+    if type(ind) is pd.Index:
+        ind = indices.tolist()
+    X_scaled = RobustScaler(with_centering=False, unit_variance=True).fit_transform(X)
+    remain = deepcopy(indices)
+    while len(indices.tolist()) > n_feats:
+        vif = [variance_inflation_factor(X_scaled.iloc[:, indices].to_numpy(), i) for i in
+                             range(len(ind))]
+        maxi = np.argmax(np.array(vif))
+        logger.debug('Eliminating feature #{} with a VIF of {}'.format(maxi, vif[maxi]))
+        remain.pop(maxi)
+    if type(indices) is pd.Index:
+        return pd.Index(remain)
+    else:
+        return remain
+
+
+def eliminate_feats(X, y, indices, score_func, param, mode=None, *args, **kwargs):
+    winners, losers = set(), set()
+    input_dict = {'random_state': 0, **kwargs}
+    score = score_func(X=X[indices], y=y, **input_dict)
+    if mode == 'kbest' or 'percent' in mode or mode is None:
+        n_feats = param
+        if 'percent' in mode:
+            n_feats = np.round(param * len(unique_ind))
+        n_feats = round(param)
+        if n_feats <= 0 or mode is None:
+            n_feats = 1
+            winners = indices[np.argsort(score, kind='stable')[-n_feats:]]
+    elif 'thresh' in mode or 'cut' in mode:
+        if np.sign(param) == -1:
+            logger.warning('Negative param of {} entered. Evaluated `score <= |param|`'.format(param))
+            winners = np.less_equal(score, -param)
+        elif np.sign(param) == 1 or np.sign(param) == 0:
+            winners = np.greater_equal(score, param)
+        winners = indices[winners]
+    else:
+        winners = set(indices[np.argmax(score)])
+    losers = indices[~winners]
+    return winners, losers
+
+def true_duplicates(X: pd.DataFrame, y=None, subset_names=None, **kwargs):
+    if subset_names is None:
+        subset_names = X.columns.tolist()
+    df = X.copy(deep=True)
+    dups = [a for a in df.columns[df.columns.duplicated()].unique() if a in subset_names]
+    rename_dict, drop_dict = dict(), dict()
+    for long_name in dups:
+        name_ind = X.index.get_indexer_for(target=[long_name])
+        true_dups = df.columns[df.T.duplicated(keep=False)]
+        true_ind = tuple(X[true_dups].index.get_indexer_for(target=[long_name]).tolist())
+        drop_dict[long_name] = true_ind
+        unique_ind = tuple([x for x in name_ind if x not in true_ind])
+        rename_dict.update(rename_duplicates(X, y, indices=unique_ind, name=long_name))
+    return drop_dict, rename_dict
Index: dmso_model_dev/qsar_readiness.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/qsar_readiness.py b/dmso_model_dev/qsar_readiness.py
deleted file mode 100644
--- a/dmso_model_dev/qsar_readiness.py	
+++ /dev/null	
@@ -1,270 +0,0 @@
-import ast
-import copy
-import os
-import pandas as pd
-import requests
-import DescriptorRequestor
-from constants import ALL_ELEMENTS, DISALLOWED, GROUP_ONES, GROUP_TWOS, HALOGENS, QSAR_COLUMNS
-import logging
-import re
-# https://hcd.rtpnc.epa.gov/api/stdizer?workflow=qsar-ready&smiles=
-
-
-def filter_by_size(smiles_list, carbon_cutoff=5, included: list = None, excluded=None):
-    long, short, confusers = list(), list(), list()
-    if included is None:
-        included = ['B', 'N', 'O', 'S', 'P', 'n', 's', 'o']
-    elif type(included) != list:
-        included = list(included)
-    carbs = ['c', 'C']
-    included.extend(carbs)
-    if excluded is None:
-        excluded = list()
-    for s in included:
-        for i in ALL_ELEMENTS:
-            if s in i and i not in included:
-                confusers.append(i)
-    if len(confusers) > 0:
-        excluded.extend(confusers)
-    print(type(ALL_ELEMENTS))
-    print(confusers)
-    smile: str
-    for smile in smiles_list:
-        atom_count = 0
-        test_smile = copy.deepcopy(smile)
-        for c in confusers:
-            test_smile = test_smile.replace(c, '')
-        # test_smile = re.sub(pattern='[*]', repl='', string=test_smile)
-        for atom in included:
-            atom_count += test_smile.count(atom)
-        if atom_count >= carbon_cutoff:
-            long.append(smile)
-        else:
-            short.append(smile)
-    return long, short
-
-
-def salts_and_intermetallics(smiles_list, intermetallics=False):
-    salt_list, intermetallics_list = list(), list()
-    halides = ['[{}-]'.format(x) for x in HALOGENS]
-    # print([*halides, *GROUP_ONES, *GROUP_TWOS])
-    intermetals = [x for x in DISALLOWED if x not in [*HALOGENS, *GROUP_ONES, *GROUP_TWOS]]
-    for smile in smiles_list:
-        #if '[' in smile:
-        print(len([x for x in [*halides, *GROUP_ONES, *GROUP_TWOS]]), smile)
-        if len([x for x in [*halides, *GROUP_ONES, *GROUP_TWOS] if x in smile]) > 0:
-            salt_list.append(smile)
-        elif len([x for x in DISALLOWED if '[{}+'.format(x) in smile]) > 0:
-            salt_list.append(smile)
-        elif len([x for x in DISALLOWED if '[{}-'.format(x) in smile]) > 0:
-            salt_list.append(smile)
-        elif intermetallics and len([x for x in intermetals if '[{}]'.format(x) in smile]) > 0:
-            intermetallics_list.append(smile)
-    return salt_list, intermetallics_list
-
-
-def load_qsar_response(qsar_csv_path):
-    print(qsar_csv_path)
-    if not os.path.isfile(qsar_csv_path):
-        logging.warning('File {} does not exist.'.format(qsar_csv_path))
-        return None
-    tmp_srs = list()
-    try:
-        with open(qsar_csv_path, encoding='utf-8') as po:
-            for ll in po.readlines():
-                try:
-                    if 'mol' in ll:
-                        llist = '{}{}'.format(re.split(", 'mol", ll)[0], '}')
-                        tmp_srs.append(pd.Series(ast.literal_eval(llist.removeprefix('['))))
-                    else:
-                        print(ll.split(','))
-                        tmp_srs.append(pd.Series(eval(ll.split(','))))
-                except SyntaxError:
-                    print('Error in QSAR Response\n', ll)
-            if len(tmp_srs) > 0:
-                div_df = pd.concat(tmp_srs, axis=1).T
-                levels = div_df.columns.nlevels
-                print(div_df.columns)
-                if levels > 1:
-                    print(levels)
-                    div_df = div_df[div_df.columns.droplevel(level=1)]
-    except TypeError:
-        div_df = pd.read_csv(qsar_csv_path)
-    # div_df.rename(columns=QSAR_COLUMNS, inplace=True)
-    print(div_df.columns)
-    if 'INCHI_KEY' in div_df.columns:
-        div_df.set_index(['INCHI_KEY'], drop=True, inplace=True)
-        if 'id' in div_df.columns:
-            div_df.drop(columns=['id'], inplace=True)
-            return div_df
-    elif len(llist) > 0:
-        return pd.concat([pd.Series(eval(i)) for i in llist], axis=1).T.rename(columns=QSAR_COLUMNS, inplace=True)
-    else:
-        print(qsar_csv_path)
-        raise TypeError
-
-
-def standardize_qsar(qsar_path, smiles_list):
-    smiles, jsmiles = list(), list()
-    grabber = None
-    if os.path.isfile(qsar_path):
-        with open(qsar_path, 'r', encoding='utf-8') as fo:
-            last_ind = fo.readlines().count('inchi')
-    else:
-        last_ind = 0
-    with open(qsar_path, 'a', encoding='utf-8') as fo:
-        grabber = DescriptorRequestor.QsarStdizer()
-        for response, smile in grabber.bulk_epa_call(smiles_list[last_ind:]):
-            jsmiles.append(response)
-            fo.write('{}\n'.format(response))
-    return jsmiles
-
-
-def qsar_standardizer_api(smiles_list, server="https://hcd.rtpnc.epa.gov/api/stdizer", temp_file=None, retry_limit=5,
-                          connect_timeout=10,
-                          response_timeout=300, complete=False, **kwargs) -> dict:
-    logging.disable(logging.INFO)
-    payload = dict()
-    qsar_dict: dict[str, str] = dict()
-    complete_dict = dict()
-    if type(smiles_list) is pd.Series:
-        smiles_list = smiles_list.copy().tolist()
-    # requester, ada = open_qsar_session(server_url=server, limit=retry_limit)
-    if len(smiles_list) == 0:
-        raise ValueError
-    with open_qsar_session(server_url=server, limit=retry_limit)[0] as requester:
-        logging.info('Starting QSAR-ready conversion calls.')
-        for i, smile in enumerate(smiles_list):
-            if smile == '' or not smile:
-                continue
-            payload['workflow'] = 'qsar-ready'
-            if smile is None or smile == "":
-                logging.warning('Empty SMILES string.')
-                continue
-            payload['smiles'] = str(smile)
-            '''
-            # prepped = requester.prepare_request(requests.Request(method='GET', url=server, params=payload))
-            # print('Regular URL: ', prepped.url)
-            if '%' in prepped.path_url or '*' in prepped.url:
-                # raise EncodingWarning
-                logging.warning('Bad SMILES input: {}'.format(prepped.url))
-                prepped.url = '{}={}'.format(prepped.url.rpartition('=')[0], payload['smiles'])
-                if '%' in prepped.path_url or '*' in prepped.url:
-                    qsar_dict[smile] = '400'
-                    continue
-            # payload['smiles'] = payload['smiles'].encode('utf-8')
-            '''
-            # req = requester.send(prepped, verify=True, timeout=(connect_timeout, response_timeout))
-            # url = '{}?workflow=qsar-ready&smiles={}'.format(server, smile)
-            success = 0
-            j = 0
-            while success == 0 and j < retry_limit:
-                try:
-                    packet = requester.get(url=server, params=payload, timeout=(connect_timeout, response_timeout))
-                    j += 1
-                except requests.exceptions.ReadTimeout:
-                    response_timeout += 15
-                    try:
-                        packet = requester.get(url=server, params=payload, timeout=(connect_timeout, response_timeout))
-                        j += 1
-                    except requests.exceptions.ReadTimeout:
-                        logging.error('ReadTimeout Error on SMILES: ', smile)
-                        qsar_dict[smile] = '443'
-                        complete_dict[smile] = packet.status_code
-                        continue
-                except ConnectionError:
-                    logging.error('Connection Error.')
-                    continue
-                if packet is None:
-                    logging.error('No packet received from server.')
-                    j += 1
-                    continue
-                elif (packet.status_code == 200 or packet.status_code == '200') and packet.json() is not None and len(packet.json()) > 0:
-                    try:
-                        print(packet.json())
-                        complete_dict[smile] = packet.json()
-                        qsar_dict[smile] = packet.json()[0]['smiles']
-                        logging.info('New smiles: {}'.format(packet.json()))
-                        if temp_file:
-                            with open(temp_file, 'a', encoding='utf-8') as tf:
-                                tf.write('{}, {}'.format(smile, packet.json()))
-                                success = 1
-                                # Begin next SMILES request.
-                    except requests.exceptions.JSONDecodeError:
-                        logging.error(packet.status_code, i, smile)
-                        qsar_dict[smile] = str(packet.status_code)
-                        complete_dict[smile] = packet.status_code
-                    except SystemError:
-                        logging.error('Temporary file not found. Aborting run. Filepath: {}'.format(temp_file))
-                        if complete:
-                            return complete_dict
-                        else:
-                            return qsar_dict
-                elif packet.status_code == 404:
-                    logging.error('Server could not be found. {}'.format(server))
-                    raise NotImplementedError
-                elif (packet.status_code == 504) or (packet.status_code == '504'):
-                    logging.error('Server Timeout')
-                    time.sleep(30)
-                    requester, ada = open_qsar_session(server_url=server, limit=retry_limit)
-                else:
-                    try:
-                        time.sleep(30)
-                        logging.error([smile, packet.json()])
-                        continue
-                    except:
-                        logging.error(smile)
-                        qsar_dict[smile] = str(packet.status_code)
-                        continue
-    print('Completed querying {} compounds'.format(len(smiles_list)))
-    if complete:
-        return complete_dict
-    else:
-        return qsar_dict
-
-
-def smiles_to_inchi(smiles, qsar_filename, connect_timeout, response_timeout):
-    # GET / InChI.asmx / SMILESToInChI?smiles = string
-    # HTTP / 1.1
-    host = 'www.chemspider.com/INCHI.asmx/INCHIToInChI'
-    requester = requests.Session()
-    payload = dict()
-    if type(smiles) is str:
-        smiles = [smiles]
-    i = 1
-    qend = 0
-    with open(qsar_filename, 'r') as qn:
-        while True:
-            l = qn.read(-i)
-            if l == '':
-                i += 1
-                continue
-            elif l.split()[0] in smiles:
-                qend = smiles.index(l) - 1
-                break
-            i += 1
-    for smile in smiles[qend:]:
-        payload['smiles'] = smile
-        req = requester.get(url=host, params=payload, timeout=(connect_timeout, response_timeout))
-        with open(qsar_filename, 'a', encoding='utf-8') as qf:
-            qf.write('{}, {}'.format(smile, req))
-
-
-def qsar_csv_to_df(path=None, qsar_name=None, csv_path=None):
-    if not os.path.isfile(csv_path):
-        csv_path = '{}{}.csv'.format(path, qsar_name)
-    if not os.path.isfile(csv_path):
-        raise FileExistsError
-    log_name = 'duplicates_{}'.format(qsar_name)
-    with open(csv_path, encoding='utf-8') as co:
-        slist = [pd.Series(eval('{}{}'.format(re.split(", 'mol", i)[0], '}'))) for i in co.readlines() if (type(i) is str and 'mol' in i) or type(i) is dict]
-    sdf.drop(columns=['id'], inplace=True, errors='ignore')
-    sdf.rename(columns=QSAR_COLUMNS, inplace=True)
-    return sdf
-
-
-def filter_qsar_df(qsar_df, path, qsar_name):
-    qsar_df.set_index(keys='INCHI_KEY', drop=False, inplace=True)
-    repeats = qsar_df[qsar_df.duplicated(subset='INCHI_KEY')]
-    repeats.to_csv(path_or_buf=repeats, index=False, encoding='utf-8')
-    sdf.drop_duplicates(subset='INCHI_KEY', inplace=True)
Index: dmso_model_dev/dmso_data/dmso_pipeline.py
===================================================================
diff --git a/dmso_model_dev/dmso_data/dmso_pipeline.py b/dmso_model_dev/dmso_data/dmso_pipeline.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/dmso_data/dmso_pipeline.py	
@@ -0,0 +1,38 @@
+import numpy as np
+import pandas as pd
+import sklearn.utils.sparsefuncs_fast
+from sklearn.pipeline import Pipeline, FunctionTransformer
+from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector, SelectKBest, VarianceThreshold
+from sklearn.compose import ColumnTransformer, make_column_selector
+from sklearn.preprocessing import RobustScaler, PowerTransformer
+from sklearn.metrics import balanced_accuracy_score, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay, \
+    silhouette_score
+from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV
+from sklearn.utils import check_X_y, compute_sample_weight, compute_class_weight
+from sklearn.neighbors import RadiusNeighborsClassifier
+from scipy.stats import kurtosis, skew
+from sklearn.decomposition import IncrementalPCA, TruncatedSVD
+
+def low_var_feats(var_cut=0.05, quants=(0.1, 0.9), memlib=None):
+
+    robust = RobustScaler(quantile_range=quants)
+    X_var = np.var()
+    low_var = SelectKBest()
+
+    # VarianceThreshold(threshold=var_cut)
+    low_var_pipe = Pipeline(steps=[('robust', robust),
+                                   ('low_var', low_var)
+                                   ])
+    return low_var_pipe
+
+
+def column_processing(cols_one, cols_two):
+    selector = SequentialFeatureSelector(estimator=score_f, scoring='balanced_accuracy',
+                                         n_jobs=-1)
+
+    score_f = ElasticNetCV(n_jobs=-1, random_state=0, max_iter=5000, tol=1e-5, cv=3)
+    col_trans = ColumnTransformer(transformers=[
+        ('cont_pipeline', cont_trans, cols_one),
+        ('disc_pipeline', disc_trans, cols_two)],
+    )
+    return col_trans
Index: dmso_model_dev/models/logistic_regression.py
===================================================================
diff --git a/dmso_model_dev/models/logistic_regression.py b/dmso_model_dev/models/logistic_regression.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/models/logistic_regression.py	
@@ -0,0 +1,9 @@
+import numpy as np
+import pandas as pd
+from sklearn.pipeline import Pipeline
+from sklearn.preprocessing import RobustScaler
+from sklearn.feature_selection import VarianceThreshold, SelectKBest
+from sklearn.linear_model import LogisticRegressionCV
+
+def get_best_features():
+    return
\ No newline at end of file
Index: dmso_model_dev/data_handling/verification.py
===================================================================
diff --git a/dmso_model_dev/data_handling/verification.py b/dmso_model_dev/data_handling/verification.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/verification.py	
@@ -0,0 +1,29 @@
+import itertools
+
+import data_importing
+import rdkit
+import rdkit.Chem
+
+
+def verify_reps(*rep_list):
+    # smiles_canoned = rdkit.Chem.CanonSmiles(input_smiles)
+    key_list = list()
+    for rep in rep_list:
+        key_list.append(_get_inchi_key(rep))
+    if len(key_list) > 0:
+        return all([i == j for i, j in itertools.pairwise(key_list)])
+    else:
+        return False
+
+
+def _get_inchi_key(input_str):
+    in_type = data_importing.get_data_column(input_str)
+    if in_type == "INCHI":
+        out_str = rdkit.Chem.InchiToInchiKey(input_str)
+    elif in_type == "SMILES_CANONICAL" or in_type == 'SMILES_QSAR':
+        out_str = rdkit.Chem.MolToInchiKey(rdkit.Chem.MolFromSmiles(input_str))
+    else:
+        out_str = None
+    return out_str
+
+
Index: dmso_model_dev/data_handling/data_splitter.py
===================================================================
diff --git a/dmso_model_dev/data_handling/data_splitter.py b/dmso_model_dev/data_handling/data_splitter.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/data_splitter.py	
@@ -0,0 +1,6 @@
+# Split samples according to label(s).
+# Labels may series or column, value pairs.
+# Values can be scalar, ranges, slices, or callables. Must be compatible with Pandas indexing.
+# Split ratios can be specified for each label. In the case of multiple labels, this can be label dependent.
+def split_samples():
+    return True
Index: dmso_model_dev/models/kNN_clf.py
===================================================================
diff --git a/dmso_model_dev/models/kNN_clf.py b/dmso_model_dev/models/kNN_clf.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/models/kNN_clf.py	
@@ -0,0 +1,7 @@
+import numpy as np
+import pandas as pd
+from sklearn.neighbors import KNeighborsClassifier
+from sklearn.pipeline import Pipeline
+
+knn_clf = KNeighborsClassifier()
+knn_pipe = Pipeline(steps=[('knn', knn_clf)])
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"Black\">\r\n    <option name=\"sdkName\" value=\"Python 3.12 (compLoiel)\" />\r\n  </component>\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.12 (compLoiel)\" project-jdk-type=\"Python SDK\" />\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	
+++ b/.idea/misc.xml	
@@ -4,4 +4,7 @@
     <option name="sdkName" value="Python 3.12 (compLoiel)" />
   </component>
   <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.12 (compLoiel)" project-jdk-type="Python SDK" />
+  <component name="PyCharmProfessionalAdvertiser">
+    <option name="shown" value="true" />
+  </component>
 </project>
\ No newline at end of file
Index: test/test_qsar_readiness.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from unittest import TestCase\r\n\r\nimport qsar_readiness\r\nfrom qsar_readiness import filter_by_size\r\n\r\nCHARGED_METALS = ['N#C[Au-]C#N', 'C1C=CC=C1[Ti+2]C1CC=CC=1]']\r\nINTERMETAL = ['Cl[Bi](Cl)(C1=CC=CC=C1)(C1=CC=CC=C1)C1=CC=CC=C1', 'CCCC[Sn]1(CCCC)OC(=O)C=CC(=O)O1',\r\n              'Cl[V](Cl)(C1C=CC=C1)C1C=CC=C1', 'Cl[Zr](Cl)(C1C=CC=C1)C1CC=CC=1']\r\nVALID_WEIRD = ['C1OC2=CC(CCNC3=NC=NC4NN=CC=43)=CC=C2O1',\r\n               'C1SC=CC=1C1=NC(=C2C=CC=CC2=N1)N1C=NC=C1',\r\n               'C=C(Cl)CN1N=C(OC1=O)C1C=CC(F)=CC=1']\r\nVALID_SHORT = ['CCO', 'COCC', 'NCOC(Cl)', '(Cl)ClC=C(Cl)Cl', 'c(F)ccn']\r\nSALTS = ['[Na+].COS([O-])(=O)=O', '[Br-].CCCCCCCCCCCCCC[N+]1=CC=CC=C1', '[Cu++].[O-]N1C=CC=CC1=S.[O-]N1C=CC=CC1=S',\r\n         '[Li+].[O-]S(=O)(=O)C(F)(F)F', '[K+].OC1=CC=C(O)C(=C1)S([O-])(=O)=O',\r\n         '[Co+3].[C-]#N.[H]C1(O)[C@H](OP([O-])(=O)O[C@H](C)CNC(=O)CC[C@]2(C)[C@@H](CC(N)=O)[C@@]3([H])[N-]\\\\C2=C(C)/C2=N/C(=C\\\\C4=N\\\\C(=C(C)/C5=N[C@]3(C)[C@@](C)(CC(N)=O)[C@@H]5CCC(N)=O)\\\\[C@@](C)(CC(N)=O)[C@@H]4CCC(N)=O)/C(C)(C)[C@@H]2CCC(N)=O)[C@@H](CO)O[C@@H]1N1C=NC2=C1C=C(C)C(C)=C2']\r\nIONIC = ['Cl[Rh](Cl)Cl', 'C[Si]1(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O1', 'O=[Sm]O[Sm]=O']\r\n\r\n\r\nclass SaltsAndIntermetallicsTest(TestCase):\r\n\r\n    def test_intermetals(self):\r\n        intermetal = ['Cl[V](Cl)(C1C=CC=C1)C1C=CC=C1', 'Cl[Zr](Cl)(C1C=CC=C1)C1CC=CC=1', 'CC[Co]CC']\r\n        long_inter, short_inter = filter_by_size(intermetal, carbon_cutoff=5)\r\n        assert long_inter == intermetal[:2], print('Expected {}, got {}'.format(intermetal[:2], long_inter))\r\n        assert short_inter == [intermetal[2]], print('Expected {}, got {}'.format(intermetal[2], short_inter))\r\n        del long_inter, short_inter\r\n\r\n    def test_valid_long(self):\r\n        long_valid, short_valid = filter_by_size(VALID_WEIRD, carbon_cutoff=5)\r\n        assert long_valid == VALID_WEIRD, print('Expected {}, got {}'.format(VALID_WEIRD, long_valid))\r\n        assert short_valid == [], print('Expected {}, got {}'.format([], short_valid))\r\n        del long_valid, short_valid\r\n\r\n    def test_size_filter(self):\r\n        long_valid, short_valid = filter_by_size(VALID_SHORT, carbon_cutoff=5)\r\n        assert len(long_valid) == 0, print('Expected [], got {}'.format(long_valid))\r\n        assert short_valid == VALID_SHORT, print('Expected {}, got {}'.format(VALID_SHORT, short_valid))\r\n        del long_valid, short_valid\r\n\r\n    def test_intermetals_only_test(self):\r\n        MIXED_INTERMETALS = [*INTERMETAL, *VALID_WEIRD]\r\n        salts, intermetals = qsar_readiness.salts_and_intermetallics(MIXED_INTERMETALS, True)\r\n        assert len(salts) == 0, 'Expected no salts found, {} returned!'.format(salts)\r\n        assert len([x for x in intermetals if x not in INTERMETAL]) == 0, 'Incorrectly classified non-intermetallics!'\r\n        assert len([x for x in intermetals if x in INTERMETAL]) == len(\r\n            INTERMETAL), 'Failed to find all intermetallics: {}'.format([x for x in INTERMETAL if x not in intermetals])\r\n        del salts, intermetals\r\n\r\n    def test_salt_only(self):\r\n        MIXED_SALTS = [*SALTS, *INTERMETAL, *VALID_WEIRD]\r\n        salts, intermetals = qsar_readiness.salts_and_intermetallics(MIXED_SALTS, False)\r\n        assert len(intermetals) == 0, 'Expected no intermetallics found, {} returned!'.format(intermetals)\r\n        assert len([x for x in salts if x not in SALTS]) == 0, 'Incorrectly classified non-salts!'\r\n        assert len([x for x in salts if x in SALTS]) == len(\r\n            SALTS), 'Failed to find all salts. \\nMissed: {}\\nFound: {}\\n'.format(\r\n            [x for x in SALTS if x not in salts], [x for x in SALTS if x in salts])\r\n        del salts, intermetals\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/test_qsar_readiness.py b/test/test_qsar_readiness.py
--- a/test/test_qsar_readiness.py	
+++ b/test/test_qsar_readiness.py	
@@ -1,21 +1,25 @@
 from unittest import TestCase
 
+import pandas as pd
+
 import qsar_readiness
 from qsar_readiness import filter_by_size
 
 CHARGED_METALS = ['N#C[Au-]C#N', 'C1C=CC=C1[Ti+2]C1CC=CC=1]']
 INTERMETAL = ['Cl[Bi](Cl)(C1=CC=CC=C1)(C1=CC=CC=C1)C1=CC=CC=C1', 'CCCC[Sn]1(CCCC)OC(=O)C=CC(=O)O1',
-              'Cl[V](Cl)(C1C=CC=C1)C1C=CC=C1', 'Cl[Zr](Cl)(C1C=CC=C1)C1CC=CC=1']
+              'Cl[V](Cl)(C1C=CC=C1)C1C=CC=C1', 'Cl[Zr](Cl)(C1C=CC=C1)C1CC=CC=1', 'CCCC[Mg]C(CC)C', 'CC[Hg]N1C(=O)C2C3(C(Cl)(Cl)C(Cl)(C2C1=O)C(Cl)=C3Cl)Cl',   'O[As](C1C=CC([N+](=O)[O-])=CC=1)(=O)O',
+]
 VALID_WEIRD = ['C1OC2=CC(CCNC3=NC=NC4NN=CC=43)=CC=C2O1',
                'C1SC=CC=1C1=NC(=C2C=CC=CC2=N1)N1C=NC=C1',
                'C=C(Cl)CN1N=C(OC1=O)C1C=CC(F)=CC=1']
 VALID_SHORT = ['CCO', 'COCC', 'NCOC(Cl)', '(Cl)ClC=C(Cl)Cl', 'c(F)ccn']
 SALTS = ['[Na+].COS([O-])(=O)=O', '[Br-].CCCCCCCCCCCCCC[N+]1=CC=CC=C1', '[Cu++].[O-]N1C=CC=CC1=S.[O-]N1C=CC=CC1=S',
-         '[Li+].[O-]S(=O)(=O)C(F)(F)F', '[K+].OC1=CC=C(O)C(=C1)S([O-])(=O)=O',
+         '[Li+].[O-]S(=O)(=O)C(F)(F)F', '[K+].OC1=CC=C(O)C(=C1)S([O-])(=O)=O', 'N#C[Au-]C#N', 'CCCC[Sn+2]CCCC'
          '[Co+3].[C-]#N.[H]C1(O)[C@H](OP([O-])(=O)O[C@H](C)CNC(=O)CC[C@]2(C)[C@@H](CC(N)=O)[C@@]3([H])[N-]\\C2=C(C)/C2=N/C(=C\\C4=N\\C(=C(C)/C5=N[C@]3(C)[C@@](C)(CC(N)=O)[C@@H]5CCC(N)=O)\\[C@@](C)(CC(N)=O)[C@@H]4CCC(N)=O)/C(C)(C)[C@@H]2CCC(N)=O)[C@@H](CO)O[C@@H]1N1C=NC2=C1C=C(C)C(C)=C2']
 IONIC = ['Cl[Rh](Cl)Cl', 'C[Si]1(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O[Si](C)(C)O1', 'O=[Sm]O[Sm]=O']
 
 
+
 class SaltsAndIntermetallicsTest(TestCase):
 
     def test_intermetals(self):
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This is a sample Python script.\r\n\r\n# Press Shift+F10 to execute it or replace it with your code.\r\n# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\r\n\r\n\r\ndef print_hi(name):\r\n    # Use a breakpoint in the code line below to debug your script.\r\n    print(f'Hi, {name}')  # Press Ctrl+F8 to toggle the breakpoint.\r\n\r\n\r\n# Press the green button in the gutter to run the script.\r\nif __name__ == '__main__':\r\n    print_hi()\r\n\r\n# See PyCharm help at https://www.jetbrains.com/help/pycharm/\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	
+++ b/main.py	
@@ -1,16 +1,6 @@
-# This is a sample Python script.
-
-# Press Shift+F10 to execute it or replace it with your code.
-# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.
-
-
-def print_hi(name):
-    # Use a breakpoint in the code line below to debug your script.
-    print(f'Hi, {name}')  # Press Ctrl+F8 to toggle the breakpoint.
 
 
 # Press the green button in the gutter to run the script.
 if __name__ == '__main__':
-    print_hi()
+    pass
 
-# See PyCharm help at https://www.jetbrains.com/help/pycharm/
Index: dmso_model_dev/data_handling/descriptor_preprocessing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import copy\r\nimport dataclasses\r\nimport functools\r\nimport glob\r\nimport itertools\r\nimport logging\r\nimport os.path\r\nfrom typing import List, Tuple\r\nfrom dataclasses import dataclass\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport plotly\r\nimport plotly.graph_objects as go\r\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif\r\nimport padel_categorization\r\nfrom constants import *\r\n\r\n\r\ndef parse_raw_api_output(api_path, desc_names=None):\r\n    if not os.path.isfile(api_path):\r\n        logging.error('{} is not a valid file path!'.format(api_path))\r\n        raise FileExistsError\r\n    with open(api_path, encoding='utf-8') as f:\r\n        cmpd_list = [dict(zip(DESC_COLUMNS, eval(s.removeprefix('dict_values')))) for s in f.readlines() if\r\n                     s.startswith('dict_values')]\r\n    descriptor_meta_df = pd.DataFrame.from_records(cmpd_list, index='INCHI_KEY')\r\n    return descriptor_meta_df\r\n\r\n\r\n# TODO: Prevent/check for conflicting group_kwargs (ex. 'soluble' and 'insoluble')\r\ndef group_filenames(data_dir, group_kwargs, file_format='.pkl', multi_df=False, multigroup=False):\r\n    group_dict = dict()\r\n    file_list = [x for x in glob.glob(file_format, root_dir=data_dir)]\r\n    for group_name in group_kwargs:\r\n        for orig_file in file_list:\r\n            file = copy.deepcopy(orig_file)\r\n            os.removedirs(file)\r\n            file_name = file.removesuffix(file_format)\r\n            name_list = list()\r\n            group_dict[group_name] = name_list\r\n            if group_name.upper() in file_name.upper():\r\n                name_list.append(orig_file)\r\n            if not multi_df:\r\n                break\r\n    return group_dict\r\n\r\n\r\ndef _list_pkl_loader(nested):\r\n    tmp_list = list()\r\n    for f in nested:\r\n        if type(f) is list or type(f) is tuple or type(f) is set:\r\n            tmp_list.append(_list_pkl_loader(f))\r\n        else:\r\n            tmp_list.append(pd.read_pickle(f))\r\n    return tmp_list\r\n\r\n\r\ndef load_file_groups(data_dir, group_kwargs, file_format='.pkl', multigroup=False, multi_df=False):\r\n    file_groups = group_filenames(data_dir, group_kwargs, file_format, multigroup=multigroup, multi_df=multi_df)\r\n    groups_list = dict()\r\n    for groupname, file_list in file_groups.items():\r\n        if len(file_list) > 0 and multi_df == 'merge':\r\n            groups_list.update([pd.concat(_list_pkl_loader(file_list), axis=1)])\r\n        else:\r\n            groups_list.update(_list_pkl_loader(file_list))\r\n    return groups_list\r\n\r\n\r\ndef zero_var_filter(df_list):\r\n    print(df_list[0])\r\n    zero_vars, df_vars = list(), list()\r\n    if len(df_list) > 1:\r\n        all_sets = pd.concat(df_list, axis=1)\r\n    else:\r\n        all_sets = df_list[0]\r\n    all_var = all_sets.var(axis=1, numeric_only=True)\r\n    print(all_var)\r\n    # zero_vars = all_var[all_var == 0]\r\n    for df_i in df_list:\r\n        df_vars.append(df_i.var(axis=1))\r\n        # zero_vars.append(df_i.copy().drop(all_var[all_var == 0].index))\r\n    return zero_vars, all_var, df_vars\r\n\r\n# Logic: pair_dfs loops through dfs and returns a [dict[df_index_tup, dfs], DataFrame[pairs, func]]. The elements of DFs are generators of the column's func callable for the concatenated data in the DF pair.\r\ndef generate_df_funcs(df, funcs):\r\n    \"\"\"\r\n    Returns a list of tuples composed of the original DataFrame and a Series for each of funcs.\r\n    :param funcs: Callable or iterable of callables to apply to df.\r\n    :return: list[tuple[DataFrame, Series]], where Series is {column names: variances} of DataFrame\r\n    :type df: pd.DataFrame\r\n    \"\"\"\r\n    df_func_list, func_list = list(), dict()\r\n    if np.lib.iterable(funcs):\r\n        for func in funcs:\r\n            df_func_list.append()\r\n        func_list.update([(col_name, ser.var()) for col_name, ser in df.items()])\r\n        df_var_list.append((df, pd.Series(data=func_list)))\r\n    return df_func_list\r\n\r\n    def _generator_df_func(df, func, cols=None):\r\n        if not cols:\r\n            cols = [c for c in cols if c in df.columns]\r\n        part_func = functools.partial(__func=func)\r\n        for col, ser in df.items():\r\n\r\n            return part_func(df[col].copy(deep=True))\r\n\r\ndef pair_dfs(dfs, paired_funcs=None):\r\n    dup_list = list()\r\n    for (di_1, df1), (di_2, df2) in list(itertools.combinations(enumerate(dfs), r=2)):\r\n        if df1.equals(df2) and di_2 not in dup_list:\r\n            dup_list.append(di_2)\r\n    [dfs.remove(x) for x in dup_list]\r\n    cols = common_columns(dfs)\r\n    if not pair_tups and paired_funcs:\r\n        assert np.lib.iterable(paired_funcs) and any([type(f) is callable() or type(f) is pd.DataFrame or type(f) is pd.Series for f in dfs])\r\n        pair_tups = list()\r\n        if type(paired_funcs) is dict:\r\n            funcs = paired_funcs.values()\r\n        elif type(paired_funcs) is not dict and np.iterable(paired_funcs):\r\n\r\n    for\r\n        for (df1, df1_var), (df2, df2_var) in pair_tups:\r\n            var_one_array = [col_name for col_name, x in df1_var.items() if np.lib.iterable(x)]\r\n            var_two_array = [col_name for col_name, x in df2_var.items() if np.lib.iterable(x)]\r\n    else:\r\n        df_vars = [(df_i, df, df.var()) for df_i, df in enumerate(dfs)]\r\n        yield itertools.combinations(df_vars, r=2)\r\n\r\n\r\n\r\ndef pair_dfs(df_list, method):\r\n    df_pair_list = list()\r\n    if method == 'auto' or method == 'all':\r\n        df_pair_list = itertools.combinations(df_list, r=2)\r\n    elif callable(method):\r\n        df_pair_list = method(df_list)\r\n    elif np.iterable(method) and all([np.iterable(pairs) for pairs in method]) and all([len(pairs) == 2 for pairs in method]):\r\n        for pair in method:\r\n            if all([type(p) is pd.DataFrame for p in pair]):\r\n                df_pair_list.append(pair)\r\n            elif all([type(p) is int and (0 < p < len(df_list)) for p in pair]):\r\n                df_pair_list.append((df_list[pair[0]], df_list[pair[1]]))\r\n            else:\r\n                raise TypeError\r\n            yield df_pair_list\r\n\r\n\r\ndef pairwise_stats(df_list, indices, func, *args, **kwargs):\r\n    import operator\r\n    return_dict = dict([(new_key, list()) for new_key in df_list])\r\n    if type(func) is str:\r\n        op = operator.methodcaller(func, *args, **kwargs)\r\n    else:\r\n        op = functools.partial(func=func, *args, **kwargs)\r\n    for i_one, i_two in indices:\r\n        df1, df2 = df_list[i_one], df_list[i_two]\r\n        return_dict[i_one, i_two] = op([df1, df2])\r\n    return pd.Series(return_dict)\r\n\r\n\r\ndef common_columns(dfs=None, cols=None, excluded=None, use_same_columns=False):\r\n    # TODO: Add handling for mixed types in dfs parameters (ex. list[DataFrame, DataFrame, tuple(DataFrame, DataFrame)]\r\n    # TODO: Add handling for iterable cols and excluded. Check match with dimensions of dfs and each other. Allow scalar inputs for either.\r\n    assert np.lib.iterable(dfs), 'dfs parameter must be an iteraable.'\r\n    if type(dfs) is pd.DataFrame:\r\n        assert not dfs.empty, 'DF is empty!'\r\n        dfs = [dfs]\r\n    elif any([type(df) is not pd.DataFrame for df in dfs]):\r\n        return [common_columns(dfs=df_list, cols=cols, excluded=excluded, use_same_columns=use_same_columns) for df_list in dfs]\r\n    common_cols = dfs[0].columns.intersection(dfs[1].columns)\r\n    for dft in dfs[2:]:\r\n        common_cols = common_cols.intersection(dft.columns)\r\n        # cols = set([[x for x in df.columns.to_list() if x not in exclude] for df in dfs])\r\n    if excluded is not None:\r\n        [common_cols.remove(x) for x in excluded if x in common_cols]\r\n    if cols:\r\n        common_cols = [these_cols for these_cols in common_cols if all([these_cols in df.columns for df in dfs])]\r\n    return common_cols\r\n\r\ndef ks_stats(dfs=None, df_pairs=None, var_tup_pairs=None, use_cols=None, exclude=None, use_same_columns=False):\r\n    #https://stackoverflow.com/questions/62088255/scipys-ks-2samp-function-gives-good-d-statistic-but-wrong-p-value?rq=3\r\n    from scipy.stats import ks_2samp\r\n    from scipy.stats._stats_py import KstestResult,\r\n    if not dfs and df_pairs:\r\n        dfs = list(itertools.chain.from_iterable(df_pairs))\r\n    elif df_pairs:\r\n        pair_dfs(dfs)\r\n    else:\r\n        raise ValueError\r\n    ks_cols, ks_pvals_list  = dict(), list()\r\n    # TODO: Change loop ordering to allow DF index to be entered into resulting DataFrame/Series.\r\n    for col in cols:\r\n        pvals = dict()\r\n        for df1_tup, df2_tup in enumerate(var_tup_pairs):\r\n            di_1, df1, df1_var = df1_tup\r\n            di_2, df2, df2_var = df1_tup\r\n            if not np.lib.iterable(df1_var[col]) or not np.lib.iterable(df2_var[col]):\r\n                print(col)\r\n            elif df1_var[col].var() <= 0.001 and df2_var[col].var() <= 0.001:\r\n                ks_pvals_list.append({col: pd.Series(np.NaN)})\r\n                continue\r\n            # _, ks_pvals_list.append(pd.Series(ks_2samp(data1=df1[col], data2=df2[col]))\r\n            assert col in df1.columns.tolist() and col in df2.columns.tolist(), \"{} not found in DataFrame\".format(col)\r\n            # print(df1[df1[col].isna()], df2[df2[col].isna()])\r\n            results = ks_2samp(data1=df1[col], data2=df2[col], method='asymp')\r\n            try:\r\n                if type(results) is KstestResult:\r\n                    ks_pvals_list.append({col: pd.Series(results[1])})\r\n                elif np.lib.iterable(results) and type(results[1]) is pd.Series:\r\n                    ks_pvals_list.append({col: pd.Series(results[1].values[1])})\r\n                elif type(results) is pd.Series:\r\n                    ks_pvals_list.append({col: pd.Series(results.values[1])})\r\n                elif np.lib.iterable(results) and type(results[1]) is tuple:\r\n                    ks_pvals_list.append({col: pd.Series(results[1][1])})\r\n                    print('Nested tuple for {}'.format(results))\r\n                elif type(results) is np.float64:\r\n                    ks_pvals_list.append({col: pd.Series(results)})\r\n                    print('Lone float value returned: {}'.format(results))\r\n            except:\r\n                print(type(results))\r\n                ks_pvals_list.append({col: pd.Series(results)})\r\n                print('Something else went wrong!!!:\\n{}'.format(results))\r\n            '''\r\n            if np.any(type(results) is tuple) and len(results) > 1 or type(results) is np.lib.iterable:\r\n                ks_pvals_list.append({col: pd.Series(results[1])\r\n            elif type(results) is object:\r\n                ks_pvals_list.append({col: pd.Series(results.pvalue)\r\n            elif np.isreal(results) or type(results) is np.float64 or type(results) is float:\r\n                ks_pvals_list.append({col: pd.Series(results)\r\n            else:\r\n                #print('KS results for {} is not a recognized tuple: {}'.format(col, results))\r\n                ks_pvals_list.append({col: pd.Series(np.nanmin(results.pvalue))\r\n                print(pvals[d_i])\r\n            # assert type(pvals[d_i]) is float or type(pvals[d_i]) is int, \"KS results not recognized\"\r\n            if not np.isreal(pvals[d_i]) and pvals[d_i] < 0.0:\r\n                print(pvals[d_i], col)\r\n                print(results, col, df_pairs[0][col])\r\n                raise ValueError\r\n            '''\r\n    ks_cols = pd.DataFrame.from_dict(orient='records', dtype=np.float64)\r\n    return ks_cols\r\n\r\n\r\ndef _pooled_vars(df_list):\r\n    if type(df_list) is pd.DataFrame:\r\n        combo_df = df_list\r\n    else:\r\n        combo_df = pd.concat(df_list)\r\n    var_pool =  pd.Series([(col_name, x.var()) for col_name, x in combo_df.items()])\r\n    return var_pool\r\n\r\n\r\ndef get_pooled_vars(df_list, method='all', *args, **kwargs):\r\n    return pooled_function(df_list, 'var', method=method, *args, **kwargs)\r\n\r\n\r\ndef pooled_function(df_list, operation, method='all', return_dict=None, *args, **kwargs):\r\n    \"\"\"\r\n    Applies a function or method to a [nested] list of DataFrames separately or concatenated with all other DataFrames or a subset. Returns them as a dictionary whose keys are list indices and values are tuples of list indices of DataFrames input into the function and the function's result.\r\n    :param df_list: Flat list of DataFrames\r\n    :type operation: method | function\r\n    :param operation: str, Name of DF method to be applied. | function\r\n    :param method: Which combination of DataFrames to use as inputs.\r\n        'separate': Include each DataFrame evaluated separately.\r\n        'not_combined': Do not use all DataFrames.\r\n        list[list[_T]]: Nested list of df_list indices.\r\n    :param return_dict: Key is index of df_list. Value is nested tuple representing indices used in each calculation and the result.\r\n    :return: return_dict: dict[int, tuple[tuple[int], Any]]]\r\n    \"\"\"\r\n    # TODO: Support both df_list indices and DataFrames in method.\r\n    import operator\r\n    if type(return_dict) is not dict:\r\n        return_dict = dict([(new_key, list()) for new_key in df_list])\r\n    if type(operation) is str:\r\n        op = operator.methodcaller(operation, *args, **kwargs)\r\n    else:\r\n        op = operation\r\n    all_dfs = list(itertools.chain.from_iterable(df_list))\r\n    if not np.lib.iterable(df_list):\r\n        df_list = [df_list]\r\n    for elem in df_list:\r\n        if np.lib.iterable(elem):\r\n            pooled_function(elem, operation=operation, method=method, return_dict=return_dict, *args, **kwargs)\r\n        else:\r\n            if 'not_combined' not in method and 'none' not in method:\r\n                pooled = op(pd.concat(all_dfs))\r\n                [return_dict[df_i].append((list(range(len(all_dfs))), pooled)) for df_i, single_df in enumerate(all_dfs)]\r\n            if 'separate' in method:\r\n                [return_dict[df_i].append((df_i, op(single_df))) for df_i, single_df in enumerate(df_list)]\r\n    return return_dict\r\n\r\n'''\r\n@dataclass\r\nclass WorkflowOptions:\r\n    var_pooling: str\r\n    pairing: str | tuple[tuple[pd.DataFrame, pd.DataFrame]] | [callable]\r\n    pair_vars: str\r\n'''\r\n\r\ndef basic_workflow(desc_dfs, var_pooling='all', pairing='auto', pair_vars='separate'):\r\n    \"\"\"\r\n    Basic data wrangling workflow for QSAR modeling.\r\n    :param desc_dfs: List of DataFrames containing descriptors.\r\n    :param var_pooling: How to calculate variance to eliminate zero var features. Alters DataFrames hereafter.\r\n        :keyword 'all': DataFrames are concatenated before variances are calculated.\r\n        'none': Each DataFrame's variance is calculated separately. Increases likelihood of feature elimination.\r\n        Iterable: DataFrames in nested iterables are pooled. Solo DataFrames are not.\r\n    :type pairing: str | iterable(iterable(int, int)) | callable\r\n    :param pairing: Iterable of 2-tuples of indices of desc_df to use for pairwise stats.\r\n    :param pair_vars: How to calculate variances for pairwise stats. May be combined unless specified. Includes var_pooling unless 'not_combined' used.\r\n        'not_combined': Use original DataFrames without any var_pooling elimination. Useful for data heterogeneity.\r\n        'separate': Calculate DataFrame variance separately. Recommended to avoid anomalous stats calculations.\r\n        'pairs': Use pairwise variance pooling for pairwise stats.\r\n        'only_pooled': Use only pooled setting in var_pooling. May not combine with others. Not recommended.\r\n    \"\"\"\r\n    # Drop NA observations and features.\r\n    # Returns dictionary of variances\r\n    pooled_vars = get_pooled_vars(df_list=desc_dfs, method=var_pooling)\r\n    # Check for zero variance features.\r\n    commons_cols = common_columns(dfs=desc_dfs)\r\n    # Assemble pair tuples for pairwise feature selection.\r\n    df_pair_list = pair_dfs(desc_dfs, method=pairing)\r\n\r\n    # Add pairwise stats here.\r\n\r\n\r\n# Vis\r\ndef kde_grouped(dfs, cols, names=None, fig_width=5):\r\n    import matplotlib.pyplot as plt\r\n    # import plotly.graph_objects as go\r\n    # from plotly import subplots\r\n    n_rows = len(dfs)\r\n    tot_cols = len(cols)\r\n    assert n_rows > 0\r\n    assert tot_cols > 0\r\n    for f_i, col_list in enumerate(itertools.batched(list(range(tot_cols)), n=fig_width)):\r\n        fig, axes = plt.subplots(nrows=n_rows, ncols=fig_width, sharex='col', sharey='row')\r\n        # fig = subplots.make_subplots(rows=n_rows, cols=fig_width, shared_xaxes='columns', vertical_spacing=0.02)\r\n        for c_i, col in enumerate(cols):\r\n            for d_i, df in enumerate(dfs):\r\n                axes[c_i, d_i].hist(data=df[col], density=True, stacked=True)\r\n                # fig.add_histogram(df[col], histnorm=\"probability density\", showlegend=True, name=names, opacity=0.5,\r\n                #                  row=d_i, col=c_i)\r\n        fig.show()\r\n\r\n\r\n# Vis\r\ndef violin_splits(ser_list, x_labels, legend_names):\r\n    fig = go.Figure()\r\n    # Separate traces before passing to function.\r\n    fig.add_trace(go.Violin(x=x_labels,\r\n                            y=ser_list[0],\r\n                            legendgroup=legend_names[0], scalegroup=legend_names[0], name=legend_names[0],\r\n                            side='negative',\r\n                            line_color='blue'))\r\n    fig.add_trace(go.Violin(x=x_labels,\r\n                            y=ser_list[1],\r\n                            legendgroup=legend_names[1], scalegroup=legend_names[1], name=legend_names[1],\r\n                            side='positive',\r\n                            line_color='orange'))\r\n    fig.update_traces(meanline_visible=True, scalemode='count')\r\n    fig.update_layout(violingap=0, violinmode='overlay')\r\n    fig.show()\r\n\r\n\r\ndef category_score(data, label, score_func, discrete='auto', chi=False, rand_state=None, copy=True, combined=False,\r\n                   **kwargs):\r\n    mi_list, fclass_list = list(), list()\r\n    if chi:\r\n        chi_list = list()\r\n    else:\r\n        chi_list = None\r\n    if combined is True and (type(data) is list or type(data) is tuple or type(data) is set):\r\n        data = pd.concat(data, axis=1, copy=True)\r\n        score_tog = category_score(data, label, score_func, discrete=discrete, rand_state=rand_state, copy=copy,\r\n                                   combined=False)\r\n    elif combined == 'both':\r\n        score_tog = category_score(data, label, score_func, discrete=discrete, rand_state=rand_state, copy=copy,\r\n                                   combined=False)\r\n        score_sep = category_score(data, label, score_func, discrete=discrete, rand_state=rand_state, copy=copy,\r\n                                   combined=True)\r\n        for dataset in zip(score_sep, score_tog):\r\n            mi_list.append(dataset[0])\r\n            fclass_list.append(dataset[1])\r\n            if chi:\r\n                chi_list.append(dataset[2])\r\n    elif not combined:\r\n        mi = mutual_info_classif(data, label, copy=copy, random_state=rand_state, **kwargs)\r\n        mi_list.append(mi)\r\n        fc = f_classif(data, label)\r\n        fclass_list.append(fc)\r\n        if chi:\r\n            negs = data < 0.0001\r\n            cs_list = list()\r\n            for col in data.columns:\r\n                if negs[col].any(axis=1):\r\n                    cs_list.append(chi2(X=negs[col] + data[col].min + 0.001, y=label[col]))\r\n                else:\r\n                    cs_list.append(chi2(X=data[col], y=label[col]))\r\n                chi_list.append(pd.Series(data=cs_list, index=data.columns))\r\n    return mi_list, fclass_list, chi_list\r\n\r\n\r\ndef descriptive_stats(df, subset=None):\r\n    for desc_match in descriptor_groups(df):\r\n        fig, ax = plt.subplots()\r\n        plt.tight_layout()\r\n        plt.show()\r\n\r\n\r\ndef descriptor_groups(desc_df):\r\n    desc_dict, group_dict = padel_categorization.group_padel_descriptors_manual()\r\n    for short_group, name in desc_dict.items():\r\n        group = group_dict[short_group]\r\n        matches = desc_df[desc_df.columns.isin(group) & desc_df.columns.isin(name)]\r\n        yield {'cols': matches, 'group': group, 'name': name}\r\n\r\n'''\r\ndef suggested_workflow():\r\n    set_labels = ['EPA', 'ENAMINE', SOLUBLE', 'INSOLUBLE']\r\n    df_dict = load_file_groups(df_dir, set_labels, multigroup=True, multi_df='merge')\r\n    zero_vars, all_var, df_vars = zero_var_filter([i for i in [j for j in df_dict.items()]])\r\n    zero_vars.to_csv()\r\n    for key, value in df_dict.items():\r\n        if 'INSOLUBLE' in key:\r\n            value.insert(0, 'Solubility', value='Insoluble')\r\n        else:\r\n            value.insert(0, 'Solubility', value='Soluble')\r\n    mi, fclass = category_score(df_dict.values().iloc[1:], df_dict.values().iloc[0], rand_state=0, combined='both')\r\n    \r\n\r\n    \r\n'''\r\ndesc_dir = \"/dmso_solubility/final_datasets/padel/\"\r\nfile_list = glob.glob(\"PADEL_*5mM.pkl\", root_dir=desc_dir)\r\ndesc_names = padel_categorization.padel_descriptors\r\ndf_dict = dict()\r\nfor f in file_list:\r\n    desc = pd.read_pickle('{}{}'.format(desc_dir, f))\r\n    df_dict[f] = desc.T.explode(desc.index.to_list()).T\r\n    df_dict[f].set_axis(columns=desc_names, inplace=True).rename(index=desc['INCHI_KEY'], inplace=True)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/data_handling/descriptor_preprocessing.py b/dmso_model_dev/data_handling/descriptor_preprocessing.py
--- a/dmso_model_dev/data_handling/descriptor_preprocessing.py	
+++ b/dmso_model_dev/data_handling/descriptor_preprocessing.py	
@@ -1,21 +1,16 @@
 import copy
-import dataclasses
 import functools
 import glob
 import itertools
 import logging
 import os.path
-from typing import List, Tuple
-from dataclasses import dataclass
-
-import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
-import plotly
+import matplotlib.pyplot as plt
 import plotly.graph_objects as go
 from sklearn.feature_selection import chi2, f_classif, mutual_info_classif
-import padel_categorization
-from constants import *
+import dmso_model_dev.data_handling.padel_categorization as padel_categorization
+from dmso_model_dev.constants import *
 
 
 def parse_raw_api_output(api_path, desc_names=None):
@@ -83,8 +78,23 @@
         # zero_vars.append(df_i.copy().drop(all_var[all_var == 0].index))
     return zero_vars, all_var, df_vars
 
-# Logic: pair_dfs loops through dfs and returns a [dict[df_index_tup, dfs], DataFrame[pairs, func]]. The elements of DFs are generators of the column's func callable for the concatenated data in the DF pair.
-def generate_df_funcs(df, funcs):
+
+def remove_duplicate_dfs(df_list, r=None):
+    # Remove duplicate DFs.
+    dup_list = list()
+    assert len([x for x in df_list if type(x) is pd.DataFrame]) > 2
+    for d_i, df in enumerate(df_list):
+        if np.lib.iterable(df):
+            remove_duplicate_dfs(df_list, r=r)
+        elif type(df) is pd.DataFrame or type(df) is pd.Series:
+            if any([df.equals(df_past) for df_past in [d for d in df_list[:d_i] if type(d) is pd.DataFrame or type(d) is pd.Series]]):
+                dup_list.append(d_i)
+    return [df_list.remove(x) for x in dup_list]
+
+
+# Logic: df_list loops through dfs and returns a [dict[df_index_tup, dfs], DataFrame[pairs, func]]. The elements of DFs are generators of the column's func callable for the concatenated data in the DF pair.
+'''
+def generate_df_funcs(df, funcs, *args, **kwargs):
     """
     Returns a list of tuples composed of the original DataFrame and a Series for each of funcs.
     :param funcs: Callable or iterable of callables to apply to df.
@@ -94,25 +104,21 @@
     df_func_list, func_list = list(), dict()
     if np.lib.iterable(funcs):
         for func in funcs:
-            df_func_list.append()
+            df_func_list.append([df, ])
         func_list.update([(col_name, ser.var()) for col_name, ser in df.items()])
         df_var_list.append((df, pd.Series(data=func_list)))
     return df_func_list
+'''
 
-    def _generator_df_func(df, func, cols=None):
-        if not cols:
-            cols = [c for c in cols if c in df.columns]
-        part_func = functools.partial(__func=func)
-        for col, ser in df.items():
-
-            return part_func(df[col].copy(deep=True))
+
+def _generator_df_func(df, func, cols=None, *args, **kwargs):
+    if not cols:
+        cols = [c for c in cols if c in df.columns]
+    part_func = functools.partial(__func=func, *args, **kwargs)
+    for col, ser in df.items():
+        return part_func(df[col].copy(deep=True))
 
-def pair_dfs(dfs, paired_funcs=None):
-    dup_list = list()
-    for (di_1, df1), (di_2, df2) in list(itertools.combinations(enumerate(dfs), r=2)):
-        if df1.equals(df2) and di_2 not in dup_list:
-            dup_list.append(di_2)
-    [dfs.remove(x) for x in dup_list]
+def pair_dfs(dfs, paired_funcs=None, pair_tups=None, *args, **kwargs):
     cols = common_columns(dfs)
     if not pair_tups and paired_funcs:
         assert np.lib.iterable(paired_funcs) and any([type(f) is callable() or type(f) is pd.DataFrame or type(f) is pd.Series for f in dfs])
@@ -120,8 +126,7 @@
         if type(paired_funcs) is dict:
             funcs = paired_funcs.values()
         elif type(paired_funcs) is not dict and np.iterable(paired_funcs):
-
-    for
+            pass
         for (df1, df1_var), (df2, df2_var) in pair_tups:
             var_one_array = [col_name for col_name, x in df1_var.items() if np.lib.iterable(x)]
             var_two_array = [col_name for col_name, x in df2_var.items() if np.lib.iterable(x)]
@@ -130,36 +135,68 @@
         yield itertools.combinations(df_vars, r=2)
 
 
-
-def pair_dfs(df_list, method):
-    df_pair_list = list()
-    if method == 'auto' or method == 'all':
-        df_pair_list = itertools.combinations(df_list, r=2)
-    elif callable(method):
-        df_pair_list = method(df_list)
-    elif np.iterable(method) and all([np.iterable(pairs) for pairs in method]) and all([len(pairs) == 2 for pairs in method]):
-        for pair in method:
-            if all([type(p) is pd.DataFrame for p in pair]):
-                df_pair_list.append(pair)
-            elif all([type(p) is int and (0 < p < len(df_list)) for p in pair]):
-                df_pair_list.append((df_list[pair[0]], df_list[pair[1]]))
-            else:
-                raise TypeError
-            yield df_pair_list
+def func_dfs_gen(df_list, funcs, method=None):
+    """
+    Loops through functions and applies them to DataFrames in df_list.
+    :param df_list:
+    :param funcs:
+    """
+    func_dfs_list = list()
+    if not np.lib.iterable(funcs):
+        funcs = [funcs]
+    for func in funcs:
+        _generator_df_func(df_list, func)
+        if callable(func):
+            func_dfs_list = func(df_list)
+        elif np.iterable(df_list) and all([np.iterable(pairs) for pairs in df_list]) and all([len(pairs) == 2 for pairs in df_list]):
+            for pair in method:
+                if all([type(p) is pd.DataFrame for p in pair]):
+                    func_dfs_list.append(pair)
+                elif all([type(p) is int and (0 < p < len(df_list)) for p in pair]):
+                    func_dfs_list.append((df_list[pair[0]], df_list[pair[1]]))
+                else:
+                    raise TypeError
+    yield from func_dfs_list
 
 
-def pairwise_stats(df_list, indices, func, *args, **kwargs):
+def generate_sers_func(ser_list, func, generator_list=None, indices_list=None, id_list=None, *args, **kwargs):
+    """
+    Loops through Series in ser_list and returns a func generator for each ser.
+    :param ser_list:
+    :param func:
+    :param indices_list:
+    :param id_list: Foreign keys for each Series.
+    :param args:
+    :param kwargs:
+    """
     import operator
-    return_dict = dict([(new_key, list()) for new_key in df_list])
-    if type(func) is str:
-        op = operator.methodcaller(func, *args, **kwargs)
-    else:
-        op = functools.partial(func=func, *args, **kwargs)
-    for i_one, i_two in indices:
-        df1, df2 = df_list[i_one], df_list[i_two]
-        return_dict[i_one, i_two] = op([df1, df2])
-    return pd.Series(return_dict)
-
+    func_list = dict()
+    if not generator_list or not np.lib.iterable(generator_list):
+        generate_sers_func(ser_list, func, indices_list=indices_list, id_list=id_list, *args, **kwargs)
+    if indices_list is None or len(indices_list) == 0:
+        indices_list = list(range(len(ser_list)))
+        generator_list = dict([(new_key, [func, ser]) for new_key, ser in zip(ser_list)])
+    if not id_list or len(id_list) == 0:
+        id_list = indices_list
+    if not np.lib.iterable(ser_list):
+        ser_list = [ser_list]
+    if type(ser_list) is dict:
+        generator_list = list([(new_key, func_list) for new_key in indices_list])
+        generator_list = list()
+    else:
+        generator_list = dict([(new_key, list(func, )) for new_key in indices_list])
+    for index_key, ser_list in generator_list.items():
+        ser = ser_list[index_key]
+        try:
+            func_ser = pd.Series.apply(func=func, *args, **kwargs)
+        except:
+            if type(func) is str:
+                op = operator.methodcaller(func, *args, **kwargs)
+            else:
+                op = functools.partial(func=func, *args, **kwargs)
+            func_ser = op(ser)
+    for d_i in indices_list:
+        yield [(d_i, ser_list[d_i], op(ser_list[d_i]))]
 
 def common_columns(dfs=None, cols=None, excluded=None, use_same_columns=False):
     # TODO: Add handling for mixed types in dfs parameters (ex. list[DataFrame, DataFrame, tuple(DataFrame, DataFrame)]
@@ -183,7 +220,7 @@
 def ks_stats(dfs=None, df_pairs=None, var_tup_pairs=None, use_cols=None, exclude=None, use_same_columns=False):
     #https://stackoverflow.com/questions/62088255/scipys-ks-2samp-function-gives-good-d-statistic-but-wrong-p-value?rq=3
     from scipy.stats import ks_2samp
-    from scipy.stats._stats_py import KstestResult,
+    from scipy.stats._stats_py import KstestResult
     if not dfs and df_pairs:
         dfs = list(itertools.chain.from_iterable(df_pairs))
     elif df_pairs:
@@ -192,7 +229,7 @@
         raise ValueError
     ks_cols, ks_pvals_list  = dict(), list()
     # TODO: Change loop ordering to allow DF index to be entered into resulting DataFrame/Series.
-    for col in cols:
+    for col in use_cols:
         pvals = dict()
         for df1_tup, df2_tup in enumerate(var_tup_pairs):
             di_1, df1, df1_var = df1_tup
@@ -432,8 +469,27 @@
     mi, fclass = category_score(df_dict.values().iloc[1:], df_dict.values().iloc[0], rand_state=0, combined='both')
     
 
-    
-'''
+'''
+'''
+def rdkit_scratch():
+    pd.DataFrame.itertuples()
+    from rdkit.ML.Cluster import Butina, Clustering
+    from rdkit.DataManip.Metric import GetTanimotoDistMat, GetTanimotoSimMat
+    from rdkit.DataStructs.cDataStructs import CreateFromBinaryText, SparseBitVect, ExplicitBitVect
+    ExplicitBitVect.ToBitString()
+    ExplicitBitVect.ToList()
+    DataStructs.SparseBitVect()
+    from rdkit.DataStructs.VectCollection import VectCollection
+    from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray
+    Butina.ClusterData()
+    rdkit.Chem.RDKFingerprint()
+    GetTanimotoDistMat()
+    from rdkit.Chem.rdFingerprintGenerator import MorganFP, RDKitFP, GetMorganGenerator, FingeprintGenerator64
+    FingeprintGenerator64.GetSparseFingerprints()
+    rdkit.Chem.Fingerprints.FingerprintMols.FingerprintsFromSmiles()
+    from rdkit.Chem.rdinchi import InchiToMol
+    tup_dir = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/filtered_dfs_tup.pkl"
+
 desc_dir = "/dmso_solubility/final_datasets/padel/"
 file_list = glob.glob("PADEL_*5mM.pkl", root_dir=desc_dir)
 desc_names = padel_categorization.padel_descriptors
@@ -442,3 +498,4 @@
     desc = pd.read_pickle('{}{}'.format(desc_dir, f))
     df_dict[f] = desc.T.explode(desc.index.to_list()).T
     df_dict[f].set_axis(columns=desc_names, inplace=True).rename(index=desc['INCHI_KEY'], inplace=True)
+'''
\ No newline at end of file
Index: requirements.txt
===================================================================
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
--- /dev/null	
+++ b/requirements.txt	
@@ -0,0 +1,137 @@
+matplotlib~=3.9.0rc2
+numpy~=1.26.4
+pandas~=2.2.2
+scikit-learn~=1.4.2
+setuptools~=70.0.0
+beautifulsoup4~=4.12.3
+soupsieve~=2.5
+h11~=0.14.0
+cffi~=1.16.0
+defusedxml~=0.7.1
+typing_extensions~=4.11.0
+future~=1.0.0
+pillow~=10.2.0
+ipython~=8.23.0
+jupyter~=1.0.0
+pip~=24.0
+attrs~=23.2.0
+response~=0.5.0
+tornado~=6.4
+Jinja2~=3.1.3
+SDF~=0.3.5
+h5py~=3.10.0
+scipy~=1.12.0
+pyzmq~=25.1.2
+colorama~=0.4.6
+comm~=0.2.2
+traitlets~=5.14.2
+fqdn~=1.5.1
+tables~=3.9.2
+parso~=0.8.3
+jedi~=0.19.1
+pytz~=2023.4
+QtPy~=2.4.1
+packaging~=24.0
+trio~=0.25.0
+outcome~=1.3.0.post0
+sniffio~=1.3.1
+sortedcontainers~=2.4.0
+idna~=3.7
+PyYAML~=6.0.1
+arrow~=1.3.0
+python-dateutil~=2.9.0.post0
+Babel~=2.14.0
+httpcore~=1.0.5
+Pygments~=2.17.2
+certifi~=2024.2.2
+pywin32~=306
+MolVS~=0.1.1
+rdkit==2023.9.5
+six~=1.16.0
+llvmlite~=0.42.0
+numba~=0.59.1
+psutil~=5.9.8
+ipykernel~=6.29.4
+nbformat~=5.10.4
+Cython~=3.0.10
+overrides~=7.7.0
+patsy~=0.5.6
+pyaml~=23.12.0
+requests~=2.31.0
+seaborn~=0.13.2
+threadpoolctl~=3.3.0
+platformdirs~=4.2.0
+joblib~=1.4.0
+webencodings~=0.5.1
+bleach~=6.1.0
+tinycss2~=1.2.1
+blosc2~=2.6.2
+msgpack~=1.0.8
+ndindex~=1.8
+py-cpuinfo~=9.0.0
+numexpr~=2.9.0
+MarkupSafe~=2.1.5
+openpyxl~=3.2.0b1
+pyarrow~=16.0.0
+plotly~=5.22.0
+tenacity~=8.2.3
+ipywidgets~=8.1.2
+debugpy~=1.8.1
+decorator~=5.1.1
+jupyter_client~=8.6.1
+wcwidth~=0.2.13
+tzdata~=2024.1
+cycler~=0.12.1
+statsmodels~=0.14.1
+urllib3~=2.2.1
+hdf5able~=0.3.0
+anyio~=4.3.0
+jupyter_core~=5.7.2
+nbclient~=0.10.0
+fastjsonschema~=2.19.1
+jsonschema~=4.21.1
+jupyter_server~=2.14.0
+jupyterlab~=4.2.0b1
+jupyterlab_server~=2.26.0
+notebook_shim~=0.2.4
+notebook~=7.1.2
+asttokens~=2.4.1
+contourpy~=1.2.1
+executing~=2.0.1
+fonttools~=4.51.0
+missingno~=0.5.2
+nbconvert~=7.16.3
+pandocfilters~=1.5.1
+mistune~=3.0.2
+jupyterlab_pygments~=0.3.0
+pycparser~=2.21
+pyparsing~=3.1.2
+qtconsole~=5.5.1
+websocket-client~=1.7.0
+terminado~=0.18.1
+referencing~=0.34.0
+webcolors~=1.13
+jsonpointer~=2.4
+isoduration~=20.11.0
+httpx~=0.27.0
+kiwisolver~=1.4.5
+Send2Trash~=1.8.3
+hdf5storage~=0.1.19
+prettyprint~=0.1.5
+prometheus_client~=0.20.0
+jupyter_server_terminals~=0.5.3
+wsproto~=1.2.0
+json5~=0.9.25
+wheel~=0.43.0
+rich~=13.7.1
+mpmath~=1.3.0
+sympy~=1.12.1
+Markdown~=3.6
+keras~=3.4.0
+tensorflow~=2.16.1
+Werkzeug~=3.0.3
+kaleido~=0.2.0
+filelock~=3.15.4
+tqdm~=4.66.4
+fsspec~=2024.6.1
+cachetools~=5.3.3
\ No newline at end of file
Index: kohodequipe/dimensionality.py
===================================================================
diff --git a/kohodequipe/dimensionality.py b/dmso_model_dev/dimensionality.py
rename from kohodequipe/dimensionality.py
rename to dmso_model_dev/dimensionality.py
--- a/kohodequipe/dimensionality.py	
+++ b/dmso_model_dev/dimensionality.py	
@@ -1,14 +1,13 @@
 import logging
 import os.path
+
 import numpy as np
 import pandas as pd
-import sklearn.utils.validation
+from sklearn.decomposition import KernelPCA, PCA
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
-#from sklearn.feature_selection import *
-from sklearn.feature_selection import mutual_info_classif, VarianceThreshold
-from sklearn.linear_model import LassoCV
-from sklearn.decomposition import PCA, KernelPCA
-from plotting_tools import plot_drop_results
+# from sklearn.feature_selection import *
+from sklearn.feature_selection import mutual_info_classif
+
 
 def bestComponentValue(spotifyDataNumCols):
     for i in range(2, len(spotifyDataNumCols.columns)):
@@ -115,7 +114,7 @@
         else:
             return original_top_feats, original_top_samps, None
         if samp_wt is not None:
-            from sklearn.feature_selection import f_classif, chi2
+            pass
             #x_sumsamp = chi2(X=data_df, y=y)
         if type(feat_pct) is float and 0. <= feat_pct <= 1. and output_stub is not None:
             feat_df = top_feats.copy()[top_feats >= top_feats.quantile(q=feat_pct)]
Index: dmso_model_dev/DescriptorRequestor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import logging\r\nimport os\r\nimport sys\r\nfrom time import sleep\r\nfrom typing import Tuple, Dict, Any\r\n\r\nimport pandas as pd\r\nimport requests\r\nfrom urllib3 import Retry\r\n\r\n\r\nclass ApiGrabber:\r\n    retries = Retry(\r\n        total=5,\r\n        backoff_factor=0.1,\r\n        status_forcelist=[502, 503, 504],\r\n        allowed_methods={'POST', 'GET'}\r\n    )\r\n\r\n    def __init__(self, api_url, headers=None, payload=None, user=None, timeout=(15, 300), input_type='smiles', flatten_output=True, url_method=\"GET\",\r\n                 key_var=None):\r\n        \"\"\"\r\n\r\n        :type api_url: str\r\n        :type payload: dict\r\n        :type headers: dict\r\n        \"\"\"\r\n        # ?workflow = qsar - ready & smiles = CCC\"\r\n        self.default_headers = headers\r\n        self.default_payload = payload\r\n        self.api_url = api_url\r\n        self.method = url_method\r\n        self.input_type = input_type\r\n        self.flatten_output = flatten_output\r\n        self.response_dict = dict()\r\n        self.info = None\r\n        if key_var:\r\n            self._api_key = os.environ[key_var]\r\n        else:\r\n            self._api_key = None\r\n        self.user = user\r\n        self.id_state = 0\r\n        self.failed_responses = {}\r\n        self.response_info = None\r\n        self.timesouts = timeout\r\n        self.set_session_params()\r\n\r\n    def set_session_params(self):\r\n        # if self._api_key and self._api_key: headers['api_key'] = API_KEY\r\n        if self.user:\r\n            self.default_headers['user-agent'] = self.user\r\n        # sess.mount('https://', HTTPAdapter(max_retries=self.retries['total']))\r\n\r\n    def bulk_epa_call(self, comlist):\r\n        with requests.session() as r:\r\n            # r.merge_environment_settings(url=self.api_url, proxies=None, verify=True, stream=False, cert=None)\r\n            for mol_id in comlist:\r\n                response, returned_input = self.make_api_call(payload_input=mol_id, api_session=r)\r\n                if isinstance(response, dict):\r\n                    yield response, returned_input\r\n                elif hasattr(response, 'status_code'):\r\n                    logging.warning(response.status_code)\r\n                    yield response.status_code, returned_input\r\n                else:\r\n                    logging.warning('Response does not have a status code. {}'.format(response))\r\n                    logging.warning(response)\r\n                    yield response, returned_input\r\n\r\n    def make_api_call(self, payload_input, api_session):\r\n        payload = self.default_payload.copy()\r\n        self.id_state += 1\r\n        payload[self.input_type] = payload_input\r\n        attempts = 0\r\n        response = None\r\n        while attempts < self.retries.total:\r\n            try:\r\n                response = api_session.request(method=self.method, url=self.api_url, params=payload,\r\n                                               timeout=self.timesouts)\r\n                if response.json():\r\n                    break\r\n                else:\r\n                    logging.warning('Status code for response is {} for {}'.format(response.status_code, payload_input))\r\n                    attempts += 1\r\n                '''\r\n                if response.status_code == 200:\r\n                    break\r\n                else:\r\n                    print(response.status_code)\r\n                '''\r\n            except requests.exceptions.ConnectTimeout or requests.exceptions.RequestsWarning or requests.ConnectionError:\r\n                logging.warning('Connection error.')\r\n                attempts += 1\r\n            except:\r\n                logging.warning('{} error for {}'.format(sys.exception(), payload_input))\r\n                attempts += 1\r\n        if response is None:\r\n            logging.warning('No response received for {}'.format(payload_input))\r\n            result = dict(('smiles', payload_input))\r\n        else:\r\n            result = self.parse_api_response(response, payload_input)\r\n        return result, payload_input\r\n\r\n    def flatten(self, my_dict):\r\n        result = dict()\r\n        if isinstance(my_dict, list) and len(my_dict) == 1:\r\n            return self.flatten(my_dict[0])\r\n        elif isinstance(my_dict, dict):\r\n            for key, value in my_dict.items():\r\n                if isinstance(value, dict):\r\n                    result.update(self.flatten(value))\r\n                elif isinstance(value, list):\r\n                    val_list = list()\r\n                    for val in value:\r\n                        if type(val) is dict:\r\n                            result.update(self.flatten(val))\r\n                        else:\r\n                            val_list.append(value)\r\n                    if len(val_list) > 0:\r\n                        result[key] = value\r\n                else:\r\n                    result[key] = value\r\n        else:\r\n            print(my_dict)\r\n            result = my_dict\r\n        return result\r\n\r\n    def parse_api_response(self, response, api_input):\r\n        try:\r\n            if type(response.json()) is list and len(response.json()) > 0:\r\n                unpacked = self.flatten(response.json())\r\n            elif type(response.json()) is dict and len(response.json().values()) > 0:\r\n                unpacked = self.flatten(response.json())\r\n        except AttributeError:\r\n            if type(response) is list and len(response) > 0:\r\n                unpacked = self.flatten(response)\r\n            elif type(response) is dict and len(response.values()) > 0:\r\n                unpacked = self.flatten(response)\r\n        except:\r\n            logging.warning('Exception: {} for {}, {}'.format(sys.exception(), api_input, response))\r\n            self.failed_responses[api_input] = sys.exception()\r\n            unpacked = dict(('SMILES', api_input))\r\n        return unpacked\r\n\r\n    def parse_input(self, compounds):\r\n        if (type(compounds) is list or type(compounds) is tuple or type(compounds) is set) and len(compounds) > 0:\r\n            data = compounds\r\n        elif type(compounds) is str:\r\n            if ',' in compounds:\r\n                data = compounds.split(',')\r\n            else:\r\n                data = [compounds]\r\n        else:\r\n            logging.error(\"{}! That's not a SMILES string!\".format(compounds))\r\n            raise UserWarning\r\n            data = None\r\n        return data\r\n\r\n    def pandas_call(self, pd_input, column_name=None):\r\n        if type(pd_input) is pd.DataFrame:\r\n            return self.bulk_epa_call(pd_input[column_name].tolist())\r\n        elif type(pd_input) is pd.Series:\r\n            return self.bulk_epa_call(pd_input.tolist())\r\n        else:\r\n            raise IOError\r\n\r\n    def grab_data(self, compounds):\r\n        self.parse_input(compounds)\r\n        df = pd.DataFrame(self.response_dict)\r\n        logging.info('Successes: {}\\n\\n'.format(len(self.response_dict.keys())))\r\n        logging.info('\\n\\nNumber of failed compounds: {}'.format(len(self.failed_responses.keys())))\r\n        logging.info(df.head())\r\n        return df, self.failed_responses\r\n\r\n\r\nclass QsarStdizer(ApiGrabber):\r\n\r\n    def __init__(self, api_url=\"https://hcd.rtpnc.epa.gov/api/stdizer\", payload=None, keep_mol=False):\r\n        super().__init__(api_url=api_url, payload=payload)\r\n        if self.default_payload is None:\r\n            self.default_payload = {'workflow': 'qsar-ready'}\r\n\r\n    # ?type=padel&smiles=ccff\r\n\r\n\r\nclass DescriptorGrabber(ApiGrabber):\r\n    SET_LIST = ['padel', 'rdkit', 'mordred', 'toxprints']\r\n\r\n    def __init__(self, desc_set, api_url=\"https://hcd.rtpnc.epa.gov/api/descriptors\", *args, **kwargs):\r\n        super().__init__(api_url=api_url, payload={'type': desc_set})\r\n        self.desc_key_list = []\r\n\r\n        if desc_set not in self.SET_LIST:\r\n            print('\\n{}} was not in list of available descriptor sets.\\n'.format(desc_set))\r\n            raise TypeError\r\n\r\n    def _parse_descriptor_response(self, response, api_input):\r\n        if 'descriptors' in response.json()['chemicals'][0]:\r\n            self.response_dict[response.json()['chemicals'][0]['smiles']] = response.json()['chemicals'][0][\r\n                'descriptors']\r\n        else:\r\n            logging.error('Descriptors are missing in {} for {}\\n.'.format(response.json(), api_input))\r\n            self.failed_responses[api_input] = response.json()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/DescriptorRequestor.py b/dmso_model_dev/DescriptorRequestor.py
--- a/dmso_model_dev/DescriptorRequestor.py	
+++ b/dmso_model_dev/DescriptorRequestor.py	
@@ -1,13 +1,13 @@
 import logging
 import os
 import sys
-from time import sleep
-from typing import Tuple, Dict, Any
 
 import pandas as pd
 import requests
 from urllib3 import Retry
 
+from dmso_model_dev.constants import QSAR_COLUMNS
+
 
 class ApiGrabber:
     retries = Retry(
@@ -52,11 +52,11 @@
         # sess.mount('https://', HTTPAdapter(max_retries=self.retries['total']))
 
     def bulk_epa_call(self, comlist):
-        with requests.session() as r:
+        with requests.session().options(url=self.api_url) as r:
             # r.merge_environment_settings(url=self.api_url, proxies=None, verify=True, stream=False, cert=None)
             for mol_id in comlist:
                 response, returned_input = self.make_api_call(payload_input=mol_id, api_session=r)
-                if isinstance(response, dict):
+                if type(response) is dict:
                     yield response, returned_input
                 elif hasattr(response, 'status_code'):
                     logging.warning(response.status_code)
@@ -95,7 +95,7 @@
                 attempts += 1
         if response is None:
             logging.warning('No response received for {}'.format(payload_input))
-            result = dict(('smiles', payload_input))
+            result = dict(('SMILES', payload_input))
         else:
             result = self.parse_api_response(response, payload_input)
         return result, payload_input
@@ -179,18 +179,32 @@
         if self.default_payload is None:
             self.default_payload = {'workflow': 'qsar-ready'}
 
+    # TODO: Wrapper for Standardizer calls.
+    def bulk_standardize(self, smiles_list, format=None):
+        for resp, api_input in super().bulk_epa_call(smiles_list):
+            if type(resp) is dict and 'mol' in resp.keys():
+                resp.pop('mol')
+            elif type(resp) is dict and len(resp.items()) > 9:
+                resp = eval(resp)
+            if format.lower() == 'series':
+                resp_ser = pd.Series(resp).rename(index=QSAR_COLUMNS)
+                if resp_ser.size > 9:
+                    resp_ser = pd.Series(eval(resp)).rename(index=QSAR_COLUMNS)
+                yield resp_ser
+            else:
+                yield eval(resp)
     # ?type=padel&smiles=ccff
 
 
 class DescriptorGrabber(ApiGrabber):
-    SET_LIST = ['padel', 'rdkit', 'mordred', 'toxprints']
-
+    SET_LIST = ['padel', 'rdkit', 'mordred', 'toxprints', 'cfp']
+    # https://hazard-dev.sciencedataexperts.com/api/rdkit?smiles=CCCCCCC&headers=true&type=cfp&radius=4&bits=1024
     def __init__(self, desc_set, api_url="https://hcd.rtpnc.epa.gov/api/descriptors", *args, **kwargs):
         super().__init__(api_url=api_url, payload={'type': desc_set})
         self.desc_key_list = []
 
         if desc_set not in self.SET_LIST:
-            print('\n{}} was not in list of available descriptor sets.\n'.format(desc_set))
+            print('\n{} was not in list of available descriptor sets.\n'.format(desc_set))
             raise TypeError
 
     def _parse_descriptor_response(self, response, api_input):
Index: dmso_model_dev/data_handling/prep_data_set.py
===================================================================
diff --git a/dmso_model_dev/data_handling/prep_data_set.py b/dmso_model_dev/data_handling/prep_data_set.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/prep_data_set.py	
@@ -0,0 +1,106 @@
+import itertools
+import json
+import os.path
+import pickle
+
+import joblib
+import numpy as np
+import pandas as pd
+import qsar_readiness
+from sklearn.cluster import HDBSCAN, OPTICS
+from sklearn.covariance import EmpiricalCovariance, GraphicalLasso, LedoitWolf, MinCovDet
+from sklearn.manifold import LocallyLinearEmbedding
+from skopt.searchcv import BayesSearchCV
+from skopt.space import Integer, Real
+
+
+def run_qsar(data_dir, smiles_list, smiles_name=None, salt_name=None, intermetallic_name=None):
+    salts, intermetallics = list(), list()
+    if salt_name is not None or intermetallic_name is not None:
+        if intermetallic_name is not None:
+            salts, intermetallics = qsar_readiness.salts_and_intermetallics(smiles_list=smiles_list,
+                                                                            intermetallics=True)
+            print(salts, intermetallics)
+        else:
+            salts, intermetallics = qsar_readiness.salts_and_intermetallics(smiles_list=smiles_list)
+    if len(salts) > 0:
+        pd.Series(salts).to_pickle('{}{}.pkl'.format(data_dir, salt_name), protocol=3)
+        smiles_list.remove(salts)
+    if len(intermetallics) > 0:
+        pd.Series(intermetallics).to_pickle(path='{}{}'.format(data_dir, intermetallics), protocol=3)
+        smiles_list.remove(intermetallics)
+    else:
+        salts, intermetallics = list(), list()
+    print('Running QSAR calls...')
+    complete_responses = qsar_readiness.qsar_standardizer_api(smiles_list=smiles_list, response_timeout=120,
+                                                              complete=True)
+    if smiles_name is None:
+        smiles_name = '{}qsar_responses_dict'.format(data_dir)
+    else:
+        smiles_name = '{}{}'.format(data_dir, smiles_name)
+        try:
+            with open('{}.pkl'.format(smiles_name), 'wb') as smile_file:
+                pickle.dump(obj=complete_responses, file=smile_file, protocol=3)
+        except:
+            with open('{}.json'.format(smiles_name), 'w') as smile_file:
+                json.dump(complete_responses, fp=smile_file)
+    return complete_responses, smiles_list, salts, intermetallics
+
+
+def main():
+    DATA_PATH = "/dmso_solubility/commercial_data/150_div_set/"
+    mol_df = pd.read_pickle('{}divset_df.pkl'.format(DATA_PATH))
+    for i, tup in enumerate(itertools.batched(mol_df['SMILE'].tolist()[40000:], 5000)):
+        chunk = list(tup)
+        print('Running {} compounds...'.format(len(chunk)))
+        qsar_name = 'qsar_response_5k_{}'.format(i)
+        salt_name = 'salts_{}'.format(i)
+        metal_name = 'intermetallics_{}'.format(i)
+        if os.path.isfile(path='{}{}.pkl'.format(DATA_PATH, qsar_name)):
+            continue
+        else:
+            qsar_responses, smiles, salts, intermetals = run_qsar(DATA_PATH, chunk, smiles_name=qsar_name,
+                                                                  salt_name=salt_name, intermetallic_name=metal_name)
+
+
+# Estimate covariance or perform discriminant analysis on clusters of 
+def cluster_covars(X, y=None, clusters=None, priors=None, mode=None):
+    jobs = 7
+    mems = joblib.Memory(location='C:/Users/mmanning/PycharmProjects/data/joblib_cache')
+    if clusters == 'lle':
+        n_components = Integer(low=2, high=np.ceil(np.log2(X.shape[1])), name='No. of Components')
+        n_neighbors = Integer(low=2, high=10, name='Neighbors')
+        search = {'n_components': n_components, 'n_neighbors': n_neighbors}
+        cls = LocallyLinearEmbedding(max_iter=1000, n_jobs=jobs)
+    elif clusters == 'optics':
+        search = {'min_cluster_size': Integer(low=1, high=5, name='Min Cluster Size'),
+                  'leaf_size': Integer(low=5, high=50, name='Leaf Size'),
+                  'xi': Real(low=0., high=1., name='Xi Coefficient')}
+        cls = OPTICS()
+    elif clusters == 'hdbscan':
+        cls = HDBSCAN()
+    baye = BayesSearchCV(estimator=cls, search_spaces=search, return_train_score=True, random_state=0, n_jobs=jobs,
+                         scoring='balanced_accuracy', n_points=3, cv=3)
+    fitted = baye.fit(X=X, y=y, groups=y)
+    print(fitted.cv_results_)
+    print(fitted.optimizer_results_)
+    print(fitted.classes_)
+    cov_list = list()
+    for i in np.unique(cls.labels_):
+        group = X[cls.labels_[i]]
+        if mode == 'empirical':
+            est = EmpiricalCovariance().fit(X, y=y)
+        elif mode == 'robust':
+            est = MinCovDet().fit(X, y=y)
+        elif mode == 'lasso':
+            est = GraphicalLasso().fit(X, y=y)
+        else:
+            est = LedoitWolf().fit(X, y=y)
+    cov_list.append(est)
+    cov_path = "//enamine_data/enamine_cov_"
+    joblib.dump(cov_list, filename='{}{}.joblib'.format(cov_path, mode), protocol=3)
+    return cov_list
+
+
+if __name__ == '__main__':
+    main()
Index: dmso_model_dev/data_handling/nan_tools.py
===================================================================
diff --git a/dmso_model_dev/data_handling/nan_tools.py b/dmso_model_dev/data_handling/nan_tools.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/nan_tools.py	
@@ -0,0 +1,36 @@
+import logging
+
+import numpy as np
+import pandas as pd
+from scipy.special import softmax
+
+
+def nan_score(feats):
+    return
+    isn = np.isnan(feats)
+    flona = isn.copy().astype(np.float64)
+    bayessamp = flona.copy().add(1. / isn.shape[1])
+    bayesfeat = flona.copy().add(1. / isn.shape[0])
+    npbs = bayessamp.to_numpy(dtype=np.float64)
+    npf = bayesfeat.to_numpy(dtype=np.float64)
+    b_i = np.sum(npbs, axis=1).reshape(npbs.shape[0], 1)
+    b_j = np.sum(npf, axis=0).reshape(1, npf.shape[1])
+    print(b_i.max(), b_j.max())
+
+    s_i = softmax(np.divide(isn.to_numpy(), b_i), axis=0)
+    f_j = softmax(np.divide(isn.to_numpy(), b_j), axis=1)
+    print(s_i.max(), f_j.max())
+    print(s_i.shape, f_j.shape)
+    sum_feat = s_i
+    sum_samp = f_j
+    print(sum_feat.max(), sum_samp.max())
+    print(sum_feat.shape, sum_samp.shape)
+    # sum_samp = bayessamp.divide(bayessamp.sum(axis=0, numeric_only=True), axis=0).sum(axis=1, numeric_only=True) # .set_axis(data_df.index)
+    # sum_samp = bayessamp.apply(bayessamp.sum(axis=0).divide, raw=True, axis=1).sum(axis=0) # .set_axis(data_df.index)
+    # sum_samp = bayessamp.apply(bayessamp.sum(axis=1).divide, raw=True, axis=0, engine='numba', engine_kwargs={'parallel': True}).sum(axis=0) # .set_axis(data_df.index)
+    logging.debug(sum_samp)
+    # featsum = np.sum(np.divide(npf, np.sum(npf, axis=1).reshape(npf.shape[0], 1)), axis=0)
+    # print(featsum)
+    logging.debug(sum_feat)
+    sum_feat = pd.Series(data=sum_feat, index=isn.columns)
+    sum_samp = pd.Series(data=sum_samp, index=isn.index)
\ No newline at end of file
Index: dmso_model_dev/data_handling/FeatureAccessor.py
===================================================================
diff --git a/dmso_model_dev/data_handling/FeatureAccessor.py b/dmso_model_dev/data_handling/FeatureAccessor.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/FeatureAccessor.py	
@@ -0,0 +1,49 @@
+import pandas as pd
+
+
+@pd.api.extensions.register_dataframe_accessor("X")
+class FeatureAccessor:
+    def __init__(self, pandas_obj):
+        #self._validate(pandas_obj)
+        self._obj = pandas_obj
+
+    '''
+    @staticmethod
+    def _validate(obj):
+        if not all(col in ob.)
+    '''
+
+    @property
+    def vary(self):
+        vary_cols = self._obj.feature_properties['nonzero_var']
+        return self._obj.X[vary_cols]
+
+    @property
+    def discrete(self):
+        discrete_cols = self._obj.feature_properties['discrete']
+        return self._obj.X[discrete_cols]
+
+    @property
+    def cont(self):
+        cont_cols = self._obj.feature_properties['cont']
+        return self._obj.X[cont_cols]
+
+    @property
+    def sparse(self):
+        sparse_cols = self._obj.feature_properties['sparse']
+        return self._obj.X[sparse_cols]
+
+    @property
+    def dense(self):
+        dense_cols = self._obj.feature_properties['dense']
+        return self._obj.X[dense_cols]
+
+    @property
+    def neg(self):
+        neg_cols = self._obj.feature_properties['neg']
+        return self._obj.X[neg_cols]
+
+    @property
+    def nonneg(self):
+        nonneg_cols = self._obj.feature_properties['nonneg']
+        return self._obj.X[nonneg_cols]
Index: dmso_model_dev/data_handling/preclustering.py
===================================================================
diff --git a/dmso_model_dev/data_handling/preclustering.py b/dmso_model_dev/data_handling/preclustering.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/preclustering.py	
@@ -0,0 +1,389 @@
+import itertools
+import os.path
+import time
+
+import matplotlib.pyplot
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import sklearn.covariance
+import sklearn.feature_selection
+import sklearn.pipeline
+from scipy import stats
+from sklearn.cluster import AgglomerativeClustering
+from sklearn.cluster._feature_agglomeration import AgglomerationTransform
+from sklearn.cluster._hdbscan import hdbscan
+from sklearn.decomposition import PCA
+from sklearn.feature_selection import GenericUnivariateSelect, mutual_info_regression
+from sklearn.model_selection import GridSearchCV
+
+from database_tools import initial_clean
+
+
+def trial_cluster(prepro_def, dpath, dname):
+    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, store_centers='both', n_jobs=6)
+    cluster_labels = clusterer.fit_predict(prepro_def)
+    probs = clusterer.probabilities_
+    roids = clusterer.centroids_
+    print(cluster_labels, roids, probs)
+    print(np.count_nonzero(np.where(probs > 0)))
+    print(np.count_nonzero(np.where(probs == 0)))
+    print(np.count_nonzero(probs))
+    center_path = os.path.join(dpath, '{}_centroids.csv'.format(dname))
+    prob_path = os.path.join(dpath, '{}_probabilities.csv'.format(dname))
+    label_path = os.path.join(dpath, '{}_labels.csv'.format(dname))
+    params_path = os.path.join(dpath, '{}_params.csv'.format(dname))
+    for file, output in zip([center_path, prob_path, label_path], [roids, probs, clusterer.labels_]):
+        np.save(file, output)
+
+
+def trial_agglom(feats, n_clusters=range(5, 21, 5), data_path=None, reload=True):
+    agglo_label_list = []
+    for n_cluster in n_clusters:
+        label_path = '{}agglo_n_{}'.format(data_path, n_cluster)
+        label_file = '{}{}.npy'.format(label_path, n_cluster)
+        if data_path is not None and reload and os.path.isfile(label_file):
+            agglo_labels = np.load(label_file)
+        else:
+            agglo = AgglomerativeClustering(n_clusters=n_cluster, metric='euclidean', linkage='ward')
+            # print('Connected components for n_cluster clusters in agglomeration: ', agglo.n_connected_components_)
+            agglo_labels = agglo.fit_predict(feats)
+        agglo_label_list.append(agglo_labels)
+        if data_path is not None:
+            np.save(file=label_path, arr=agglo_labels)
+    return agglo_label_list
+    # plot_tsne(X, clusters=agglo_labels)
+
+
+def agglo_features(feats, n_clusters=50):
+    agglo = AgglomerationTransform(n_clusters=n_clusters)
+    feat_lumps = agglo.fit_transform(feats)
+
+
+def compute_tsne(df, perps=(20, 35, 50), exagg=12, data_path=None):
+    mappings = []
+    for perp in perps:
+        manifolder = sklearn.manifold.TSNE(n_jobs=6, verbose=False, early_exaggeration=exagg, perplexity=perp,
+                                           n_iter=5000)
+        mapping = manifolder.fit_transform(df)
+        # print(mapping)
+        print('KL Divergence: \n{}'.format(manifolder.kl_divergence_))
+        # print('Random state: {}'.format(manifolder.random_state))
+        mappings.append(mapping)
+    return mappings
+
+
+
+def center_by_median(feat_arr:np.ndarray):
+    for c in range(feat_arr.shape[1]):
+        feat_arr[:, c] = np.subtract(feat_arr[:, c], np.median(feat_arr[:, c]))
+    return center_by_median()
+
+
+def _var_thresh_df(feats_df:pd.DataFrame, dropped_dict=None, var_thresh=0.01):
+    # = feats_df.astype(dtype=np.float64)
+    variance = feats_df.var(axis='rows')
+    vary = ((variance >= var_thresh) & (~pd.isna(variance))).reindex_like(feats_df.T)
+    # print(vary.index)
+    # print(feats_df.columns)
+    print('Number of features with finite variance: ', vary.count())
+    print('Percent of features with finite variance: ', 100 * vary.count() / feats_df.shape[1])
+    over_var = feats_df[feats_df.columns[vary]]
+    zero_var = feats_df[feats_df.columns[~vary]]
+    dropped_dict['var'] = zero_var.columns
+    return over_var, dropped_dict
+
+
+def _var_thres_np(feat_arr:np.ndarray, dropped_dict, var_thresh=0.01):
+    feat_varred = []
+    if var_thresh is not None:
+        n = 0
+        for c in range(feat_arr.shape[1]):
+            vary = np.var(feat_arr[:, c], ddof=1)
+            if vary < var_thresh or vary is not np.nan:
+                feat_varred.append(c)
+                n += 1
+            else:
+                dropped_dict['var'].append(feat_arr[:, c])
+    return feat_arr[:, feat_varred], dropped_dict
+
+
+def var_thresh(feats, dropped_dict=None, var_thresh=0.01):
+    if type(feats) is np.ndarray:
+        return _var_thres_np(feats, dropped_dict=dropped_dict, var_thresh=var_thresh)
+    elif type(feats) is pd.DataFrame:
+        return _var_thresh_df(feats, dropped_dict=dropped_dict, var_thresh=var_thresh)
+
+
+def _med_iqr_tanh_ndarr(feat_arr: np.ndarray, **kwargs):
+    for c in range(feat_arr.shape[1]):
+        iqr = np.abs(np.percentile(feat_arr[:, c], q=75) - np.percentile(feat_arr[:, c], q=25))
+        # print(iqr, feat_arr[:, c])
+        if iqr == 0 or np.max(feat_arr[:,c]) <= 10:
+            iqr = 1
+        feat_arr[:, c] = np.divide(feat_arr[:, c], iqr)
+        feat_arr[:, c] = np.tanh(feat_arr[:, c])
+    return feat_arr
+
+
+def range_df(df, upper, lower, axis=0):
+    for a in [upper, lower]:
+        if 1.0 < a < 100.:
+            a = a / 100.
+    # print(feat_df.quantile(q=lower, axis=1, numeric_only=True))
+    # print(feat_df.quantile(q=lower, axis=0, numeric_only=True))
+    # print(feat_df.quantile(q=upper, axis=1, numeric_only=True))
+    # print(feat_df.quantile(q=upper, axis=0, numeric_only=True))
+    iqr: pd.Series = df.quantile(q=upper, axis=axis, numeric_only=True).subtract(df.quantile(q=lower, axis=0, numeric_only=True)).reindex_like(df.T)
+    return iqr
+
+def _med_iqr_tanh_df(feat_df: pd.DataFrame, upper=.75, lower=.25, **kwargs):
+    iqr = range_df(feat_df, upper, lower, )
+    print('\nNormalizing IQR such that (IQR < 1) = 1. This avoids division by zero.\n')
+    iqr[((0. <= iqr) & (iqr < 1.))] = 1.
+    #iqr[((0. > iqr) & (iqr > -1.))] = -1.
+    # print('IQR: ', iqr.head())
+    time.sleep(0.01)
+    valid = (iqr.notna())
+    # feat_df.columns.astype(int, copy=False)'
+    # print(feat_df[feat_df.columns[valid]])
+    valid_df = feat_df[feat_df.columns[valid]]
+    drop_df = feat_df[feat_df.columns[~valid]]
+    # print(valid_df.shape, drop_df.shape)
+    #print('Median (rows):\n', valid_df.median(axis='rows'))
+    # print('Nan values: ', (valid_df.size - (valid_df.count()).sum()))
+    scaled_df = valid_df.subtract(other=valid_df.median(axis='rows'), axis='columns')
+    # print(scaled_df.head())
+    scaled_df = scaled_df.divide(other=iqr)
+    # print(scaled_df.head())
+    return scaled_df, drop_df
+
+
+def med_iqr_tanh(feat_arr, **kwargs) -> pd.DataFrame:
+    if type(feat_arr) is np.ndarray:
+        return _med_iqr_tanh_ndarr(feat_arr, **kwargs)
+    elif type(feat_arr) is pd.DataFrame:
+        return _med_iqr_tanh_df(feat_arr, **kwargs)
+
+
+def tanh_zero_one(feats_arr: np.ndarray, log_thresh=10):
+    for c in range(feats_arr.shape[1]):
+        if np.less(np.min(feats_arr[:, c]), 1):
+            feats_arr[:, c] = np.add(feats_arr[:, c], (1 - np.min(feats_arr[:, c])))
+        # if np.greater(np.max(feats_arr[:, c]), log_thresh):
+        #    feats_arr[:, c] = np.log(feats_arr[:, c])
+        # print(feats_arr[:5, :5])
+        feats_arr[:, c] = np.arctanh(feats_arr[:, c])
+         # print(feats_arr[:5, :5])
+        feats_arr[:, c] = np.divide(feats_arr[:, c], np.max(feats_arr, axis=1))
+    # print(feats_arr)
+    return feats_arr
+
+
+def _subplotter(mapping, ax: matplotlib.pyplot.Axes, plot_num, clusters=None, cluster_opt=""):
+    if type(clusters) is not None:
+        scatter = ax.scatter(np.array(mapping)[:, 0], np.array(mapping)[:, 1], s=2.5, alpha=0.15, c=clusters,
+                             cmap='tab20')
+    else:
+        scatter = ax.scatter(np.array(mapping)[:, 0], np.array(mapping)[:, 1], s=2.5, alpha=0.15)
+    # plt.savefig('tsne_test.png')
+    labels = [str(c) for c in np.unique(clusters)]
+    if len(labels) > 1:
+        labels[0] = 'Noise'
+        labels[-1] = 'Minority Class'
+        legend = ax.legend(loc='upper right', labels=labels)
+        ax.add_artist(legend)
+    if cluster_opt == "" or cluster_opt is None:
+        cluster_opt = "Cluster #: {}".format(len(labels))
+    ax.text(0.99, 0.1, cluster_opt,
+            size=8,
+            horizontalalignment="right")
+
+
+def plot_mapping(mappings, clusters=None, cluster_opt=""):
+    print(clusters)
+    figure = plt.figure()
+    plot_num = 0
+    if clusters is None:
+        clusters = [None]
+    axes = figure.subplots(nrows=2, ncols=3, sharex='col', sharey='row', squeeze=True)
+    ax_nums = [a for a in itertools.product(list(range(2)), list(range(3)))]
+    for mapping in mappings:
+        for i, cluster in enumerate(clusters):
+            # print(mapping, cluster)
+            if plot_num >= len(axes):
+                break
+            _subplotter(mapping, axes[ax_nums[plot_num]], plot_num=plot_num, clusters=cluster,
+                        cluster_opt=cluster_opt[i])
+            plot_num += 1
+            print("Plotting # ", plot_num)
+    figure.tight_layout(pad=0.5, h_pad=0, w_pad=0)
+    plt.show(block=True)
+
+
+def retrieve_df(data_path):
+    return pd.read_pickle(data_path).T
+
+
+def store_rejects(data_stem, data_name, rejects, null_comps, null_factors):
+    reject_path = os.path.join(data_stem, '{}_rejects.csv'.format(data_name))
+    null_path = os.path.join(data_stem, '{}_all_null.csv'.format(data_name))
+    null_fact_path = os.path.join(data_stem, '{}_all_factors.csv'.format(data_name))
+    pd.Series(rejects).to_csv(reject_path)
+    pd.Series(null_comps).to_csv(null_path)
+    null_factors.to_csv(null_fact_path)
+
+
+def pkl_routine():
+    data_path = "dmso_solubility/epa_internal/toxcast_insoluble_20mM_padel.tar"
+    pickle_jar = os.path.normpath(data_path)
+    data_name = 'tox_insoluble'
+    desc_df = retrieve_df(pickle_jar)
+    clean_df, all_empty, null_factors = initial_clean(desc_df)
+    print('Empty compounds...{}'.format(all_empty))
+    print('Non-null factors...{}'.format(null_factors))
+    print(clean_df.head())
+    store_rejects(os.path.dirname(pickle_jar), data_name, all_empty, null_factors)
+
+
+def main(desc_df: pd.DataFrame, data_name, sparse_thresh=0.10, nan_thresh=0.2):
+    print('Number of elements in dataframe: ', desc_df.size)
+    print('Detected {} N/A cells in dataframe.'.format(desc_df.isna().sum().sum()))
+    print('Detected {} 0 valued cells in dataframe.'.format(desc_df[desc_df == 0.].count().sum()))
+    # desc_df = desc_df.replace(np.nan, 0)
+    # print(desc_df[desc_df == 0].count(axis=0).sort_values(ascending=False))
+    # print(desc_df[desc_df == 0].count(axis=1).sort_values(ascending=False))
+    # print(desc_df[desc_df == 0].count(axis=1) / len(desc_df.columns))
+    # print(desc_df[desc_df == 0].count(axis=1) / len(desc_df.columns) < sparse_thresh)
+    zero_frac = desc_df[desc_df == 0].count(axis=0) / len(desc_df.index)
+    nan_frac = desc_df.isna().count(axis=0) / len(desc_df.index)
+    print(zero_frac)
+    print('Zeros\n', zero_frac)
+    # dense_df = desc_df[desc_df.columns[zero_frac < sparse_thresh]].copy()
+    dense_df = desc_df.dropna(axis=1)
+    sparse_df = desc_df[desc_df.columns[zero_frac >= sparse_thresh]].dropna(axis=1).copy()
+    dense_arr = np.array(dense_df)
+    sparse_arr = np.array(sparse_df)
+    print(dense_arr)
+    print('Dense, sparse: {}, {}'.format(dense_df.shape, sparse_df.shape))
+    cluster_path = "/dmso_solubility/CPDAT_test/cluster_test"
+    max_scaled_arr = sklearn.preprocessing.maxabs_scale(dense_arr)
+    max_scaled_all_arr = sklearn.preprocessing.maxabs_scale(dense_arr)
+    print(dense_df.count(axis=1).sort_values())
+    maxabs_path = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/list-chemicals"
+    print(max_scaled_arr)
+    # trial_agglom(max_scaled_arr)
+    # trial_cluster(max_scaled_arr, maxabs_path, data_name)
+    mappings = compute_tsne(max_scaled_all_arr)
+    agglo_labels = trial_agglom(max_scaled_arr, data_path=maxabs_path)
+    for labels in agglo_labels:
+        plot_mapping(mappings, clusters=labels)
+    exit()
+    desc_df.fillna(0, inplace=True)
+    dset = 'padel'
+    index = ['{}_{}'.format(dset, i) for i in desc_df.columns.tolist()]
+    # if desc_names:
+    #    df.set_index(desc_names)
+    if dset:
+        desc_df = desc_df.set_axis(index, axis='columns', copy=False)
+    # print(desc_df.columns)
+    # print(desc_df)
+    dense_df = desc_df.T
+    # dense_df = desc_df[desc_df != 0].dropna(axis='rows', thresh=int(desc_df.shape[0]/3)).copy()
+    sparse_mask = dense_df[dense_df == 0].count(axis=1) / len(dense_df.columns) < sparse_thresh
+    # print(sparse_mask)
+    dense_df = dense_df.loc[dense_df[dense_df == 0].count(axis=1) / len(dense_df.columns) < sparse_thresh]
+    print('Dense info', dense_df.info())
+    sparse_df = desc_df  # first_pass(desc_df.T[~sparse_mask].copy())
+    # print('Sparse info', sparse_df.info())
+
+    # print('After first pass: describe, variance/n/n.', nona_df.T.describe())
+    indices = np.where(dense_df == '')
+    # print(len(indices[0]), len(indices[1]))
+    dense_df.replace(to_replace='', value=float(0), inplace=True)
+    indices = np.where(dense_df == '')
+    # print(len(indices[0]), len(indices[1]))
+    # print('The rat bastard is at: {} {}/n/n/n'.format(indices[0], indices[1]))
+    # print('The rat bastard is a: {}/n/n/n'.format(type((dense_df == '').any().idxmax())))
+    # pd.to_numeric(dense_df[:], errors='coerce', downcast='float')
+    cont_df = dense_df
+    # print(cont_df.info())
+    indices = np.where(cont_df == '')
+    # print(len(indices[0]), len(indices[1]))
+    np.mean(cont_df.fillna(0))
+    sparse_feats = sparse_df.replace(to_replace='', value=float(0)).astype(np.float32).to_numpy(dtype=np.float32)
+    cont_feats = cont_df[(np.abs(stats.zscore(cont_df)) < z_thresh).all(axis=1)].to_numpy(dtype=np.float32)
+
+    # power_hour = sklearn.preprocessing.PowerTransformer()
+    # power_fit = power_hour.fit(cont_feats)
+    # cont_power = power_hour.transform(cont_feats)
+    # lowvar = sklearn.feature_selection.VarianceThreshold(threshold=0.01)
+    print(np.min(cont_feats), np.max(cont_feats))
+    print(stats.kurtosis(cont_feats))
+    print(cont_feats[0:5, 0:5])
+    # cont_center = sklearn.preprocessing.scale(cont_feats, axis=0, with_mean=True, with_std=False)
+    # print(np.size(cont_feats[:, 0:100]))
+    cont_center = sklearn.preprocessing.robust_scale(cont_feats, quantile_range=(0.1, 0.9))
+    # cont_log = sklearn.preprocessing.PowerTransformer().fit_transform(cont_center)
+    print(np.min(cont_center), np.max(cont_center))
+    print(stats.kurtosis(cont_center))
+    print(cont_center[0:5, 0:5])
+    # shuffler = np.random.shuffle(x=cont_log, axis=0)
+    # print(cont_log[0:5, 0:5])
+    # cont_0 = cont_log[0:int(cont_log.shape[0]/5)]
+    # print(cont_0[0:5, 0:5])
+    state = np.random.randint(1000000)
+    empcov = sklearn.covariance.MinCovDet(random_state=state)
+    cont_sfs = sklearn.feature_selection.SequentialFeatureSelector(empcov, direction="forward", n_jobs=5, cv=None)
+    cont_sfs.fit(X=cont_center)
+    cont_names = cont_sfs.get_feature_names_out()
+    print('Number of continuous features after variance and covariance selection.', len(cont_names))
+    cont_cov = empcov.covariance_
+    print('Continuous feature names: {}'.format(cont_names))
+    print('Matthews Correlation Coefficient: {}/n/n'.format(cont_sfs.scoring))
+    print('/n/nCov matrix')
+    # with open('cont_cov.pkl', 'w') as cov_pick:
+    #   pickle.dump(obj=cont_cov, file=cov_pick)
+    # plt.imshow(cont_cov)
+    # plt.show()
+    exit()
+    N_FEATURES_OPTIONS = [2, 4, 8]
+    C_OPTIONS = [1, 10, 100, 1000]
+    param_grid = [
+        {
+            "reduce_dim": [PCA(iterated_power=7)],
+            "reduce_dim__n_components": N_FEATURES_OPTIONS,
+            "classify__C": C_OPTIONS,
+        },
+        {
+            "reduce_dim": [GenericUnivariateSelect(mutual_info_regression)],
+            "reduce_dim__k": N_FEATURES_OPTIONS,
+            "classify__C": C_OPTIONS,
+        },
+    ]
+    reducer_labels = ["PCA", "GUS(mutual_info_classif)"]
+    pipe = sklearn.pipeline.Pipeline(param_grid, memory=True, verbose=True)
+    grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)
+    # grid.fit(cont_feats)
+    print('Best estimator: {}/n/n'.format(grid.best_estimator_))
+    print('Scoring: {}/n'.format(grid.scoring))
+    print('Error Score: {}'.format(grid.error_score))
+    print('Parameters: {}'.format(grid))
+
+
+'''
+    int_mutuals = GenericUnivariateSelect(mutual_info_classif)
+    cont_sfs = sklearn.feature_selection.SequentialFeatureSelector(sklearn.covariance.EmpiricalCovariance,
+                                                              scoring='euclidean',
+                                                              direction="backward")
+    cont_sfs.fit(cont_feats)
+    cont_names = cont_sfs.get_feature_names_out()
+    cont_cov = cont_sfs.estimator.covariance_
+    int_sfs = sklearn.feature_selection.SequentialFeatureSelector(sklearn.covariance.EmpiricalCovariance,
+                                                              scoring='euclidean',
+                                                              direction="backward")
+    int_sfs.fit(int_feats)
+    int_names = int_sfs.get_feature_names_out()
+
+    # filtered_df = cleaner.clean_descriptors(merged_df.T)
+'''
Index: dmso_model_dev/constants.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>ALL_ELEMENTS = list(['H',\r\n                     'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar',\r\n                     'K',\r\n                     'Ca', 'Sc', 'Ti',\r\n                     'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr',\r\n                     'Y',\r\n                     'Zr', 'Nb', 'Mo',\r\n                     'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce',\r\n                     'Pr',\r\n                     'Nd', 'Pm', 'Sm',\r\n                     'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt',\r\n                     'Au',\r\n                     'Hg', 'Tl', 'Pb',\r\n                     'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf',\r\n                     'Es',\r\n                     'Fm', 'Md', 'No',\r\n                     'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'])\r\nNONMETALS = list(['B', 'C', 'N', 'O', 'P', 'S'])\r\nNONMETALS_LOWER = list(['b', 'c', 'n', 'o', 'p', 's'])\r\nATYPICAL = list(['B', 'Si', 'Se', 'Te'])\r\nHALOGENS = list(['F', 'Cl', 'Br', 'I'])\r\nGROUP_ONES = list(['Li', 'Na', 'K', 'Rb', 'Cs', 'Fr'])\r\nGROUP_TWOS = list(['Be', 'Mg', 'Ca', 'Sr', 'Ba', 'Ra'])\r\nALLOWED = [*NONMETALS, *HALOGENS, *NONMETALS_LOWER, 'H']\r\nDISALLOWED = list([e for e in ALL_ELEMENTS if e not in ALLOWED])\r\nQSAR_COLUMNS = {'id': 'FOREIGN_KEY', 'cid': 'CID', 'sid': 'SID', 'casrn': 'CASRN', 'name': 'NAME', 'smiles': 'SMILES_QSAR',\r\n                'canonicalSmiles': 'SMILES_CANONICAL', 'inchi': 'INCHI', 'inchiKey': 'INCHI_KEY', 'mol': 'MOL'}\r\nDESC_COLUMNS = ['DESCRIPTOR_SET', 'VERSION_NUMBER', 'PLACEONE', 'PLACETWO', 'SMILES_QSAR', 'INCHI', 'INCHI_KEY', 'DESCRIPTORS']
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/constants.py b/dmso_model_dev/constants.py
--- a/dmso_model_dev/constants.py	
+++ b/dmso_model_dev/constants.py	
@@ -25,4 +25,4 @@
 DISALLOWED = list([e for e in ALL_ELEMENTS if e not in ALLOWED])
 QSAR_COLUMNS = {'id': 'FOREIGN_KEY', 'cid': 'CID', 'sid': 'SID', 'casrn': 'CASRN', 'name': 'NAME', 'smiles': 'SMILES_QSAR',
                 'canonicalSmiles': 'SMILES_CANONICAL', 'inchi': 'INCHI', 'inchiKey': 'INCHI_KEY', 'mol': 'MOL'}
-DESC_COLUMNS = ['DESCRIPTOR_SET', 'VERSION_NUMBER', 'PLACEONE', 'PLACETWO', 'SMILES_QSAR', 'INCHI', 'INCHI_KEY', 'DESCRIPTORS']
\ No newline at end of file
+DESC_COLUMNS = ['DESCRIPTOR_SET', 'VERSION_NUMBER', 'PLACEONE', 'PLACETWO', 'SMILES_QSAR', 'INCHI', 'INCHI_KEY', 'DESCRIPTORS']
Index: .idea/encodings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"Encoding\">\r\n    <file url=\"file://$PROJECT_DIR$/../data/dmso_solubility/CPDAT_test/cluster_test/tox_insoluble_centroids.csv\" charset=\"US-ASCII\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/encodings.xml b/.idea/encodings.xml
--- a/.idea/encodings.xml	
+++ b/.idea/encodings.xml	
@@ -1,6 +1,7 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="Encoding">
+    <file url="file://$PROJECT_DIR$/dmso_model_dev/padel_names.txt" charset="windows-1252" />
     <file url="file://$PROJECT_DIR$/../data/dmso_solubility/CPDAT_test/cluster_test/tox_insoluble_centroids.csv" charset="US-ASCII" />
   </component>
 </project>
\ No newline at end of file
Index: dmso_model_dev/models/RF_Imbalanced_01.py
===================================================================
diff --git a/dmso_model_dev/models/RF_Imbalanced_01.py b/dmso_model_dev/models/RF_Imbalanced_01.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/models/RF_Imbalanced_01.py	
@@ -0,0 +1,9 @@
+import numpy as np
+import pandas as pd
+from sklearn.ensemble import RandomForestClassifier
+from sklearn.cluster import HDBSCAN
+from sklearn.feature_selection import VarianceThreshold, SequentialFeatureSelector, mutual_info_classif, f_classif, chi2
+from sklearn.metrics import matthews_corrcoef, confusion_matrix, roc_curve, classification_report, balanced_accuracy_score, roc_auc_score, cohen_kappa_score, precision_recall_curve
+from sklearn.neural_network import MLPClassifier
+import imblearn
+
Index: dmso_model_dev/data_handling/dmso_preprocessing_runner.py
===================================================================
diff --git a/dmso_model_dev/data_handling/dmso_preprocessing_runner.py b/dmso_model_dev/data_handling/dmso_preprocessing_runner.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/dmso_preprocessing_runner.py	
@@ -0,0 +1,275 @@
+import os.path
+import pprint
+import pickle
+
+import pandas as pd
+import numpy as np
+from functools import partial
+import sklearn
+from sklearn.feature_selection import mutual_info_classif
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import ConfusionMatrixDisplay
+from sklearn.preprocessing import RobustScaler
+from sklearn.utils import check_X_y
+from dmso_model_dev.models import feature_selector
+
+sklearn.set_config(transform_output='pandas')
+
+FINAL_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/"
+COMBO_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/PADEL_CFP_COMBO_5mM.pkl"
+TUP_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/padel/PADEL_EPA_ENAMINE_5mM_TUPLES.pkl"
+TRAIN_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/MAXMIN_PADEL_TRAIN.pkl"
+STATS_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/stats/"
+
+
+# High-priority for testing!
+
+
+def set_options(**kwargs):
+    pd.set_option('display.precision', 3)
+    pd.set_option('format.precision', 3)
+    pass
+
+
+set_options()
+
+# Logging
+log_opts = {'filename': "newfile.log",
+            'style':    '{',
+            'filemode': 'w'}
+from sklearn.feature_selection import GenericUnivariateSelect
+
+# Other options
+robust = RobustScaler(unit_variance=True)
+checker = partial(check_X_y, accept_sparse=True, ensure_min_features=10, ensure_min_samples=30000, y_numeric=True)
+# Load Data
+with open(COMBO_DIR, 'rb') as f:
+    total_meta_df = pickle.load(f)
+with open(TRAIN_DIR, 'rb') as f:
+    train_tup = pickle.load(f)
+train_X, train_y = train_tup
+train_y = train_y.squeeze().astype(int)[train_X.index]
+checker(train_X, y=train_y)
+# sol_weighting = {1: 1, 0: 10}
+solubility_vec = total_meta_df['DMSO_SOLUBILITY'].squeeze()[train_X.index]
+from sklearn.utils.class_weight import compute_class_weight
+
+classes = np.unique(train_y)
+class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_y)
+class_weights = class_weights / np.min(class_weights)
+sol_weighting = dict(zip(classes, class_weights))
+cweights = solubility_vec.copy(deep=True).astype(float).map(sol_weighting)
+# print(solubility_vec)
+dataset_vec = total_meta_df['DATA_SOURCE'].squeeze()[train_X.index]
+source_converter = dict(zip(dataset_vec.unique(), list(range(dataset_vec.unique().size))))
+
+min_feats = {'sparse': 10, 'dense': 10, 'continuous': 20, 'discrete': 10}
+tolerances = {'tol_discrete': 0.05, 'tol_sparse': 0.01, 'sparsity': 0.01, 'cov_thresh': 0.75}
+pjobs = 6
+corr_meth = 'pearson'
+run_debug = False
+opt_dict = {'run_debug':  run_debug, 'corr_meth': corr_meth, 'class_wts': class_weights,
+            'sample_wts': cweights, 'pjobs': pjobs, 'data_dir': FINAL_DIR, 'stats_dir': STATS_DIR}
+opt_dict.update(**tolerances)
+opt_dict.update(**min_feats)
+cov_path = '{}cov.pkl'.format(FINAL_DIR)
+feature_frame_path = '{}feature_frame.pkl'.format(FINAL_DIR)
+# print(opt_dict.items())
+
+# FS_OPTS = namedtuple('FS_OPTS', field_names=[x for x in opt_dict.keys()])
+# fs_opts = FS_OPTS(*[v for k, v in opt_dict.items()])
+fs_loaded = False
+ci, use_vif, isolate = False, False, False
+fs = None
+robust.fit_transform = partial
+if not opt_dict['run_debug'] and os.path.isfile(feature_frame_path):
+    with open(feature_frame_path, 'rb') as f:
+        try:
+            fs = pickle.load(f)
+        except EOFError:
+            print('FeatureFrame file is invalid or corrupt.')
+            # os.remove(path=feature_frame_path)
+            fs_loaded = False
+    if fs is not None and type(fs) is feature_selector.FeatureSelectorTrainer:
+        fs_loaded = True
+    else:
+        print('Content from {} is not valid!!!'.format(feature_frame_path))
+if not fs_loaded:
+    fs = feature_selector.FeatureSelectorTrainer(train_X, labels=train_y, options=opt_dict, subsets=solubility_vec,
+                                                 logger=log_opts)
+if fs is None or type(fs) is not feature_selector.FeatureSelectorTrainer:
+    raise ValueError('FeatureSelectionTrainer was not instantiated!!!')
+print(fs.X.feat_frame.shape, flush=True)
+# fs.X.check_duplicate_feats()
+print(fs.X.feat_frame.shape, flush=True)
+if not opt_dict['run_debug'] and not os.path.isfile(feature_frame_path) and not fs.X.feat_frame.empty:
+    with open(feature_frame_path, 'wb') as f:
+        try:
+            pickle.dump(fs.X, f)
+        except pickle.PicklingError:
+            print('Pickling failed!')
+# Isolation Forest found no outliers for the command below.
+# isofor, pr = fs.isolate_observations(n_samples=0.1, n_est=0.05 * fs.X.feat_frame.shape[1])
+#  predicted = pd.DataFrame(pr, index=fs.X.feat_frame.index)
+# outliers = predicted[predicted == -1].index
+# fs.X.feat_frame = fs.X.feat_frame.drop(index=outliers, inplace=True)
+# print('Outliers based on Isolation Forest method:\n'.format(outliers))
+if os.path.isfile(cov_path) and os.path.getsize(cov_path) > 10000:
+    with open(cov_path, 'rb') as f:
+        fs.calc_cov_mat(calc_cov=pickle.load(f) / (class_weights.sum() - 1))
+        print('Covariance matrix has been loaded with shape: {}'.format(fs.corr_mat.shape))
+
+elif not opt_dict['run_debug'] and (
+        fs.corr_mat is None or (type(fs.corr_mat) is not pd.DataFrame or fs.corr_mat.empty)):
+    print('Calculating covariance matrix.')
+    fs.calc_cov_mat()
+    if fs.corr_mat is not None and fs.corr_mat.shape[1] == fs.X.feat_frame.shape[1]:
+        print('Covariance matrix calculated successfully!!!')
+    else:
+        print('Failed to calculated covariance matrix!!!')
+else:
+    print('Not calculating covariance matrix this run.')
+if not opt_dict['run_debug']:
+    print('Calculating covariance matrix', flush=True)
+    fs.calc_cov_mat()
+if fs.corr_mat is not None and type(fs.corr_mat) is pd.DataFrame and not fs.corr_mat.empty:
+    with open(cov_path, 'wb') as f:
+        print('Saving covariance matrix.')
+        pickle.dump(obj=fs.corr_mat, file=f)
+else:
+    print('Covariance matrix could not be calculated.')
+cov_pairs = fs.set_cov_pairs(cov_thresh=opt_dict['cov_thresh'])
+if not (cov_pairs is None or len(cov_pairs) > 0):
+    print('Correlated feature pairs:\n{}'.format(cov_pairs))
+else:
+    print('No correlated features were found.')
+# logging.info('Covariance threshold is set to {} for highly correlated feature pairs.'.format(opt_dict['cov_thresh']))
+'''
+fs.set_condition_num()
+with open('{}condition_num.pkl'.format(STATS_DIR), 'wb') as f:
+    try:
+        pickle.dump(obj=fs.condition_num, file=f)
+    except:
+        pickle.dump(obj=fs.condition_num, file=f)
+logging.info('Feature with Condition Number over 5:\n{}'.format(fs.condition_num))
+'''
+if use_vif and not opt_dict['run_debug']:
+    vif = fs.multi_collin_feats()
+    if not opt_dict['run_debug'] and type(vif) is pd.DataFrame and not vif.empty:
+        print('Saving VIF.')
+        vif.to_csv('{}vif.csv'.format(STATS_DIR))
+    print('VIF Factor:\n{}'.format(vif))
+    # for feat, factor in vif.items():
+
+duped_feats = fs.X.cont.intersection(fs.X.discrete)
+print('Duplicated features:\n{}'.format(duped_feats))
+continuous_df = (fs.X.feat_frame[fs.X.cont]).copy(deep=True)
+if len(duped_feats.tolist()) == 0:
+    continuous_df.drop(columns=duped_feats, inplace=True)
+discrete_df = fs.X.feat_frame[fs.X.discrete]
+# discrete_df = KBinsDiscretizer(encode='ordinal', strategy='kmeans', subsample=None, random_state=0).fit_transform(X=disc_df, y=fs.y)
+# mi_disc = partial(mutual_info_classif, discrete_features=True, random_state=0)
+# mi_cont = partial(mutual_info_classif, discrete_features=False, random_state=0)
+if continuous_df is None or continuous_df.empty:
+    print('Continuous feature DF is empty!!!')
+    print(fs.X.dense.intersection(fs.X.cont), flush=True)
+    print(fs.X.dense, flush=True)
+    print(fs.X.cont, flush=True)
+    raise ValueError
+cont_arr, y_cont = checker(RobustScaler(unit_variance=True).fit_transform(X=continuous_df.copy(deep=True), y=fs.y),
+                           y=fs.y)
+# print('Continuous features and target:\n{}'.format(cont_df), flush=True)
+# print(fs.y.head(), flush=True)
+# cont_sample, y_sample = sklearn.utils.resample(cont_arr, y_cont, replace=False, n_samples=int(0.1 * cont_arr.shape[0]),random_state=0, stratify=y_cont)
+'''
+sel_cont = GenericUnivariateSelect(score_func=mutual_info_classif, mode='fpr')
+sel_cont.fit(X=cont_sample, y=y_sample)
+print('P-values from MI feature selection of continuous features:\n{}'.format(sel_cont.pvalues_), flush=True)
+if sel_cont.pvalues_ is None or len(sel_cont.pvalues_) == 0:
+    assert ValueError('MI feature selector contains no p-values!!!')
+else:
+    mi_cont_df = sel_cont.transform(X=cont_df)
+    print('Continuous features selected by forward BA-scored MI: {}'.format(mi_cont_df.columns), flush=True)
+    '''
+cont_mi_clf = mutual_info_classif(X=cont_arr, y=y_cont, discrete_features=False, random_state=0)
+# print('Continuous mutual info scores: \n{}'.format(sorted(cont_mi_clf), reversed=True), flush=True)
+
+# sfs_disc = SequentialFeatureSelector(estimator=sel_disc, scoring='balanced_accuracy', n_jobs=-1)
+disc_arr, y_disc = checker(RobustScaler(unit_variance=True).fit_transform(X=discrete_df.copy(deep=True), y=fs.y),
+                           y=fs.y)
+disc_mi_clf = mutual_info_classif(X=disc_arr, y=y_disc, discrete_features='auto', random_state=0)
+'''
+sel_disc = GenericUnivariateSelect(score_func=mi_disc, mode='fpr')
+mi_disc_df = sel_disc.fit_transform(X=discrete_df, y=fs.y)
+# sfs_cont = fs.seq_linear_select(df=fs.X.feat_frame[fs.X.cont], label=fs.y, method=mi_disc)
+mi_disc_feats = mi_disc_df.columns
+mi_cont_feats = mi_cont_df.columns
+'''
+mi_cont_feats = pd.Series(data=cont_mi_clf, index=continuous_df.columns)
+mi_disc_feats = pd.Series(data=disc_mi_clf, index=discrete_df.columns)
+dup_feats = continuous_df.columns.intersection(discrete_df.columns)
+if len(duped_feats.tolist()) > 0:
+    mi_cont_feats.drop(columns=dup_feats)
+# print('Duplicated columns:\n{}'.format(dup_feats))
+
+
+n_mi_feats = 30
+num_cont_feats = int(4 * n_mi_feats / 5)
+num_disc_feats = int(n_mi_feats / 5)
+top_cont_feats = mi_cont_feats.sort_values(ascending=False)[:num_cont_feats]
+top_disc_feats = mi_disc_feats.sort_values(ascending=False)[:num_disc_feats]
+pprint.pprint('Top {} continuous features from supervised MI using sequential selection:{}'.format(num_cont_feats,
+                                                                                                   top_cont_feats),
+              compact=True)
+pprint.pprint(
+    'Top {} discrete features from supervised MI using sequential selection: {}'.format(num_disc_feats, top_disc_feats),
+    compact=True)
+try:
+    scaled_mi_feats = pd.concat([top_cont_feats, mi_cont_feats])
+except:
+    scaled_mi_feats = pd.concat([top_disc_feats, mi_cont_feats], axis=1)
+
+# pprint.pprint(scaled_mi_feats.index, compact=True)
+# Build "basic most-likely" model.
+import matplotlib.pyplot as plt
+
+if scaled_mi_feats is not None and not scaled_mi_feats.empty:
+    # Imbalanced RF
+    from sklearn.model_selection import StratifiedGroupKFold
+    from imblearn.ensemble import BalancedRandomForestClassifier
+
+    mi_typed = fs.X.feat_frame[scaled_mi_feats.index].astype(np.float64)
+    for i, (dev_ind, eval_ind) in StratifiedGroupKFold().split(X=mi_typed, y=fs.y, groups=fs.y):
+        fig, axes = plt.subplots(nrows=2, ncols=2, sharex='col', sharey='row')
+        imrf = BalancedRandomForestClassifier(n_estimators=250, criterion='entropy', n_jobs=-1, random_state=0,
+                                              verbose=1, class_weight='balanced', ccp_alpha=0.005).fit(
+            X=mi_typed[dev_ind], y=fs.y[dev_ind])
+        with open('{}imb_rf__mi{}_scaled_{}.pkl'.format(STATS_DIR, n_mi_feats, i), 'wb') as f:
+            pickle.dump(imrf, file=f)
+        ConfusionMatrixDisplay.from_estimator(estimator=imrf, X=mi_typed[dev_ind], y=y_cont[dev_ind],
+                                              normalize='true', display_labels=np.array(['Insoluble', 'Soluble']),
+                                              ax=axes[0, 0],
+                                              colorbar=False).plot(xticks_rotation=30)
+        ConfusionMatrixDisplay.from_estimator(estimator=imrf, X=mi_typed, y=y_cont,
+                                              normalize='true', display_labels=np.array(['Insoluble', 'Soluble']),
+                                              ax=axes[1, 0],
+                                              colorbar=False).plot(xticks_rotation=30)
+        # Logistic Regression Model
+        mi_lr = LogisticRegression(penalty='l2', max_iter=25000, solver='newton-cg', class_weight='balanced', n_jobs=7,
+                                   random_state=0).fit(
+            mi_typed, y=fs.y)
+        with open('{}logit_ncgl2_mi{}_scaled_{}.pkl'.format(STATS_DIR, n_mi_feats, i), 'wb') as f:
+            pickle.dump(mi_lr, file=f)
+        ConfusionMatrixDisplay.from_estimator(estimator=mi_lr, X=mi_typed, y=y_cont,
+                                              normalize='true', display_labels=np.array(['Insoluble', 'Soluble']),
+                                              ax=axes[0, 1],
+                                              colorbar=False).plot(xticks_rotation=30)
+        ConfusionMatrixDisplay.from_estimator(estimator=mi_lr, X=mi_typed, y=y_cont,
+                                              normalize='true', display_labels=np.array(['Insoluble', 'Soluble']),
+                                              ax=axes[1, 1],
+                                              colorbar=False).plot(xticks_rotation=30)
+else:
+    print('Scaled MI features DF is empty!!!')
+
+# RFECV(estimator=mi_lr, scoring='balanced_accuracy, n_jobs=4').
+# Highest VIF/Condition_Num -> Eliminate n feats -> Reevaluate models -> Continue until {stop} features.
Index: dmso_model_dev/data_handling/__init__.py
===================================================================
diff --git a/dmso_model_dev/data_handling/__init__.py b/dmso_model_dev/data_handling/__init__.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/data_handling/__init__.py	
@@ -0,0 +1,1 @@
+from . import *
\ No newline at end of file
Index: dmso_model_dev/dmso_data/epa_enamine_data_loading.pyi
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import itertools\r\nimport sys\r\nimport pickle\r\n\r\nimport imblearn.metrics\r\nimport numpy as np\r\nimport pandas as pd\r\nimport plotly.express\r\nimport scipy.ndimage\r\nimport sklearn.feature_selection\r\nimport sklearn.linear_model\r\n\r\n# from rdkit.ML.Cluster import Butina\r\n# from rdkit.DataManip.Metric import GetTanimotoDistMat, GetTanimotoSimMat\r\nfrom dmso_model_dev.constants import DISALLOWED\r\n# from rdkit.DataStructs.cDataStructs import CreateFromBinaryText\r\nfrom dmso_model_dev.DescriptorRequestor import DescriptorGrabber\r\n# from dmso_model_dev.data_handling.descriptor_preprocessing import ks_stats, kde_grouped\r\nimport dmso_model_dev.data_handling.padel_categorization as padel_categorization\r\nimport dmso_model_dev.data_handling.descriptor_preprocessing as descriptor_preprocessing\r\n\r\nsys.path.insert(0, 'C:/Users/mmanning/PycharmProjects/compLoiel/dmso_model_dev/data_handling')\r\nsys.path.insert(0, 'C:/Users/mmanning/PycharmProjects/compLoiel/dmso_model_dev/')\r\n\r\nFP_MAPPER = col_mapper = dict(zip(('info', 'name', 'version', 'options', 'type', 'radius', 'bits', 'chemicals', 'smiles', 'inchi', 'inchiKey', 'descriptors'), ('FP_SOFTWARE', 'FP_NAME', 'FP_VERSION', 'FP_OPTIONS', 'FP_TYPE', 'FP_RADIUS', 'FP_BITS', 'FP_COMPOUNDS', 'FP_SMILES', 'FP_INCHI', 'FP_INCHI_KEY', 'FP_DESCRIPTORS')))\r\n\r\nFP_MULTIINDEX = pd.MultiIndex.from_tuples([('FP_SOFTWARE', 'FP_NAME', 'FP_VERSION'), ('FP_OPTIONS', 'FP_TYPE', 'FP_RADIUS', 'FP_BITS'),\r\n                           ('FP_COMPOUNDS', 'FP_INCHI', 'FP_INCHI_KEY',\r\n                            'FP_DESCRIPTORS')])  # \"?smiles=CCCCCCC&headers=true&type=cfp&radius=4&bits=1024\r\ncfp_radius = 4\r\nfp_num_bits = 1024\r\nfp_name = 'cpf'\r\nCOL_LABELS = padel_categorization.padel_names\r\nDROP_STRINGS = DISALLOWED\r\nDROP_STRINGS.append('SH')\r\nDATA_KEYS = ['epa_sol', 'epa_in', 'en_sol', 'en_in']\r\ncache_dir = \"C:/Users/mmanning/PycharmProjects/data/tmp_cache.pkl\"\r\nDATA_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/PADEL_EPA_ENAMINE_5mM.pkl\"\r\nTRAIN_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/MAXMIN_PADEL_TRAIN.pkl\"\r\nTEST_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/MAXMIN_PADEL_TEST.pkl\"\r\nCOMBO_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/PADEL_CFP_COMBO_5mM.pkl\"\r\nTUP_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/padel/PADEL_EPA_ENAMINE_5mM_TUPLES.pkl\"\r\ndist_path = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/circ_fingerprints/TANIMOTO_R{}_{}.pkl\".format(cfp_radius, fp_num_bits)\r\nDROPPED_DIR = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/PADEL_EPA_ENAMINE_5mM_FILTERED.pkl\"\r\n\r\nnew_desc_list, cfp_df_list, padel_df_list, new_cfps, fp_list = list(), list(), list(), list(), list()\r\ndropped_dict = dict()\r\ndropped_dict['total'] = list()\r\n'''\r\n# TODO: Make INCHI_KEYS the index.\r\n# TODO: Verify agreement with descriptor (ie. Padel) API output.\r\n# TODO: Incorporate class weights into data structures.\r\n# Recover cache.\r\nwith (open(TUP_DIR, 'rb') as f):\r\n    tups = pickle.load(f)\r\nwith open(cache_dir, 'rb') as f:\r\n    while True:\r\n        try:\r\n            fp_list.append(pickle.load(f))\r\n        except EOFError:\r\n            break\r\ncfp_total = pd.DataFrame.from_records(data=fp_list)\r\n# Change dtypes to categorical to save memory.\r\ncfp_total['name'] = cfp_total['name'].astype('category')\r\ncfp_total['version'] = cfp_total['version'].astype('category')\r\ncfp_total['type'] = cfp_total['type'].astype('category')\r\ncfp_total['radius'] = cfp_total['radius'].astype('category')\r\ncfp_total['DATA_SOURCE'] = cfp_total['DATA_SOURCE'].astype('category')\r\ncfp_total['DMSO_SOLUBILITY'] = cfp_total['DMSO_SOLUBILITY'].astype('category')\r\ncfp_total.rename(mapper=FP_MAPPER, axis='columns', inplace=True)\r\ncfp_total.set_index(keys='FP_INCHI_KEY', drop=False, inplace=True)\r\ndrop_comps = cfp_total[cfp_total['FP_SMILES'].isin(DROP_STRINGS)].index\r\nif not drop_comps.size > 0:\r\n    cfp_total.drop(index=drop_comps, inplace=True)\r\ndropped_dict['total'].append(drop_comps)\r\ncfp_dups = cfp_total.index.duplicated()\r\nif not cfp_dups.size > 0:\r\n    cfp_total.drop(index=cfp_dups, inplace=True)\r\ndropped_dict['total'].append(cfp_dups)\r\nif True:\r\n    for key, value in tups.items():\r\n        # Filter out problematic compounds and eliminate NaN while preserving info.\r\n        dropped_indices = pd.concat([value[1][value[1]['SMILES_QSAR'].str.contains(s)] for s in DROP_STRINGS]).index\r\n        [df.drop(index=[a for a in dropped_indices if a in df.index], inplace=True) for df in value]\r\n        comp_na_dropped = value[2].dropna(axis=0, thresh=len(COL_LABELS) - 50).index.symmetric_difference(value[2].index)\r\n        value[2].drop(index=comp_na_dropped, inplace=True)\r\n        feat_na_dropped = value[2].dropna(axis=1, thresh=value[2].shape[0] * .9).columns.symmetric_difference(value[2].columns)\r\n        value[2].drop(index=feat_na_dropped, inplace=True)\r\n        final_na_dropped = value[2].dropna(axis=0).index.symmetric_difference(value[2].index)\r\n        value[2].drop(index=final_na_dropped, inplace=True)\r\n        value[1].drop(value[1].index.difference(value[2].index), inplace=True)\r\n        value[0].drop(value[0].index.difference(value[2].index), inplace=True)\r\n        if 'DESCRIPTORS' in value[1].columns:\r\n            value[1].drop(columns='DESCRIPTORS', inplace=True)\r\n        # print(value[2].columns[:5])\r\n        # print(value[2].isna().sum(axis=0).sort_values(ascending=False)[:7])\r\n        # print(value[2].isna().sum(axis=1).sort_values(ascending=False)[:7])\r\n        # new_desc_df = pd.concat([value[0], value[1], value[2]], axis=1)\r\n        new_desc_df = value[2]\r\n        new_desc_df.attrs['type'] = 'PADEL'\r\n        new_desc_df.attrs['version'] = '2.21'\r\n        if 'epa' in key:\r\n            new_desc_df.__setattr__('source', 'EPA')\r\n            source = 'EPA'\r\n        elif 'en' in key:\r\n            new_desc_df.__setattr__('source', 'ENAMINE')\r\n            source = 'ENAMINE'\r\n        if 'sol' in key:\r\n            new_desc_df.__setattr__('dmso_soluble', 1)\r\n            dmso = 1\r\n        elif 'in' in key:\r\n            new_desc_df.__setattr__('dmso_soluble', 0)\r\n            dmso = 0\r\n        value[1].drop(columns=['PLACEONE', 'PLACETWO'], inplace=True)\r\n        value[0].rename('Solubility', inplace=True)\r\n        [a.rename(mapper=dict(zip(a.columns, padel_categorization.padel_names)), axis='columns', inplace=True) for a in\r\n         new_desc_list if 1443 in a.columns]\r\n        fp_padel_diff = cfp_total.index.symmetric_difference(new_desc_df.index)\r\n        cfp_total.drop(index=[a for a in fp_padel_diff if a in cfp_total.columns], inplace=True)\r\n        new_desc_df.drop(index=[a for a in fp_padel_diff if a in new_desc_df.columns], inplace=True)\r\n        padel_df_list.append(value[1])\r\n        new_desc_list.append(new_desc_df)\r\n        dropped_dict[key] = [dropped_indices, comp_na_dropped, feat_na_dropped, final_na_dropped, fp_padel_diff]\r\n\r\n    def splitter(dfs_dict):\r\n        test_list = []\r\n        train_list = []\r\n        for key, df in dfs_dict.items():\r\n            if 'in' in key:\r\n                all_labels = pd.Series(index=df.index, data=0)\r\n            elif 'sol' in key:\r\n                all_labels = pd.Series(index=df.index, data=1)\r\n            train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(df, all_labels, train_size=0.8, random_state=0)\r\n            test_list.append((test_label, test_data))\r\n            train_list.append((train_label, train_data))\r\n        train_df = (pd.concat([x[0] for x in train_list]), pd.concat([x[1] for x in train_list]))\r\n        test_df = (pd.concat([x[0] for x in test_list]), pd.concat([x[1] for x in test_list]))\r\n        return train_df, test_df\r\n    with open(cache_dir, 'ab', buffering=0) as f:\r\n        new_smiles = value[1]['SMILES_QSAR'].to_list()\r\n        new_inchikeys = value[1].index.tolist()\r\n        cfp_header = {'headers': 'true', 'type': fp_name, 'radius': cfp_radius, 'bits': fp_num_bits}\r\n        fingerprinter = DescriptorGrabber(desc_set='cfp', api_url=\"https://hazard-dev.sciencedataexperts.com/api/rdkit\", payload=cfp_header)\r\n        # TODO: Cache file for API calls.\r\n        with requests.session() as r:\r\n            for ik, smi in zip(new_inchikeys, new_smiles):\r\n                fp_return, _ = fingerprinter.make_api_call(payload_input=smi, api_session=r)\r\n                fp_return['INCHI_KEY'] = ik\r\n                if 'descriptors' not in list(fp_return.keys()):\r\n                    print(_)\r\n                    fp_return['descriptors'] =  np.NaN\r\n                    fp_return['FP_BIT_VECTOR'] = np.NaN\r\n                else:\r\n                    bitvec = ''.join([str(i) for i in fp_return['descriptors']]).strip('[').strip(']').replace(', ', '')\r\n                    fp_return['FP_BIT_VECTOR'] = bitvec\r\n                fp_return['DATA_SOURCE'] = source\r\n                fp_return['DMSO_SOLUBILITY'] = dmso\r\n                pickle.dump(fp_return, f)\r\n                new_cfps.append(pd.Series(fp_return))\r\n    # new_cfp_df = pd.concat(new_cfps)\r\n    # new_cfp_df = pd.DataFrame.from_records(new_cfps, orient='columns').rename_axis(columns=FP_MAPPER).set_index(keys='INCHI_KEY', drop=True, inplace=True)\r\n    # new_cfp_df = pd.concat([pd.Series(x[0]) for x in new_cfps], axis=1).T.rename_axis(columns={'inchiKey': 'INCHI_KEY'}).set_index('INCHI_KEY', drop=True, inplace=True)\r\n    # cfp_df_list.append(new_cfp_df)\r\ncfp_total = pd.concat(new_cfps)\r\ncfp_df_path = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/circ_fingerprints/PADEL_TANIMOTO_COMBINED_5mM.pkl\"\r\ncfp_total.to_pickle(cfp_df_path)\r\n''\r\n# Combine DataFrames\r\ntotal_padel_df = pd.concat(new_desc_list)\r\ndesc_duplicated = total_padel_df.index.duplicated()\r\nif not desc_duplicated.size > 0:\r\n    total_padel_df.drop(index=desc_duplicated.index, inplace=True)\r\ntotal_padel_df.to_pickle(DATA_DIR)\r\ndropped_dict['total'].append(desc_duplicated)\r\npadel_meta_df = pd.concat(padel_df_list)\r\npadel_dups = padel_meta_df.index.duplicated()\r\nif not padel_dups.size > 0:\r\n    padel_meta_df.drop(index=padel_dups, inplace=True)\r\ndropped_dict['total'].append(padel_dups)\r\n# Save list of dropped compounds\r\nwith open(DROPPED_DIR, 'wb') as f:\r\n    pickle.dump(dropped_dict, f)\r\nprint(padel_meta_df.columns, cfp_total.columns)\r\ntotal_meta_df = pd.merge(left=padel_meta_df, right=cfp_total, left_index=True, right_index=True)\r\n#missing_cols = [(i, a) for i, a in enumerate(set(itertools.chain.from_iterable([df[2].columns for df in tups.values()]))) if not all([a in b[2].columns for b in tups.values()])]\r\ndel dropped_dict, padel_df_list, cfp_df_list\r\nwith open(COMBO_DIR, 'wb') as f:\r\n    pickle.dump(total_meta_df, f)\r\n\r\ndel fp_list, padel_meta_df, cfp_total\r\n# rdk_vecs = dict([(key, CreateFromBinaryText(bits)) for key, bits in cfp_total['FP_BIT_VECTOR'].items()])\r\n# .drop(2366) for Fe[CO]4 in EPA Soluble set, which gave nan for cfp.\r\n\r\nfrom scipy.spatial.distance import jaccard, rogerstanimoto\r\nfrom scipy.spatial.distance import pdist\r\ntwod_bits = np.vstack(np.array(cfp_total['descriptors'].values))\r\nrogers_arr = pdist(twod_bits['descriptors'].to_numpy, metric='rogerstanimoto')\r\ntani_sim_arr = GetTanimotoSimMat(list(rdk_vecs.values()))\r\ntani_dis_arr = -np.log2(tani_sim_arr)\r\ndist_mat = rogers_arr\r\n'''\r\n\r\n# np.save(dist_path, dist_mat)\r\n'''\r\nimport deepchem.data\r\nfrom rdkit.SimDivFilters import MaxMinPicker\r\n\r\nwith open(COMBO_DIR, 'rb') as f:\r\n    total_meta_df = pickle.load(f)\r\nwith open(dist_path, 'rb') as f:\r\n    dist_mat = pickle.load(f)\r\nassert total_meta_df.shape[0] > 0\r\ncweights = total_meta_df['DMSO_SOLUBILITY'].astype(int).replace({1: 1, 0: 10})\r\nrog_set = deepchem.data.NumpyDataset(dist_mat, y=total_meta_df['DMSO_SOLUBILITY'].to_numpy(), w=cweights)\r\ndel dist_mat\r\nmms = MaxMinPicker()\r\nn_test = int(total_meta_df.shape[0] / 5)\r\nn_train = int(total_meta_df.shape[0] - n_test)\r\nmms_picks = list(mms.Pick(distMat=rog_set.X, poolSize=int(total_meta_df.shape[0]), pickSize=int(n_test), seed=0))\r\ndel rog_set\r\nwith open(DATA_DIR, 'rb') as f:\r\n    total_padel_df = pickle.load(f)\r\n# from rdkit.Chem.Fingerprints.ClusterMols import GetDistanceMatrix\r\n# from rdkit.DataManip.Metric.rdMetricMatrixCalc import GetEuclideanDistMat\r\n# dist_arr = GetEuclideanDistMat(list(rdk_vecs.values()))\r\n# from rdkit.DataStructs.cDataStructs import CreateFromBinaryText, SparseBitVect, ExplicitBitVect\r\n# from deepchem.splits import Splitter\r\n# from rdkit.DataStructs import BulkTanimotoSimilarity\r\nassert total_padel_df.shape[0] == total_meta_df.shape[0]\r\nprint(total_padel_df.shape, total_meta_df.shape)\r\n# Diversity Optimization\r\n\r\ntest = total_meta_df.index[mms_picks]\r\ntrain = total_meta_df.index.symmetric_difference(test)\r\nprint(test.shape, '\\n', train.shape)\r\nassert test.shape[0] > 0\r\nassert train.shape[0] > 0\r\ntrain_meta, test_meta = total_meta_df.loc[train], total_meta_df.loc[test]\r\n# train_missing, test_missing = train_meta.loc[train.difference(total_padel_df.index)], test_meta.loc[test.difference(total_padel_df.index)]\r\n# print(train_missing['FP_SMILES'], test_missing['FP_SMILES'])\r\n# TODO: Optimize overlap/intersection code.\r\n# TODO: Store trimmed data and load from file instead of rerunning cleaning code.\r\n# TODO: Implement assert methods at beginning to validate incoming data before any operations.\r\ntrain_X = total_padel_df.loc[train]\r\ntest_X = total_padel_df.loc[test]\r\ntrain_y = train_meta['DMSO_SOLUBILITY'].to_frame()\r\ntest_y = test_meta['DMSO_SOLUBILITY'].to_frame()\r\ncheck_X_y(train_X, train_y)\r\ncheck_X_y(test_X, test_y)\r\n\r\nwith open(TRAIN_DIR, 'wb') as f:\r\n    pickle.dump(train_tup, f)\r\nwith open(TEST_DIR, 'wb') as f:\r\n    pickle.dump(test_tup, f)\r\n'''\r\n# Graphing Options\r\nfrom plotly.express import scatter\r\nimport plotly.io as pio\r\nfrom plotly.express.colors import qualitative, sequential, diverging, colorscale_to_colors\r\nvivid_colors = qualitative.Vivid\r\nsol_colors = [qualitative.Light24[0], qualitative.Light24[13]]\r\npio.renderers.default = 'browser'\r\n# Load Data\r\nfrom sklearn.utils.validation import check_X_y\r\nwith open(COMBO_DIR, 'rb') as f:\r\n    total_meta_df = pickle.load(f)\r\nwith open(TRAIN_DIR, 'rb') as f:\r\n    train_tup = pickle.load(f)\r\ntrain_X, train_y = train_tup\r\ntrain_y = train_y.squeeze().astype(int)\r\nsklearn.utils.check_X_y(train_X, train_y, y_numeric=True)\r\n# Cluster and display dimensionality reduction of descriptors\r\n# plotmarkers = pd.concat([total_meta_df['DATA_SOURCE'].loc[train], total_meta_df['DATA_SOURCE'].loc[test]])\r\n# TODO: Implement splitting loop.\r\nestate = [a for a in train_X.columns if 'e-state' in a.lower()]\r\nbcut = [a for a in train_X.columns if 'bcut' in a.lower()]\r\nmatrix = [a for a in train_X.columns if 'matrix' in a.lower()]\r\nfrom dmso_model_dev.models.feature_selector import pd_sparse_feats\r\nsparse = pd_sparse_feats(train_X)\r\ndense = train_X.columns.symmetric_difference(sparse)\r\n# from sklearn.feature_selection import VarianceThreshold, SelectFromModel, SequentialFeatureSelector\r\n# from sklearn.neural_network import MLPClassifier\r\n# from sklearn.decomposition import PCA, KernelPCA\r\n# from sklearn.metrics import make_scorer, adjusted_mutual_info_score\r\n# from functools import partial, partialmethod\r\nprint(train_X.shape)\r\n\r\n# x_sample = train_X.sample(axis=1, frac=0.2, random_state=0)\r\n# TODO: Optimize this sorting.\r\nsparse_df = train_X[(train_X.nunique(dropna=False, axis=0) <= 2).index]\r\ndense_df = train_X[train_X.columns.symmetric_difference(sparse_df.columns)]\r\n\r\nserlist = [(col, ser.value_counts(normalize=True)) for col, ser in train_X.items()]\r\n# sorted(serlist, key=lambda x: max(x[1].sort_values(ascending=False)))\r\nfreq_list = [(s[1].iloc[0], s[0]) for s in serlist if s[1].shape[0] > 1]\r\n# sorted(freq_list, key=lambda x: x[0], reverse=True)\r\nx_var = pd.Series(index=train_X.columns).value_counts()\r\nprint(x_var[:10])\r\nfor col, ser in train_X.items():\r\n    if col in sparse:\r\n        x_var[col] = 1\r\n    else:\r\n        x_var[col] = ser.var()\r\nprint(x_var.index.difference(train_X.columns))\r\nx_var = train_X.var()\r\n# plotly.express.scatter(y=x_var.add(1).sort_values(), log_y=True).show()\r\nX_train = train_X[(x_var > 0).index]\r\nn_feats = 100\r\n# Remove correlated features.\r\nnot_corr = False\r\nif not_corr:\r\n    X_corr = dense_df.corr(method='kendall').abs()\r\n    x_na_corr = X_corr.isna().astype(int)\r\n    col_na = x_na_corr.sum(axis=0) < X_corr.shape[1]-1\r\n    print(col_na)\r\n    X_corr_val = X_corr.loc[col_na][col_na]\r\n    print(X_corr_val, X_corr_val.shape)\r\n    # X_corr.dropna(how='all', axis=0, inplace=True)\r\n    # X_corr = X_corr[X_corr.index.tolist()]\r\n    print(X_corr.shape)\r\n    print(X_corr[:10])\r\n    corr_sorted = X_corr.unstack().sort_values(kind='quicksort', ascending=False)\r\n    print('Sorted Correlation values:\\n{}'.format(corr_sorted[:10]))\r\n    pairs_to_drop = set()\r\n    for i in range(0, X_train.shape[1]):\r\n        for j in range(0, i + 1):\r\n            pairs_to_drop.add((X_train.columns[i], X_train.columns[j]))\r\ncweights = train_y.replace({1: 1, 0: 10})\r\nX_train_robust = sklearn.preprocessing.RobustScaler(quantile_range=(0.05, 0.95), unit_variance=False).fit_transform(X_train)\r\nX_train_robust_unit = sklearn.preprocessing.RobustScaler(quantile_range=(0.05, 0.95), unit_variance=True).fit_transform(X_train)\r\nX_train_normal = sklearn.preprocessing.Normalizer().fit_transform(X_train)\r\nlinear_dim_reduction = False\r\nif linear_dim_reduction:\r\n    if dense_df.shape[1] < 50:\r\n        score_f = ElasticNetCV(n_jobs=n_parallel, random_state=0, max_iter=5000, tol=1e-5, cv=3)\r\n    else:\r\n        score_f = LassoCV(fit_intercept=False, max_iter=2500, tol=1e-5, n_jobs=n_parallel, random_state=0, cv='balanced')\r\n    selector = sklearn.feature_selection.SequentialFeatureSelector(estimator=score_f, scoring='balanced_accuracy', n_jobs=n_parallel)\r\n# selector = sklearn.feature_selection.SelectFromModel(estimator=score_f, max_features=n_feats, prefit=True, threshold='median')\r\n# Model running options.\r\nplot_tsne = True\r\nplot_logreg = False\r\nforest = True\r\nn_parallel = 4\r\nfeat_group_names = ['Sparse', 'Dense', 'Matrix-based', 'E-States', 'All Features']\r\nfeat_selector_bool = [False, False, True, True, True]\r\nselector_dict = dict(zip(feat_group_names, feat_selector_bool))\r\nfeat_col_list = [sparse_df, dense_df, matrix, estate, X_train.columns]\r\nfor i, feat_cols in zip(feat_group_names, feat_col_list):\r\n    dense_in = [c for c in feat_cols if c in dense_df.columns.to_list()]\r\n    sparse_in = [c for c in feat_cols if c in sparse_df.columns.to_list()]\r\n    if len(feat_cols) > 50 and not feat_cols.equals(dense_df.columns) and not feat_cols.equals(sparse_df.columns):\r\n            best_feats = pd.concat([selector.fit_transform(X_train_robust_unit[dense_in], train_y), X_train[sparse_in]])\r\n    elif len(feat_cols) > 50 and feat_cols.equals(dense_df.columns):\r\n        best_feats = selector.fit_transform(X_train_robust_unit[dense_in])\r\n    else:\r\n        best_feats = X_train_robust_unit\r\n    from sklearn.metrics import balanced_accuracy_score, RocCurveDisplay\r\n    # TODO: Implement hovertext, opacity,\r\n    if plot_tsne:\r\n        from sklearn.manifold import TSNE, LocallyLinearEmbedding\r\n        sne = TSNE(n_jobs=n_parallel, early_exaggeration=8.0, n_iter=1500, verbose=1, random_state=0)\r\n        mapped_comps = sne.fit_transform(X=best_feats[dense])\r\n        plotdata = total_meta_df\r\n        hover_df = plotdata[['SMILES_QSAR', 'DATA_SOURCE', 'INCHI_KEY']]\r\n        fig = scatter(data_frame=plotdata, x='X', y='y', color='DMSO_SOLUBILITY', symbol='DATA_SOURCE', opacity=0.2, marginal_x='violin', marginal_y='violin')\r\n        fig.update_traces(\r\n            fillcolor=\"rgb(0, 0, 0)\",\r\n            customdata=hover_df,\r\n            hovertemplate =\r\n                        \"SMILES: <b>%{customdata[0]}</b><br>\" +\r\n                        \"Data Source: %{customdata[1]}<br>\" +\r\n                        \"INCHI Key: %{customdata[2]}\" +\r\n                        \"<extra></extra>\",\r\n        )\r\n        fig.show(validate=True)\r\n        print('Saving t-SNE image...')\r\n        fig.write_image(file='tsne_8_1500_{}_marginal.png'.format(i), format='png')\r\n        fig.write_html(file='tsne_8_1500_{}_marginal.html'.format(i))\r\n    if plot_logreg:\r\n        from sklearn.linear_model import LassoCV, RidgeClassifierCV, ElasticNetCV\r\n        logcv = RidgeClassifierCV(scoring='balanced_accuracy', max_iter=2500, class_weight='balanced', n_jobs=n_parallel, random_state=0)\r\n        for lname, lin_clf in [('LogisticCV', logcv)]:\r\n            lin_clf.fit(X=best_feats, y=train_y)\r\n            print('{} CV Values: {}'.format(lname, lin_clf.cv_values_))\r\n            print('{} Coefficients: {}'.format(lname, lin_clf.coef_))\r\n            print('{} Alpha: {}'.format(cname, lin_clf.alpha_))\r\n            rocd = RocCurveDisplay.from_predictions(y_true=eval_y, y_pred=eval_y_pred, ax=sub, plot_chance_level=True,\r\n                                                    name=lname)\r\n            rocd.plot()\r\n    if forest:\r\n        report_list = list()\r\n        import matplotlib.pyplot as plt\r\n        from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\r\n        from sklearn.model_selection import StratifiedKFold, cross_val_score\r\n        from sklearn.metrics import ConfusionMatrixDisplay, pair_confusion_matrix\r\n        for train, test in StratifiedKFold().split(best_feats, y=train_y):\r\n            dev_X, eval_X = best_feats[train, :], best_feats[test, :]\r\n            dev_y, eval_y = train_y[train], train_y[test]\r\n            params = dict()\r\n            rbc = RUSBoostClassifier(random_state=0)\r\n            brf = BalancedRandomForestClassifier(n_estimators=50, max_depth=10000, random_state=0, verbose=0, bootstrap=False, sampling_strategy='majority', n_jobs=n_parallel)\r\n            # cv_res = cross_validate(estimator=clf, X=best_feats, y=train_y.ravel(), scoring='balanced_accuracy', cv=5, n_jobs=n_parallel, params=params)\r\n            # cv_res = cross_val_score(estimator=clf, X=best_feats, y=train_y, scoring='balanced_accuracy', n_jobs=n_parallel)\r\n            sub, fig = plt.subplots()\r\n            for cname, clf in [('Random Undersampling Boost', rbc), ('Balanced RF', 'brf')]:\r\n                clf.fit(X=dev_X, y=dev_y)\r\n                eval_y_pred = clf.predict(eval_X)\r\n                report = imblearn.metrics.classification_report_imbalanced(y_true=eval_y, y_pred=eval_y_pred, target_names=('Insoluble', 'Soluble'))\r\n                report_list.append(report)\r\n                rocd = RocCurveDisplay.from_predictions(y_true=eval_y, y_pred=eval_y_pred, ax=sub, plot_chance_level=True, name=cname)\r\n                print('ROC-AUC for {}: {}'.format(cname, rocd.roc_auc))\r\n                rocd.plot(ax=sub, name=cname, plot_chance_level=True)\r\n            fig.show()\r\n        [print(a) for a in report_list]\r\n'''from rdkit.DataStructs import cDataStructs\r\nbutina_clusters = Butina.ClusterData(data=dist_mat, nPts=total_meta_df.shape[0], distThresh=0.6, isDistData=True, reordering=True)\r\n[print(len(a) for a in butina_clusters)]\r\nbutina_path = \"C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/circ_fingerprints/BUTINA_CLUSTERS.pkl\"\r\nwith open(butina_path, 'wb') as f:\r\n    pickle.dump(file=f, obj=(train, test))\r\n'''\r\n\r\nfrom sklearn.manifold import TSNE\r\n'''\r\nintercols = new_desc_list[0].columns.intersection(new_desc_list[1].columns)\r\nfor dft in new_desc_list[2:]:\r\n    intercols = intercols.intersection(dft.columns)\r\nif 1443 in intercols:\r\n    raise KeyError\r\n''\r\ndf_pairs, ks_list = list(), list()\r\n# Calculate KS statistic and p-value for each column in each pair of DFs.\r\n\r\nfor df1_meta, df2_meta in itertools.combinations(new_desc_list, r=2):\r\n    if df1_meta.source != df2_meta.source and df1_meta.dmso_soluble != df2_meta.dmso_soluble:\r\n        print('Nonmatching pair found\\n\\n.')\r\n        continue\r\n    # df1 = pd.DataFrame(df1_meta['DESCRIPTORS'].to_list(), columns=COL_LABELS)\r\n    df1 = df1_meta[intercols]\r\n    df2 = df2_meta[intercols]\r\n    df_pairs.append((df1, df2))\r\nks_pvals = descriptor_preprocessing.ks_stats(dfs=new_desc_list, df_pairs=df_pairs, use_cols=intercols)\r\nks_list.append(pd.DataFrame.from_dict(ks_pvals, orient='index'))\r\nks_results = pd.concat(ks_list, axis=1).T\r\n# Nested dict col -> (pair, results)\r\nfrom scipy.stats import pmean\r\npair_dict = dict(enumerate(df_pairs))\r\n# ks_summary = pd.DataFrame(index=list(pair_dict.keys()), columns=intercols)\r\nfrom scipy.stats import pmean\r\n# ks_summary = pd.DataFrame(index=df_pairs, columns=COL_LABELS).reset_index(inplace=True, names='df_pairs')\r\nfor ks_col in ks_results.columns:\r\n    ks_ser = ks_results[ks_col]\r\n        # Optimization Possible by only checking one value/df_pairs pair\r\n    for pair_key in pair_dict.keys():\r\n        # pair_key = [key for key, value in pair_dict.items() if all([x.equals(y) for x, y in zip(value, df_pairs)])]\r\n        if pair_key and ks_col and type(pair_key) is not list:\r\n            ks_summary.loc[pair_key, ks_col] = ks_ser\r\n        else:\r\n            print('Pair_key is an empty list.')\r\n            ks_summary[pair_key, ks_col] = 1.0\r\n\r\nks_summary = pd.DataFrame(columns=ks_results.columns)\r\nks_results.drop_duplicates(subset=ks_results.columns[:20], inplace=True)\r\nfor col_name, ser in ks_results.items():\r\n    if type(ser[1]) is np.float64:\r\n        ks_summary[col_name] =[pmean(x[1], p=2) for col_name, x in ks_results.items()]\r\n    elif type(ser[1]) is pd.Series:\r\n        ks_summary[col_name] = [pmean(x[1], p=2) for col_name, x in ks_results.items()]\r\n# for cname, tcol in ks_results.items():\r\n#    ks_summary[cname] = pmean(tcol.drop_duplicates().tolist(), p=2)\r\n# sorted_cols = ks_summary.max(axis='rows')\r\n# sorted_cols.dropna(inplace=True)\r\n# print(sorted_cols.shape)\r\nsorted_cols = ks_summary.sort_values().squeeze()\r\nif len(sorted_cols.shape) < 2 or sorted_cols.shape[1] <= 0:\r\n    raise ValueError\r\nprint(sorted_cols)\r\nks_results.var(axis='columns', numeric_only=True)\r\ndescriptor_preprocessing.kde_grouped(dfs=new_desc_list, cols=sorted_cols[:100])\r\n\r\n# Calculate weighted geometric mean.\r\n# Sort columns by WGM. Plot KDEs of columns.\r\n# Add mutual info/F-score to KDEs.\r\n#\r\npmeans = dict()\r\nfor col, pvals in ks_summary.items():\r\n    try:\r\n        pmeans[col] = hmean(pvals.to_numpy())\r\n    except:\r\n        print(pvals)\r\nsorted_cols = pd.Series(pmeans)\r\nprint(sorted_cols.sort_values())\r\ndescriptor_preprocessing.kde_grouped(dfs=new_desc_list, cols=sorted_cols.sort_values()[:100])\r\n'''\r\n# pd.DataFrame(df['DESCRIPTORS'].to_list(), columns=COL_NAMES)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/dmso_data/epa_enamine_data_loading.pyi b/dmso_model_dev/dmso_data/epa_enamine_data_loading.pyi
--- a/dmso_model_dev/dmso_data/epa_enamine_data_loading.pyi	
+++ b/dmso_model_dev/dmso_data/epa_enamine_data_loading.pyi	
@@ -4,9 +4,10 @@
 
 import imblearn.metrics
 import numpy as np
+import scipy.ndimage
+import matplotlib.pyplot as plt
 import pandas as pd
 import plotly.express
-import scipy.ndimage
 import sklearn.feature_selection
 import sklearn.linear_model
 
@@ -271,97 +272,61 @@
 sklearn.utils.check_X_y(train_X, train_y, y_numeric=True)
 # Cluster and display dimensionality reduction of descriptors
 # plotmarkers = pd.concat([total_meta_df['DATA_SOURCE'].loc[train], total_meta_df['DATA_SOURCE'].loc[test]])
-# TODO: Implement splitting loop.
-estate = [a for a in train_X.columns if 'e-state' in a.lower()]
-bcut = [a for a in train_X.columns if 'bcut' in a.lower()]
-matrix = [a for a in train_X.columns if 'matrix' in a.lower()]
-from dmso_model_dev.models.feature_selector import pd_sparse_feats
-sparse = pd_sparse_feats(train_X)
-dense = train_X.columns.symmetric_difference(sparse)
-# from sklearn.feature_selection import VarianceThreshold, SelectFromModel, SequentialFeatureSelector
-# from sklearn.neural_network import MLPClassifier
-# from sklearn.decomposition import PCA, KernelPCA
-# from sklearn.metrics import make_scorer, adjusted_mutual_info_score
-# from functools import partial, partialmethod
-print(train_X.shape)
-
-# x_sample = train_X.sample(axis=1, frac=0.2, random_state=0)
-# TODO: Optimize this sorting.
-sparse_df = train_X[(train_X.nunique(dropna=False, axis=0) <= 2).index]
-dense_df = train_X[train_X.columns.symmetric_difference(sparse_df.columns)]
-
+'''
 serlist = [(col, ser.value_counts(normalize=True)) for col, ser in train_X.items()]
 # sorted(serlist, key=lambda x: max(x[1].sort_values(ascending=False)))
 freq_list = [(s[1].iloc[0], s[0]) for s in serlist if s[1].shape[0] > 1]
 # sorted(freq_list, key=lambda x: x[0], reverse=True)
 x_var = pd.Series(index=train_X.columns).value_counts()
-print(x_var[:10])
-for col, ser in train_X.items():
-    if col in sparse:
-        x_var[col] = 1
-    else:
-        x_var[col] = ser.var()
-print(x_var.index.difference(train_X.columns))
-x_var = train_X.var()
-# plotly.express.scatter(y=x_var.add(1).sort_values(), log_y=True).show()
-X_train = train_X[(x_var > 0).index]
+print(x_var.loc[:10])
+print(x_var.index.symmetric_difference(train_X.columns))
+'''
+
 n_feats = 100
 # Remove correlated features.
-not_corr = False
-if not_corr:
-    X_corr = dense_df.corr(method='kendall').abs()
-    x_na_corr = X_corr.isna().astype(int)
-    col_na = x_na_corr.sum(axis=0) < X_corr.shape[1]-1
-    print(col_na)
-    X_corr_val = X_corr.loc[col_na][col_na]
-    print(X_corr_val, X_corr_val.shape)
-    # X_corr.dropna(how='all', axis=0, inplace=True)
-    # X_corr = X_corr[X_corr.index.tolist()]
-    print(X_corr.shape)
-    print(X_corr[:10])
-    corr_sorted = X_corr.unstack().sort_values(kind='quicksort', ascending=False)
-    print('Sorted Correlation values:\n{}'.format(corr_sorted[:10]))
-    pairs_to_drop = set()
-    for i in range(0, X_train.shape[1]):
-        for j in range(0, i + 1):
-            pairs_to_drop.add((X_train.columns[i], X_train.columns[j]))
-cweights = train_y.replace({1: 1, 0: 10})
-X_train_robust = sklearn.preprocessing.RobustScaler(quantile_range=(0.05, 0.95), unit_variance=False).fit_transform(X_train)
-X_train_robust_unit = sklearn.preprocessing.RobustScaler(quantile_range=(0.05, 0.95), unit_variance=True).fit_transform(X_train)
-X_train_normal = sklearn.preprocessing.Normalizer().fit_transform(X_train)
+from dmso_model_dev.models import feature_selector
+X_train_robust, X_train_robust_unit, X_train_normal = feature_selector.get_scalers(X_train)
+check_X_y(X_train_robust, train_y)
+check_X_y(X_train_robust_unit, train_y)
+check_X_y(X_train_normal, train_y)
 linear_dim_reduction = False
-if linear_dim_reduction:
-    if dense_df.shape[1] < 50:
-        score_f = ElasticNetCV(n_jobs=n_parallel, random_state=0, max_iter=5000, tol=1e-5, cv=3)
-    else:
-        score_f = LassoCV(fit_intercept=False, max_iter=2500, tol=1e-5, n_jobs=n_parallel, random_state=0, cv='balanced')
-    selector = sklearn.feature_selection.SequentialFeatureSelector(estimator=score_f, scoring='balanced_accuracy', n_jobs=n_parallel)
 # selector = sklearn.feature_selection.SelectFromModel(estimator=score_f, max_features=n_feats, prefit=True, threshold='median')
 # Model running options.
-plot_tsne = True
-plot_logreg = False
-forest = True
-n_parallel = 4
-feat_group_names = ['Sparse', 'Dense', 'Matrix-based', 'E-States', 'All Features']
-feat_selector_bool = [False, False, True, True, True]
-selector_dict = dict(zip(feat_group_names, feat_selector_bool))
-feat_col_list = [sparse_df, dense_df, matrix, estate, X_train.columns]
+
+train_feats = X_train.copy()
+train_labels = train_y.copy()
 for i, feat_cols in zip(feat_group_names, feat_col_list):
-    dense_in = [c for c in feat_cols if c in dense_df.columns.to_list()]
-    sparse_in = [c for c in feat_cols if c in sparse_df.columns.to_list()]
-    if len(feat_cols) > 50 and not feat_cols.equals(dense_df.columns) and not feat_cols.equals(sparse_df.columns):
-            best_feats = pd.concat([selector.fit_transform(X_train_robust_unit[dense_in], train_y), X_train[sparse_in]])
-    elif len(feat_cols) > 50 and feat_cols.equals(dense_df.columns):
-        best_feats = selector.fit_transform(X_train_robust_unit[dense_in])
+    train_X = train_feats.copy()
+    train_y = train_labels.copy()
+    dense_in = [c for c in feat_cols if c in dense.to_list()]
+    sparse_in = [c for c in feat_cols if c in sparse.to_list()]
+    sparse_df = train_X[sparse_in]
+    dense_df = train_X[dense_in]
+    print(i, len(dense_in), len(sparse_in))
+
+    from sklearn.metrics import make_scorer
+    fake_score = make_scorer(score_func=np.random.random)
+    selector = sklearn.feature_selection.SelectPercentile(score_func=fake_score, percentile=100)
+    if len(feat_cols) > 50 and not i == 'Dense' and len(dense_in) > 0 and len(sparse_in) > 0:
+        best_feats = pd.concat([selector.fit_transform(X=X_train_robust_unit.copy()[dense_in], y=train_y), X_train.copy()[sparse_in]], verify_integrity=True)
+    elif len(feat_cols) > 50 and i == 'Dense' and len(dense_in) > 0:
+        best_feats = pd.DataFrame(selector.fit_transform(X=X_train_robust_unit.copy()[dense_in], y=train_y), columns=selector.feature_names_in_)
+    elif i == 'Sparse' and len(sparse_in) > 0:
+        best_feats = X_train.copy()[sparse_in]
     else:
-        best_feats = X_train_robust_unit
+        best_feats = X_train_robust_unit.copy()
+
+
+
+
+
     from sklearn.metrics import balanced_accuracy_score, RocCurveDisplay
     # TODO: Implement hovertext, opacity,
-    if plot_tsne:
+    if plot_tsne and len(dense_in) >= 10:
         from sklearn.manifold import TSNE, LocallyLinearEmbedding
         sne = TSNE(n_jobs=n_parallel, early_exaggeration=8.0, n_iter=1500, verbose=1, random_state=0)
-        mapped_comps = sne.fit_transform(X=best_feats[dense])
-        plotdata = total_meta_df
+        mapped_comps = sne.fit_transform(X=best_feats.copy(), y=train_y.copy())
+        plotdata = pd.concat([pd.DataFrame(mapped_comps, columns=['X', 'y']), total_meta_df], axis=1)
         hover_df = plotdata[['SMILES_QSAR', 'DATA_SOURCE', 'INCHI_KEY']]
         fig = scatter(data_frame=plotdata, x='X', y='y', color='DMSO_SOLUBILITY', symbol='DATA_SOURCE', opacity=0.2, marginal_x='violin', marginal_y='violin')
         fig.update_traces(
@@ -373,21 +338,26 @@
                         "INCHI Key: %{customdata[2]}" +
                         "<extra></extra>",
         )
-        fig.show(validate=True)
-        print('Saving t-SNE image...')
-        fig.write_image(file='tsne_8_1500_{}_marginal.png'.format(i), format='png')
-        fig.write_html(file='tsne_8_1500_{}_marginal.html'.format(i))
-    if plot_logreg:
+        fig.show()
+        # print('Saving t-SNE image...')
+        # fig.write_image(file='tsne_8_1500_{}_marginal.png'.format(i), format='png')
+        # fig.write_html(file='tsne_8_1500_{}_marginal.html'.format(i))
+    if plot_logreg and len(dense_in) >= 5:
+        remove_correlated(best_feats[dense_in])
         from sklearn.linear_model import LassoCV, RidgeClassifierCV, ElasticNetCV
-        logcv = RidgeClassifierCV(scoring='balanced_accuracy', max_iter=2500, class_weight='balanced', n_jobs=n_parallel, random_state=0)
+        #logcv = RidgeClassifierCV(scoring='balanced_accuracy', max_iter=2500, class_weight='balanced', n_jobs=n_parallel, random_state=0)
+        logcv = RidgeClassifierCV(alphas=(50, 500, 2500), scoring='balanced_accuracy', class_weight='balanced', cv=5)
         for lname, lin_clf in [('LogisticCV', logcv)]:
             lin_clf.fit(X=best_feats, y=train_y)
-            print('{} CV Values: {}'.format(lname, lin_clf.cv_values_))
             print('{} Coefficients: {}'.format(lname, lin_clf.coef_))
-            print('{} Alpha: {}'.format(cname, lin_clf.alpha_))
-            rocd = RocCurveDisplay.from_predictions(y_true=eval_y, y_pred=eval_y_pred, ax=sub, plot_chance_level=True,
+            print('{} CV Values: {}'.format(lname, lin_clf.best_score_))
+            print('{} Alpha: {}'.format(lname, lin_clf.alpha_))
+            fig, ax = plt.subplots()
+            rocd = RocCurveDisplay.from_estimator(estimator=lin_clf, X=best_feats, y=train_y, ax=ax, plot_chance_level=True,
                                                     name=lname)
-            rocd.plot()
+            rocd.plot(ax=ax, plot_chance_level=True)
+
+
     if forest:
         report_list = list()
         import matplotlib.pyplot as plt
@@ -395,24 +365,24 @@
         from sklearn.model_selection import StratifiedKFold, cross_val_score
         from sklearn.metrics import ConfusionMatrixDisplay, pair_confusion_matrix
         for train, test in StratifiedKFold().split(best_feats, y=train_y):
-            dev_X, eval_X = best_feats[train, :], best_feats[test, :]
-            dev_y, eval_y = train_y[train], train_y[test]
+            dev_X, eval_X = best_feats.copy().iloc[train, :], best_feats.copy().iloc[test, :]
+            dev_y, eval_y = train_y.copy().iloc[train], train_y.copy().iloc[test]
             params = dict()
-            rbc = RUSBoostClassifier(random_state=0)
-            brf = BalancedRandomForestClassifier(n_estimators=50, max_depth=10000, random_state=0, verbose=0, bootstrap=False, sampling_strategy='majority', n_jobs=n_parallel)
+            brf = BalancedRandomForestClassifier(n_estimators=50, max_depth=10000, random_state=0, verbose=0, bootstrap=False, sampling_strategy='majority', replacement=False, n_jobs=n_parallel)
+            rbc = RUSBoostClassifier(estimator=brf, random_state=0)
             # cv_res = cross_validate(estimator=clf, X=best_feats, y=train_y.ravel(), scoring='balanced_accuracy', cv=5, n_jobs=n_parallel, params=params)
             # cv_res = cross_val_score(estimator=clf, X=best_feats, y=train_y, scoring='balanced_accuracy', n_jobs=n_parallel)
-            sub, fig = plt.subplots()
-            for cname, clf in [('Random Undersampling Boost', rbc), ('Balanced RF', 'brf')]:
+            for cname, clf in [('Random Undersampling Boost', rbc), ('Balanced RF', brf)]:
+                fig, ax = plt.subplots()
                 clf.fit(X=dev_X, y=dev_y)
                 eval_y_pred = clf.predict(eval_X)
                 report = imblearn.metrics.classification_report_imbalanced(y_true=eval_y, y_pred=eval_y_pred, target_names=('Insoluble', 'Soluble'))
-                report_list.append(report)
-                rocd = RocCurveDisplay.from_predictions(y_true=eval_y, y_pred=eval_y_pred, ax=sub, plot_chance_level=True, name=cname)
+                report_list.append((cname, report))
+                rocd = RocCurveDisplay.from_predictions(y_true=eval_y, y_pred=eval_y_pred, ax=ax, plot_chance_level=True, name=cname)
                 print('ROC-AUC for {}: {}'.format(cname, rocd.roc_auc))
-                rocd.plot(ax=sub, name=cname, plot_chance_level=True)
-            fig.show()
-        [print(a) for a in report_list]
+                rocd.plot(ax=ax, name=cname, plot_chance_level=True)
+        print(cname)
+        print([a for a in report_list])
 '''from rdkit.DataStructs import cDataStructs
 butina_clusters = Butina.ClusterData(data=dist_mat, nPts=total_meta_df.shape[0], distThresh=0.6, isDistData=True, reordering=True)
 [print(len(a) for a in butina_clusters)]
@@ -421,7 +391,6 @@
     pickle.dump(file=f, obj=(train, test))
 '''
 
-from sklearn.manifold import TSNE
 '''
 intercols = new_desc_list[0].columns.intersection(new_desc_list[1].columns)
 for dft in new_desc_list[2:]:
Index: kohodequipe/formatting_tools.py
===================================================================
diff --git a/kohodequipe/formatting_tools.py b/dmso_model_dev/formatting_tools.py
rename from kohodequipe/formatting_tools.py
rename to dmso_model_dev/formatting_tools.py
--- a/kohodequipe/formatting_tools.py	
+++ b/dmso_model_dev/formatting_tools.py	
@@ -1,6 +1,3 @@
-import logging
-
-
 from difflib import SequenceMatcher
 
 def longestSubstring(str1, str2):
Index: dmso_model_dev/models/model_runner.py
===================================================================
diff --git a/dmso_model_dev/models/model_runner.py b/dmso_model_dev/models/model_runner.py
new file mode 100644
--- /dev/null	
+++ b/dmso_model_dev/models/model_runner.py	
@@ -0,0 +1,14 @@
+import numpy as np
+import pandas as pd
+import pickle
+from sklearn.utils.validation import check_X_y
+from dmso_model_dev.models.feature_selector import pd_sparse_feats, get_high_vars
+TRAIN_DIR = "C:/Users/mmanning/PycharmProjects/data/dmso_solubility/final_datasets/filtered/MAXMIN_PADEL_TRAIN.pkl"
+
+
+def main():
+    with open(TRAIN_DIR, 'rb') as f:
+        train_tup = pickle.load(f)
+    train_X, train_y = train_tup
+    train_y = train_y.squeeze().astype(int)
+    check_X_y(train_X, train_y, y_numeric=True)
Index: setup.py
===================================================================
diff --git a/setup.py b/setup.py
new file mode 100644
--- /dev/null	
+++ b/setup.py	
@@ -0,0 +1,16 @@
+import setuptools, setuptools.logging
+
+setuptools.logging.configure()
+setuptools.logging.set_threshold(3)
+
+setuptools.setup(
+    name='dmso_solubility',
+    version='0.10',
+    packages=['test', 'dmso_model_dev'],
+    package_dir={'': 'dmso_model_dev'},
+    url='',
+    license='MIT',
+    author='Matthew Manning',
+    author_email='Manning.Matthew@EPA.gov',
+    description='Module for predicting organic compound solubility in DMSO.'
+)
Index: kohodequipe/padel_categorization.py
===================================================================
diff --git a/kohodequipe/padel_categorization.py b/dmso_model_dev/data_handling/padel_categorization.py
rename from kohodequipe/padel_categorization.py
rename to dmso_model_dev/data_handling/padel_categorization.py
--- a/kohodequipe/padel_categorization.py	
+++ b/dmso_model_dev/data_handling/padel_categorization.py	
@@ -3,9 +3,7 @@
 import itertools
 from dataclasses import dataclass
 
-import numpy as np
 import pandas as pd
-from pandas import DataFrame
 
 '''
 Tuples = (Standard, Average, Centered, Average-Centered)
@@ -29,9 +27,9 @@
 bm_ac_ist = (range(67, 76), range(121, 130), range(184, 193), range(247, 256))
 bm_c_ch = (range(130, 139), range(193, 202))
 
-
+'''
 def get_desc_vals(name=None):
-    padel_path = "C:/Users/mmanning/PycharmProjects/data/padel_descriptor_names.csv"
+    padel_path = "/padel_descriptor_names.csv"
     padel = pd.read_csv(filepath_or_buffer=padel_path)
     print(padel.head())
     if name in padel.columns:
@@ -46,7 +44,7 @@
 
 def get_desc_full():
     return get_desc_vals('Description')
-
+'''
 
 def group_topos(substr, full_names: pd.DataFrame = None, str_dict=None):
     if not full_names:
@@ -64,13 +62,6 @@
     return str_dict
 
 
-
-def group_by_padel_type(full=True):
-    full_name_df = get_desc_full()
-    grouped = full_name_df.groupby('Type')
-    return grouped
-
-
 @dataclass
 class Descriptor:
     index: int
@@ -80,9 +71,6 @@
     dimension: int
     ex_class: str
 
-def sparse_padel_counts(df):
-    return df
-
 
 def get_padel_containing_indices(keywords, df: pd.DataFrame = None, three_d=False, output=None, output_type='list'):
     """
@@ -95,7 +83,7 @@
     if three_d:
         p_df = padel_df
     else:
-        p_df = padel_df.iloc[0:1443]
+        p_df = padel_df.iloc[0:1444]
     discrete_indices = set()
     for p in p_df.index.tolist():
         for kw in keywords:
@@ -144,14 +132,6 @@
         return desc_val_df['Type']
 
 
-def get_feat_names():
-    return padel_names
-
-
-def name_features(feat_frame, axis):
-    return None
-
-
 def autogroup_descriptors(desc_names, vectorizer, clusterer):
     train = vectorizer.fit_transform(raw_documents=desc_names)
     # print(vectorizer.get_feature_names_out())
@@ -164,7 +144,8 @@
     return nested
 
 
-def group_padel_descriptors_manual(desc_names):
+def group_padel_descriptors_manual():
+    desc_names = padel_descriptors
     lags = ['lag {}'.format(a) for a in range(0, 12)]
     ac_names = ["Broto-Moreau", "Geary", "Moran"]
     weights = ['charges', 'mass', 'van der Waals volumes', 'Sanderson electronegativities', 'polarizabilities',
@@ -181,15 +162,15 @@
     frings = ['Number of {} fused rings'.format(*['{}-membered'.format(a) for a in range(3, 13)])]
     stop_words = []  # ['average', 'logarithm', 'centered']
     kwarg_dict = dict(
-        [('lrings', lrings), ('names', names), ('frings', frings), ('autocorr', name_lags), ('lagac', name_weights),
+        [('lrings', lrings), ('frings', frings), ('names', names), ('autocorr', name_lags), ('lagac', name_weights),
          ('measures', measures)])
     desc_dict = dict()
     # ('names', names),
-    # desc_dict['halogen'] = list()
-    # desc_dict['halogen'] = [desc_dict['halogen'].extend([desc_dict[a] for a in desc_dict.keys()])]
-    temp_list = list()
+    # response_dict['halogen'] = list()
+    # response_dict['halogen'] = [response_dict['halogen'].extend([response_dict[a] for a in response_dict.keys()])]
     for key in kwarg_dict.keys():
         for subst in kwarg_dict[key]:
+            temp_list = list()
             desc_dict[subst] = list()
             for name in desc_names:
                 if key == "autocorr" or key == "lagac":
@@ -198,16 +179,27 @@
                 else:
                     if subst in name:
                         temp_list.append(name)
-            # print('{}\n'.format(desc_dict.items()))
+            # print('{}\n'.format(response_dict.items()))
             if len(temp_list) > 0:
-                desc_dict[subst] = sorted(temp_list)
-                temp_list = list()
-    return desc_dict
+                desc_dict[subst] = sorted(temp_list).copy()
+    return desc_dict, kwarg_dict
 
 
-padel_df: pd.DataFrame = pd.read_excel("C:/Users/mmanning/PycharmProjects/data/padel_all_descriptor_names.xlsx")
-padel_type_dict = dict()
-two_d_padel = padel_df.iloc[0:1443]
-padel_descriptors = two_d_padel['Description']
-padel_names = padel_descriptors.tolist()
-padel_types = padel_df['Type'].tolist()
+# TODO: Replace system-specific path.
+def load_padel_df(padel_dir=None, dims=(True, True, False)):
+    padel_df: pd.DataFrame = pd.read_excel("C:/Users/mmanning/PycharmProjects/data/padel_all_descriptor_names.xlsx")
+    if dims[0] and dims[1] and not dims[2]:
+        return padel_df.iloc[0:1444]
+
+
+def get_padel_long(dims):
+    padel_df = load_padel_df(dims=dims)
+    padel_descriptors = padel_df['Description']
+    return padel_descriptors
+
+
+def get_padel_short():
+    padel_df = load_padel_df()
+    return padel_df['Descriptor name']
+
+padel_types = load_padel_df()['Type'].tolist()
Index: kohodequipe/sql_tools.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/sql_tools.py b/dmso_model_dev/sql_tools.py
rename from kohodequipe/sql_tools.py
rename to dmso_model_dev/sql_tools.py
--- a/kohodequipe/sql_tools.py	
+++ b/dmso_model_dev/sql_tools.py	
@@ -1,5 +1,5 @@
 import sqlite3
-import requests
+
 
 def connect_db(db_path=None):
     con = sqlite3.connect(db_path)
Index: kohodequipe/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/main.py b/dmso_model_dev/main.py
rename from kohodequipe/main.py
rename to dmso_model_dev/main.py
--- a/kohodequipe/main.py	
+++ b/dmso_model_dev/main.py	
@@ -2,9 +2,11 @@
 import os.path
 import timeit
 from dataclasses import dataclass, field
+
 import numpy as np
 import pandas as pd
-from scrap_heap import recruiter, prep_map_data
+from scrap_heap import prep_map_data, recruiter
+
 import som
 
 SET_LIST = ['padel', 'rdkit', 'mordred', 'toxprints']
Index: kohodequipe/enamine_script.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/enamine_script.py b/dmso_model_dev/enamine_script.py
rename from kohodequipe/enamine_script.py
rename to dmso_model_dev/enamine_script.py
--- a/kohodequipe/enamine_script.py	
+++ b/dmso_model_dev/enamine_script.py	
@@ -1,10 +1,10 @@
 import os.path
-import shake
-from qsar_readiness import qsar_standardizer_api
+from glob import glob
+
 import numpy as np
 import pandas as pd
+
 import DescriptorRequestor
-from glob import glob
 
 DATA_PATH = "C:/Users/mmanning/PycharmProjects/compLoiel/kohodequipe/enamine_data/"
 
Index: kohodequipe/dmso_classifier.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/dmso_classifier.py b/dmso_model_dev/dmso_classifier.py
rename from kohodequipe/dmso_classifier.py
rename to dmso_model_dev/dmso_classifier.py
--- a/kohodequipe/dmso_classifier.py	
+++ b/dmso_model_dev/dmso_classifier.py	
@@ -1,37 +1,36 @@
 import itertools
+import logging
 import sys
 
+import data_importing
 import joblib
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 import seaborn as sns
 import sklearn
-from sklearn.experimental import enable_halving_search_cv  # noqa
-from sklearn.utils import validation
-from sklearn.model_selection import StratifiedGroupKFold, train_test_split, HalvingRandomSearchCV, ShuffleSplit
+from sklearn.cluster import AffinityPropagation, KMeans
+from sklearn.decomposition import KernelPCA
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
-from sklearn.cluster import KMeans, AffinityPropagation
-from sklearn.decomposition import KernelPCA
+from sklearn.ensemble import HistGradientBoostingClassifier
+from sklearn.experimental import enable_halving_search_cv  # noqa
 # explicitly require this experimental feature
-from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
+from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
 from sklearn.feature_selection import f_classif
-from sklearn.metrics import balanced_accuracy_score
 from sklearn.inspection import PartialDependenceDisplay
-
+from sklearn.metrics import balanced_accuracy_score
+from sklearn.model_selection import HalvingRandomSearchCV, ShuffleSplit, StratifiedGroupKFold, train_test_split
 # now you can import normally from model_selection
 from sklearn.pipeline import Pipeline
-from sklearn.preprocessing import RobustScaler, PowerTransformer, StandardScaler, MinMaxScaler
-from sklearn.ensemble import HistGradientBoostingClassifier
+from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler
+from sklearn.utils import validation
 
-import data_importing
 import dimensionality
 import enamine_script
-import padel_categorization
+from dmso_model_dev.data_handling import padel_categorization
 import plotting_tools
-from padel_categorization import padel_df, autogroup_descriptors, group_padel_descriptors_manual, padel_type_dict, \
-    padel_names, padel_types
-import logging
+from dmso_model_dev.data_handling.padel_categorization import autogroup_descriptors, group_padel_descriptors_manual, padel_df, padel_names, \
+    padel_type_dict, padel_types
 
 np.set_printoptions(precision=3)
 # Logging
@@ -519,7 +518,6 @@
 test_weights = 20 - 20 * y_test_raw
 
 mem = joblib.Memory(location="C:/Users/mmanning/PycharmProjects/data/joblib_cache")
-from sklearn.feature_selection import mutual_info_classif
 from sklearn.preprocessing import RobustScaler
 from sklearn.cluster import FeatureAgglomeration
 rs = RobustScaler(unit_variance=True)
@@ -570,7 +568,7 @@
 cls_list = list()
 confuse_list = list()
 from sklearn.ensemble import RandomForestClassifier
-from sklearn.metrics import ConfusionMatrixDisplay
+
 y_train = y_train.reshape(-1, 1)
 X_train_fit, y_train_fit = sklearn.utils.check_X_y(fitted_data, y_train)
 print(X_train_fit.shape, y_train_fit.shape)
@@ -578,7 +576,6 @@
 print(X_train_fit)
 # for X_rfc_train, y_rfc_train in sgk.split(X=X_train_fit.copy(), y=y_train_fit.copy(), groups=y_dev):
 X_rfc_train = X_train_fit
-from sklearn.preprocessing import LabelBinarizer
 print(y_train)
 print(np.unique(y_train, return_counts=True))
 y_rfc_train = y_train_fit.ravel()
@@ -633,7 +630,7 @@
 histo_boost(combined)
 
 
-# desc_groups = [group[0] for group in grouped_descriptors.items() if group[0] in combined['X_df'].columns]
+# descriptor_groups_df = [group[0] for group in grouped_descriptors.items() if group[0] in combined['X_df'].columns]
 def grouped_partial_depend(data_dict, classifier):
     for key, feat_list in grouped_descriptors.items():
         features = [f for f in feat_list if f in data_dict['X_df'].columns.tolist()]
@@ -656,9 +653,9 @@
     for i, (key, values) in enumerate(grouped_descriptors.items()):
         feats = [n for n in values if n in data_df['X_df'].columns.tolist()]
         continue
-        # if len(feats) < 3:
+        # if len(X) < 3:
         #     print('{} not enought features'.format(key))
-        #    print(len(feats), feats)
+        #    print(len(X), X)
         #    continue
         print(data_df['X_df'][feats].shape[1])
         best = pca(data_df['X_df'][feats].to_numpy(), y_labels=data_df['y_df'].to_numpy())
Index: kohodequipe/plotting_tools.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/plotting_tools.py b/dmso_model_dev/plotting_tools.py
rename from kohodequipe/plotting_tools.py
rename to dmso_model_dev/plotting_tools.py
--- a/kohodequipe/plotting_tools.py	
+++ b/dmso_model_dev/plotting_tools.py	
@@ -1,17 +1,16 @@
 import itertools
-import logging
 
+import matplotlib.pylab as plt
+import missingno as msno
 import numpy as np
 import pandas as pd
-import matplotlib.pylab as plt
 import seaborn as sns
-
-import FrameWeight
-import formatting_tools
 import sklearn.decomposition
-import missingno as msno
 from scipy.cluster.hierarchy import dendrogram
 
+import formatting_tools
+import FrameWeight
+
 
 def plot_pca_results(pca_obj: sklearn.decomposition.PCA, *args, **kwargs):
     sns.set_palette('Blues')
Index: kohodequipe/pipeline_one.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/pipeline_one.py b/dmso_model_dev/pipeline_one.py
rename from kohodequipe/pipeline_one.py
rename to dmso_model_dev/pipeline_one.py
--- a/kohodequipe/pipeline_one.py	
+++ b/dmso_model_dev/pipeline_one.py	
@@ -1,73 +1,70 @@
 import os.path
+
+import imblearn
+# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
+import joblib
 import matplotlib.pylab as plt
 import numpy as np
 import pandas as pd
-import imblearn
-import scipy.stats
 import sklearn
-import skopt
-from scipy.stats import norm, loguniform, expon, uniform, randint
 from sklearn import set_config
-from sklearn.pipeline import Pipeline
 from sklearn.compose import ColumnTransformer
-from sklearn.utils.validation import check_X_y
-from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
-from sklearn.metrics import *
-from sklearn.preprocessing import RobustScaler, KBinsDiscretizer, QuantileTransformer, LabelBinarizer
-from sklearn.feature_selection import GenericUnivariateSelect, VarianceThreshold, SelectFromModel, \
-    mutual_info_classif, SelectPercentile, SelectFwe
 from sklearn.ensemble import AdaBoostClassifier
-from sklearn.decomposition import KernelPCA, PCA, IncrementalPCA
-from sklearn.cross_decomposition import PLSCanonical
-from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
+from sklearn.linear_model import LogisticRegression
 from sklearn.experimental import enable_halving_search_cv  # noqa
+from sklearn.feature_selection import mutual_info_classif, VarianceThreshold, SequentialFeatureSelector
+from sklearn.metrics import *
+from sklearn.mixture import GaussianMixture
 from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split
-from sklearn.inspection import PartialDependenceDisplay
-from sklearn.covariance import LedoitWolf, GraphicalLasso
-# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
-import logging
-import joblib_cache, joblib
-import dimensionality, padel_categorization
-from functools import partial
+from sklearn.pipeline import Pipeline
+from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
+from sklearn.utils.validation import check_X_y
 from skopt import BayesSearchCV
-from skopt.space import Dimension, rv_discrete, Real, Integer
+from skopt.space import Real, Integer
+from sklearn.feature_selection import f_classif
 
 np.printoptions(precision=3, floatmode='fixed')
 PROJECT_PATH = "C:/Users/mmanning/PycharmProjects/compLoiel/kohodequipe/"
 
-def define_pipeline_params(disc_cols, cont_cols, mems=None):
+
+def define_pipeline_params(disc_cols=None, cont_cols=None, mems=None):
     # 'nan_eliminator': dimensionality.nan_matrix,
-    scorer = make_scorer(score_func=mutual_info_classif,
+    scorer = make_scorer(score_func=f_classif,
                          greater_is_better=False)
-    disc_trans = Pipeline(steps=[('d_vary', VarianceThreshold(0.05))])
-                                 #('d_univar', GenericUnivariateSelect(
-                                 #    score_func=partial(mutual_info_classif, discrete_features=True, n_neighbors=3), param=75,
-                                 #    mode='k_best'))])
-    cont_trans = Pipeline(steps=[# ('c_scaler', RobustScaler(with_centering=False)),
-                                 ('c_vary', VarianceThreshold(threshold=0.05))])
-                                 #('c_univar', GenericUnivariateSelect(partial(mutual_info_classif, discrete_features=False, n_neighbors=3),
-                                 #    param=150, mode='k_best'))])
-    col_trans = ColumnTransformer(transformers=[
-        ('cont_pipeline', cont_trans, cont_cols),
-        ('disc_pipeline', disc_trans, disc_cols)],
-    )
+    if disc_cols and cont_cols:
+        disc_trans = Pipeline(steps=[('d_vary', VarianceThreshold(0.05))])
+        # ('d_univar', GenericUnivariateSelect(
+        #    score_func=partial(mutual_info_classif, discrete_features=True, n_neighbors=3), param=75,
+        #    mode='k_best'))])
+        cont_trans = Pipeline(steps=[  # ('c_scaler', RobustScaler(with_centering=False)),
+            ('c_vary', VarianceThreshold(threshold=0.05))])
+        # ('c_univar', GenericUnivariateSelect(partial(mutual_info_classif, discrete_features=False, n_neighbors=3),
+        #    param=150, mode='k_best'))])
+        col_trans = ColumnTransformer(transformers=[
+            ('cont_pipeline', cont_trans, cont_cols),
+            ('disc_pipeline', disc_trans, disc_cols)],
+        )
+    else:
+        col_trans = Pipeline(steps=[('c_vary', VarianceThreshold(threshold=0.05)),
+                                    ('c_seq', SequentialFeatureSelector(scorer))])
     gauss, entropy, gini = estimators()
-    clf_pipe = Pipeline(steps=[('col_trans', col_trans),
-                               #('ipca', IncrementalPCA()),
+    clf_pipe = Pipeline(steps=[('feat_select', col_trans),
+                               # ('ipca', IncrementalPCA()),
                                ('model',
-                               (AdaBoostClassifier(estimator=entropy, algorithm='SAMME')
-                                ))])
+                                (AdaBoostClassifier(algorithm='SAMME')
+                                 ))])
     return clf_pipe
 
 
 def estimators():
     gmix = GaussianMixture(covariance_type='full', init_params='k-means++', random_state=0, warm_start=True)
     entree = [imblearn.ensemble.BalancedRandomForestClassifier(bootstrap=True,
-                                                              sampling_strategy='majority',
-                                                              criterion='entropy',
-                                                              max_features='sqrt',
-                                                              n_estimators=n,
-                                                              replacement=False) for n in np.square(np.linspace(5, 25, dtype=int))]
+                                                               sampling_strategy='majority',
+                                                               criterion='entropy',
+                                                               max_features='sqrt',
+                                                               n_estimators=n,
+                                                               replacement=False) for n in
+              np.square(np.linspace(5, 25, dtype=int))]
     gintree = imblearn.ensemble.BalancedRandomForestClassifier(bootstrap=False,
                                                                sampling_strategy='majority',
                                                                criterion='gini',
@@ -82,12 +79,12 @@
 
 
 def grid_search(pipe, data, labels, eval_data=None, eval_labels=None, score=None):
-    from scipy.stats.distributions import binom, hypergeom, nchypergeom_fisher
     d, labels = sklearn.utils.validation.check_X_y(X=data, y=labels, ensure_min_features=100)
     print(type(d), type(labels), labels.dtype)
     print(labels.dtype)
     class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)
     gauss, entree, gintree = estimators()
+    lregress = LogisticRegression(class_weight=class_weights, random_state=0, n_jobs=7)
     param_dist = {
         # 'col_trans__cont_pipeline__c_scaler__with_centering': False,
         # "col_trans__cont_pipeline__c_vary__threshold": norm(loc=0.15, scale=0.03),
@@ -102,8 +99,8 @@
         # "col_trans__cont_pipeline__c_univar__score_func": [partial(mutual_info_classif, n_neighbors=3, random_state=0)],
         # "ipca__n_components":
         #    Integer(10, 75, name="IPCA Components"), #: randint(10, 30),
-        # "model__n_estimators": [250],
-        "model__estimator": entree,
+        "model__n_estimators":  Integer(10, high=1000, name='num_trees'),
+        "model__estimator":     [entree, gintree, lregress],
         # "model__learning_rate": norm(loc=3, scale=1)
         "model__learning_rate": Real(low=5, high=50, name="Learning Rate")
         # 'model__n_components': Integer(5, 30, name='No. of Components'),
@@ -113,12 +110,12 @@
     results_file = '{}enamine_data/pipeone_run9_big_boosted_tree.log'.format(PROJECT_PATH)
     model_file = '{}enamine_data/pipeone_run9.joblib'.format(PROJECT_PATH)
     print('\n\nRunning hyperparameter search...\n')
-    hrs = BayesSearchCV(estimator=pipe, search_spaces=param_dist, n_jobs=-1, n_iter=1, cv=2, n_points=4,
+    hrs = BayesSearchCV(estimator=pipe, search_spaces=param_dist, n_jobs=7, n_iter=1, cv=5, n_points=4,
                         scoring=balanced_scorer, error_score='raise',
                         # cv=StratifiedKFold(n_splits=5),
                         verbose=10, return_train_score=True)
     dmso_fit = hrs.fit(data, labels)
-    from skopt.utils import dump, load
+    from skopt.utils import dump
     dump(res=dmso_fit, filename=model_file, store_objective=True, protocol=5)
     formed = dmso_fit.predict(data)
     print(dmso_fit.best_estimator_)
@@ -132,18 +129,19 @@
             print(hrs.cv_results_, file=rf)
             print(hrs.best_estimator_, file=rf)
             print('\n\n\n# Best parameters!!!:', hrs.best_params_, file=rf)
-    sample_weights = compute_sample_weight(class_weight={0:25, 1:1}, y=labels)
+    sample_weights = compute_sample_weight(class_weight={0: 25, 1: 1}, y=labels)
     fig, axs = plt.subplots(ncols=2)
     con = ConfusionMatrixDisplay.from_predictions(y_true=labels, y_pred=formed, ax=axs[1])
     # con = ConfusionMatrixDisplay.from_estimator(estimator=dmso_fit.best_estimator_, X=data, y=labels, ax=axs[1])
-    roc_pred = RocCurveDisplay.from_predictions(y_true=labels, y_pred=formed, sample_weight=sample_weights, ax=axs[0], plot_chance_level=True)
+    roc_pred = RocCurveDisplay.from_predictions(y_true=labels, y_pred=formed, sample_weight=sample_weights, ax=axs[0],
+                                                plot_chance_level=True)
     # roc_display = RocCurveDisplay.from_estimator(estimator=dmso_fit.best_estimator_, X=data, y=labels, sample_weight=sample_weights, ax=axs[0])
     plt.show()
-    from skopt.plots import partial_dependence_2D, plot_evaluations, plot_convergence
+    from skopt.plots import partial_dependence_2D, plot_evaluations
     plot_evaluations(dmso_fit)
     plt.show()
-    partial_dependence_2D(space=param_dist, model=dmso_fit, n_points=250, samples=pd.DataFrame.sample(formed, n=250, weights=labels, random_state=0))
-    plt.show()
+    # partial_dependence_2D(space=param_dist, model=dmso_fit, n_points=250, samples=pd.DataFrame.sample(formed, n=250, weights=labels, random_state=0))
+    # plt.show()
     # RocCurveDisplay.from_estimator(estimator=hrs.fit, X=data, y=labels, name='DMSO Solubility',
     #                                plot_chance_level=True,
     #                                sample_weight=class_weights)
@@ -198,6 +196,7 @@
         sklearn.utils.validation.check_X_y(X_eval, y_eval)
 
 
+
 def retrieve_data(all_feats=False, feat_kws='Count'):
     print(PROJECT_PATH)
     if all_feats:
@@ -208,24 +207,34 @@
         pkl_data = pd.read_pickle('{}enamine_data/dashboard_enamine_nanless_padel.pkl'.format(PROJECT_PATH))
         trimmed_data = pkl_data.copy()
         # trimmed_data.to_pickle('{}enamine_data/dashboard_enamine_nanless_padel.pkl'.format(PROJECT_PATH))
-        pkl_labels = pd.read_pickle('{}enamine_data/dashboard_enamine_nanless_labels.pkl'.format(PROJECT_PATH)).squeeze()
+        pkl_labels = pd.read_pickle(
+            '{}enamine_data/dashboard_enamine_nanless_labels.pkl'.format(PROJECT_PATH)).squeeze()
         trimmed_labels = pkl_labels.copy()
     # trimmed_labels.to_pickle('{}enamine_data/dashboard_enamine_nanless_labels.pkl'.format(PROJECT_PATH))
     return trimmed_data, trimmed_labels
 
 
-def main():
+def main(data=None):
     mem = joblib.Memory(location="C:/Users/mmanning/PycharmProjects/data/joblib_cache")
-    trimmed_data, trimmed_labels, discrete_data, cont_data = retrieve_data()
-    X_train_set, X_test, y_train_set, y_test = split_data(trimmed_data, trimmed_labels, rstate=0)
-    # print(y_train_set.sum() - y_train_set.size)
-    # print(y_test.sum() - y_test.size)
     scores = dict()
     scores['names'] = {'bac': balanced_accuracy_score, 'mcc': matthews_corrcoef, 'cm': confusion_matrix,
-                       'cs': consensus_score, 'roc': roc_curve}
-    clf = define_pipeline_params(disc_cols=discrete_data.columns.tolist(), cont_cols=cont_data.columns.tolist(),
-                                 mems=mem)
-    grid_search(clf, X_train_set, y_train_set, score=scores)
+                       'cs':  consensus_score, 'roc': roc_curve}
+    '''
+    if not data:
+        print(data)
+        (trimmed_data, trimmed_labels), (discrete_data, cont_data) = retrieve_data()
+        X_train_set, X_test, y_train_set, y_test = split_data(trimmed_data, trimmed_labels, rstate=0)
+        clf = define_pipeline_params(disc_cols=discrete_data.columns.tolist(), cont_cols=cont_data.columns.tolist(),
+                                     mems=mem)
+        grid_search(clf, X_train_set, y_train_set, score=scores)
+    else:
+    '''
+    if True:
+        X_train_set, X_test, y_train_set, y_test = data
+        clf = define_pipeline_params(mems=mem)
+        grid_search(clf, X_train_set, y_train_set, score=scores)
+    # print(y_train_set.sum() - y_train_set.size)
+    # print(y_test.sum() - y_test.size)
 
 
 if __name__ == '__main__':
diff --git a/dmso_model_dev/models/__init__.py b/dmso_model_dev/models/__init__.py
new file mode 100644
diff --git a/kohodequipe/__init__.py b/dmso_model_dev/knime_tools.py
rename from kohodequipe/__init__.py
rename to dmso_model_dev/knime_tools.py
diff --git a/__init__.py b/__init__.py
new file mode 100644
diff --git a/dmso_model_dev/models/data_processing_visualization.py b/dmso_model_dev/models/data_processing_visualization.py
new file mode 100644
