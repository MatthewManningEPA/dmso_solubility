Index: kohodequipe/enamine_script.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/enamine_script.py b/dmso_model_dev/enamine_script.py
rename from kohodequipe/enamine_script.py
rename to dmso_model_dev/enamine_script.py
--- a/kohodequipe/enamine_script.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/enamine_script.py	(date 1717444857686)
@@ -1,3 +1,4 @@
+import logging
 import os.path
 import shake
 from qsar_readiness import qsar_standardizer_api
@@ -6,7 +7,7 @@
 import DescriptorRequestor
 from glob import glob
 
-DATA_PATH = "C:/Users/mmanning/PycharmProjects/compLoiel/kohodequipe/enamine_data/"
+DATA_PATH = "//enamine_data/"
 
 def combine_downloads():
     combined_df = pd.DataFrame([])
@@ -39,7 +40,7 @@
 
 
 def get_descriptors(original_df, desc_fn, desc_set, failed_fn=None):
-    grabber = DescriptorRequestor.Grabber()
+    grabber = DescriptorRequestor.ApiGrabber()
     print(original_df.columns)
     desc_df, failed_dict = grabber.grab_data(desc_set, original_df)
     if desc_fn is None:
@@ -99,7 +100,7 @@
         if not os.path.exists(qsar_path):
             logging.error('Path for saving results is not valid: {}'.format(qsar_path))
             raise NotADirectoryError
-        qsar_ready = qsar_standardizer_api(subset, **kwargs)
+        qsar_ready, smile = DescriptorRequestor.QsarStdizer().bulk_epa_call(subset)
         qsar_df = pd.DataFrame.from_dict(data=qsar_ready, orient='index', dtype=str, columns=['QSAR']).reset_index(
             names='Original')
         logging.info(qsar_df.head())
Index: kohodequipe/dmso_classifier.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/dmso_classifier.py b/dmso_model_dev/dmso_classifier.py
rename from kohodequipe/dmso_classifier.py
rename to dmso_model_dev/dmso_classifier.py
--- a/kohodequipe/dmso_classifier.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/dmso_classifier.py	(date 1721840566738)
@@ -44,7 +44,7 @@
 logger.addHandler(stream_handle)
 
 # print(padel_df.loc[np.arange(10)])
-PROJECT_PATH = "C:/Users/mmanning/PycharmProjects/compLoiel/kohodequipe/"
+PROJECT_PATH = "//"
 
 
 def plot_hist(feature_arr):
@@ -320,7 +320,7 @@
     # vectors.append(subkernel.eigenvectors_)
 
 
-# Performs feature clustering based on groups described in desc_dict, which should be chemistry based. This prevents
+# Performs feature clustering based on groups described in response_dict, which should be chemistry based. This prevents
 # loss of information when training is missing or undersampling certain features (ex. iodine containing molecules) by
 # combining similiar features. The purpose of the clustering algorithm is to account for differential effects of features
 # on the output(s).
Index: kohodequipe/sql_tools.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/sql_tools.py b/dmso_model_dev/sql_tools.py
rename from kohodequipe/sql_tools.py
rename to dmso_model_dev/sql_tools.py
--- a/kohodequipe/sql_tools.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/sql_tools.py	(date 1717444857463)
@@ -13,3 +13,11 @@
         # cur.execute("""CREATE TABLE DMSOSOLUBILITY(SMILES VARCHAR(255),RECORDID VARCHAR(255),MOLECULEID VARCHAR(255),NAME VARCHAR(255),ARTICLEID VARCHAR(255),PUBMEDID VARCHAR(255), DMSOSOL VARCHAR(255),INCHI_KEY VARCHAR(255));""")
     con.commit()
     con.close()
+
+def write_descriptors(con, desc_row):
+    con.cursor().execute("""CREATE TABLE IF NOT EXISTS projects (
+        id integer PRIMARY KEY,
+        name text NOT NULL,
+        begin_date text,
+        end_date text
+    """);
Index: kohodequipe/plotting_tools.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/plotting_tools.py b/dmso_model_dev/plotting_tools.py
rename from kohodequipe/plotting_tools.py
rename to dmso_model_dev/plotting_tools.py
--- a/kohodequipe/plotting_tools.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/plotting_tools.py	(date 1717444857332)
@@ -92,11 +92,10 @@
     for i, (frames, framef) in enumerate(zip(samp_list, sfeat_list)):
         if frames is None or framef is None:
             print('None type DataFrame.')
-        print(framef.shape, frames.shape)
         if frames is not None:
-            sns.lineplot(x=list(range(frames.shape[0])), y=frames.to_numpy(), color=next(pals), legend=None, ax=axs[0])
+            sns.lineplot(x=list(range(frames.shape[0])), y=np.sort(frames.to_numpy()), color=next(pals), legend=None, ax=axs[0])
         if framef is not None:
-            sns.lineplot(x=list(range(framef.shape[0])), y=framef.to_numpy(), color=next(palf), legend=None, ax=axs[1])
+            sns.lineplot(x=list(range(framef.shape[0])), y=np.sort(framef.to_numpy()), color=next(palf), legend=None, ax=axs[1])
     axs[0].set_xlabel('Observation Index')
     plt.xticks(labels=None)
     axs[0].set_ylabel('NaN Contribution')
@@ -232,13 +231,6 @@
     plt.tight_layout()
 
 
-def llf(id, n, count):
-    if id < n:
-        return str(id)
-    else:
-        return '[%d %d %1.2f]' % (id, count, R[n - id, 3])
-
-
 def plot_dendrogram(model, **kwargs):
     # create the counts of samples under each node
     counts = np.zeros(model.children_.shape[0])
@@ -262,4 +254,4 @@
     plt.tight_layout(pad=0.75)
     # plot the top three levels of the dendrogram
     plt.xlabel("Number of points in node (or index of point if no parenthesis).")
-    plt.show()
+    plt.show()
\ No newline at end of file
Index: kohodequipe/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/main.py b/dmso_model_dev/main.py
rename from kohodequipe/main.py
rename to dmso_model_dev/main.py
--- a/kohodequipe/main.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/main.py	(date 1717444857344)
@@ -1,4 +1,4 @@
-Reimport logging
+import logging
 import os.path
 import timeit
 from dataclasses import dataclass, field
Index: kohodequipe/pipeline_one.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/kohodequipe/pipeline_one.py b/dmso_model_dev/pipeline_one.py
rename from kohodequipe/pipeline_one.py
rename to dmso_model_dev/pipeline_one.py
--- a/kohodequipe/pipeline_one.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/dmso_model_dev/pipeline_one.py	(date 1720292025052)
@@ -33,7 +33,7 @@
 from skopt.space import Dimension, rv_discrete, Real, Integer
 
 np.printoptions(precision=3, floatmode='fixed')
-PROJECT_PATH = "C:/Users/mmanning/PycharmProjects/compLoiel/kohodequipe/"
+PROJECT_PATH = "//"
 
 def define_pipeline_params(disc_cols, cont_cols, mems=None):
     # 'nan_eliminator': dimensionality.nan_matrix,
@@ -188,15 +188,16 @@
     return train_test_split(features, labels, test_size=test_size, random_state=rstate, stratify=labels)
 
 
-def cross_val_pipeline():
-    for dev_ind, eval_ind in RepeatedStratifiedKFold(random_state=0).split(X_train_set, y=y_train_set):
+def cross_val_pipeline(X_train_set, y_train_set, rstate=0):
+    for dev_ind, eval_ind in RepeatedStratifiedKFold(random_state=rstate).split(X_train_set, y=y_train_set,
+                                                                                groups=np.unique(y_train_set)):
         X_dev = X_train_set.iloc[dev_ind, :]
         y_dev = y_train_set.iloc[dev_ind]
         X_eval = X_train_set.iloc[eval_ind, :]
         y_eval = y_train_set.iloc[eval_ind]
         sklearn.utils.validation.check_X_y(X_dev, y_dev)
         sklearn.utils.validation.check_X_y(X_eval, y_eval)
-
+        yield X_dev, X_eval, y_dev, y_eval
 
 def retrieve_data(all_feats=False, feat_kws='Count'):
     print(PROJECT_PATH)
Index: kohodequipe/qsar_readiness.py
===================================================================
diff --git a/kohodequipe/qsar_readiness.py b/kohodequipe/qsar_readiness.py
deleted file mode 100644
--- a/kohodequipe/qsar_readiness.py	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ /dev/null	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
@@ -1,265 +0,0 @@
-import logging
-import os
-import time
-from typing import Dict, Any
-
-import rdkit.Chem.SaltRemover
-import molvs.standardize
-import molvs.normalize
-import molvs
-import molvs.metal
-import molvs.validate, molvs.validations
-import numpy as np
-import pandas as pd
-import rdkit.Chem.Descriptors, rdkit.Chem.AllChem
-import rdkit.Chem.MolStandardize as std
-import requests
-
-# https://hcd.rtpnc.epa.gov/api/stdizer?workflow=qsar-ready&smiles=
-
-ALL_ELEMENTS = ('H',
-                'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K',
-                'Ca', 'Sc', 'Ti',
-                'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y',
-                'Zr', 'Nb', 'Mo',
-                'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr',
-                'Nd', 'Pm', 'Sm',
-                'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au',
-                'Hg', 'Tl', 'Pb',
-                'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es',
-                'Fm', 'Md', 'No',
-                'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og')
-
-NONMETALS = ('B', 'C', 'N', 'O', 'P', 'S')
-NONMETALS_LOWER = ('b', 'c', 'n', 'o', 'p', 's')
-ATYPICAL = ('B', 'Si', 'Se', 'Te')
-HALOGENS = ('F', 'Cl', 'Br', 'I')
-
-ALLOWED = [*NONMETALS, *HALOGENS, *NONMETALS_LOWER, 'H']
-DISALLOWED = [e for e in ALL_ELEMENTS if e not in ALLOWED]
-
-
-def rdkit_df_cleaner(smiles_df: pd.DataFrame, smiles_axis=None, smiles_label=None, mol=False, dropped=None,
-                     verbose=False):
-    if smiles_axis is None and smiles_label is None:
-        print('Must input either smiles_axis or smiles_label.')
-        raise IOError
-    # Use if smiles_axis in df.columns | df.index to determine what methods to use for dropping/replacing.
-    # smiles_label: Name of column or row that contains the SMILES strings.
-    # smiles_axis = 'column' or 'index' if that axis contains the SMILES strings.
-    label_axis = None
-    if smiles_label is not None:
-        if smiles_label in smiles_df.columns:
-            # smiles_label = smiles_df.columns.tolist()[smiles_df.columns.tolist().index(smiles_label)]
-            smiles = smiles_df[smiles_label]
-            label_axis = 1
-        elif smiles_label in smiles_df.index:
-            # smiles_label = smiles_df.index.tolist()[smiles_df.index.tolist().index(smiles_label)]
-            smiles = smiles_df[smiles_label]
-            label_axis = 0
-        else:
-            print('{} not in either DataFrame axes.'.format(smiles_label))
-            raise IndexError
-        smiles_dict, mol_dict, desc_df, failed = rdkit_cleaner(smiles, mol=mol, verbose=verbose)
-        renamed_df = smiles_df[smiles_label].transform(lambda x: smiles_dict[x])
-        print(renamed_df.head())
-        drop_df = smiles_df[smiles_df[smiles_label] == 'NQSAR']
-        cleaned_df = smiles_df[smiles_df[smiles_label] != 'NQSAR']
-        # cleaned_df = smiles_df[smiles_df.drop(labels=smiles_label, axis=label_axis)]
-    else:
-        if smiles_axis == 0 or smiles_axis == 'index' or smiles_axis == 'rows':
-            smiles_axis = 0
-        elif smiles_axis == 1 or smiles_axis == 'columns' or smiles_axis == 'column':
-            smiles_axis = 1
-        else:
-            print('SMILES location not found in {} for DataFrame.'.format(smiles_axis))
-            raise IOError
-        smiles = smiles_df.axes[smiles_axis].tolist()
-        smiles_dict, mol_dict, desc_df, failed = rdkit_cleaner(smiles, mol=mol, verbose=verbose)
-        clean_dict = {[s for s in smiles_dict.items() if s[1] != 'NQSAR']}
-        nqsar_list = [s[0] for s in smiles_dict.items() if s[1] == 'NQSAR']
-        renamed_df = smiles_df.rename_axis(mapper=smiles_dict, axis=smiles_axis)
-        cleaned_df = smiles_df[renamed_df.axis[smiles_axis] != 'NQSAR']
-        drop_df = smiles_df[renamed_df.axis[smiles_axis] == 'NQSAR']
-        # drop_df = smiles_df[smiles_df.axes[smiles_axis]]
-        if not mol:
-            mol_dict = None
-    return cleaned_df.copy(deep=True), drop_df.copy(deep=True), mol_dict
-
-
-def rdkit_cleaner(smiles_list: list, rdk_mol=True, verbose=False, **kwargs):
-    # print(ALLOWED)
-    clean_dict, mol_dict, desc_holder, failed = dict(), dict(), list(), list()
-    n = molvs.normalize.Normalizer()
-    s = molvs.standardize.Standardizer()
-    r = rdkit.Chem.SaltRemover.SaltRemover()
-    d = molvs.metal.MetalDisconnector()
-    u = molvs.standardize.Uncharger()
-    # v = molvs.validate.Validator(validations=(molvs.validations.IsNoneValidation))
-    for smile in smiles_list:
-        if smile == 'nan' or type(smile) is np.nan or not smile:
-            print('Invalid SMILES string')
-            continue
-        mol = rdkit.Chem.MolFromSmiles(smile)
-        if type(mol) is None:
-            print('{} is defective!'.format(smile))
-            failed.append(smile)
-            continue
-        '''
-        std.Chem.RemoveHs(mol, sanitize=False)
-        s.isotope_parent(mol, skip_standardize=True)
-        import rdkit.Chem.rdMolEnumerator as db
-        db.Enumerate()
-        rdkit.Chem.ChemUtils.
-        u.uncharge(mol)
-        std.Chem.RemoveHs(mol, sanitize=False)
-        time.sleep(0.1)
-        '''
-        try:
-            n.normalize(mol)
-            rdkit.Chem.CleanupOrganometallics(mol)
-            rdkit.Chem.RemoveStereochemistry(mol)
-            d.disconnect(mol)
-            r.StripMol(mol)
-            new_smile = rdkit.Chem.MolToSmiles(mol, isomericSmiles=False)
-            # print(smile, new_smile)
-            descs = get_rdk_descs(mol)
-            descs['SMILES'] = new_smile
-            desc_holder.append(descs)
-        except:
-            new_smile = 'NQSAR'
-            failed.append(smile)
-            print('Failed convert ', smile)
-        clean_dict[smile] = new_smile
-        mol_dict[smile] = mol
-    time.sleep(0.01)
-    desc_df = pd.DataFrame(desc_holder)
-    if not rdk_mol:
-        mol_dict = None
-    return clean_dict, mol_dict, desc_df, failed
-
-
-def get_rdk_descs(mol):
-    return rdkit.Chem.Descriptors.CalcMolDescriptors(mol)
-
-
-def _list_qsar_quick_and_dirty(smiles_list: list, remove_salts=True):
-    smile: str
-    new_smiles, salted = dict(), list()
-    if remove_salts:
-        s = rdkit.Chem.SaltRemover.SaltRemover()
-        u = molvs.standardize.Uncharger()
-    for smile in smiles_list:
-        a, z = 0, -1
-        if remove_salts and ('+' in smile or '-' in smile):
-            rmol = rdkit.Chem.MolFromSmiles(smile)
-            s.StripMolWithDeleted(rmol)
-            u.uncharge(rmol)
-            smile = rdkit.Chem.MolToSmiles(rmol, isomericSmiles=False)
-        if '[' in smile:
-            a = smile.find('[', a, z)
-            z = smile.find(']', a, z)
-            bracket = smile[a + 1:z]
-            if any([e for e in DISALLOWED if e in bracket]):
-                new_smiles[smile] = 'NQSAR'
-                # print('Contains disallowed element: ', bracket, smile)
-                continue
-            else:
-                # print('Allowed element in brackets for ', bracket, smile)
-                new_smiles[smile] = smile
-                continue
-        else:
-            new_smiles[smile] = smile
-    # new_smiles.__setattr__('__missing__', 'NQSAR')
-    return new_smiles
-
-
-def qsar_quick_and_dirty(smiles, remove_salts=True):
-    v = molvs.validate.Validator(validations=(molvs.validations.NoAtomValidation, molvs.validations.IsNoneValidation),
-                                 stdout=True)
-    if type(smiles) is list:
-        smiles_dict = _list_qsar_quick_and_dirty(smiles, remove_salts=remove_salts)
-        return smiles_dict.values()
-    elif type(smiles) is pd.Series:
-        smiles_dict = _list_qsar_quick_and_dirty(smiles.tolist(), remove_salts=remove_salts)
-        transformed = smiles.map(arg=pd.Series(smiles_dict))
-        return transformed[transformed != 'NQSAR'], transformed[transformed == 'NQSAR']
-    elif type(smiles) is pd.DataFrame:
-        if ('SMILES' in smiles.columns) or ('SMILES' in smiles.index):
-            smiles_dict = _list_qsar_quick_and_dirty(smiles['SMILES'].tolist(), remove_salts=remove_salts)
-            transformed = smiles['SMILES'].map(arg=pd.Series(smiles_dict))
-            time.sleep(0.1)
-            return transformed[transformed['SMILES'] != 'NQSAR'], transformed[transformed['SMILES'] == 'NQSAR']
-        else:
-            print(
-                'Could not find a column or row labelled "SMILES". If SMILES strings are column labels or rows, please convert to list before passing.')
-            raise NotImplementedError
-            '''
-            try: 
-                v.validate(smiles.columns[-1])
-                smiles_dict = _list_qsar_quick_and_dirty(smiles.columns.tolist(), remove_salts=remove_salts)
-            except molvs.errors.ValidateError:
-                v.validate(smiles.index[-1])
-                smiles_dict = _list_qsar_quick_and_dirty(smiles.columns.tolist(), remove_salts=remove_salts)
-            '''
-    elif type(smiles) is np.ndarray:
-        ok, rejects = qsar_quick_and_dirty(pd.DataFrame(smiles), remove_salts=remove_salts)
-        return ok.to_numpy(), rejects.to_numpy()
-    else:
-        return _list_qsar_quick_and_dirty(smiles, remove_salts=remove_salts)
-
-
-def open_qsar_session(server_url, limit=5):
-    requester = requests.Session()
-    ada = requester.get_adapter(url=server_url)
-    retries = 0
-    while retries == 0 or not requester.verify:
-        if retries >= 1:
-            logging.error('Requester still bad.')
-            time.sleep(2.5 * retries)
-        requester = requests.Session()
-        ada = requester.get_adapter(url=server_url)
-        retries += 1
-        if retries > limit:
-            requester.close()
-            raise ConnectionError
-    return requester, ada
-
-
-def qsar_standardizer_api(smiles_list, server="https://hcd.rtpnc.epa.gov/api/stdizer", retry_limit=5, connect_timeout=5,
-                          response_timeout=15, **kwargs) -> dict:
-    payload = dict()
-    ready_dict: dict[str, str] = dict()
-    payload['workflow'] = 'qsar-ready'
-    j = 0
-    requester, ada = open_qsar_session(server_url=server, limit=retry_limit)
-    logging.info('Starting QSAR-ready conversion calls.')
-    for i, smile in enumerate(smiles_list):
-        if smile is None or smile == "":
-            continue
-        payload['smiles'] = smile
-        qsar_req = requests.Request(method='GET', url=server, params=payload)
-        qsar_ask = qsar_req.prepare()
-        packet = ada.send(qsar_ask, timeout=(connect_timeout, response_timeout))
-        j += 1
-        if (packet.status_code == 200 or packet.status_code == '200') and packet.json() is not None and len(
-                packet.json()) > 0:
-            ready_dict[smile] = packet.json()[0]['canonicalSmiles']
-        elif packet.status_code == 404:
-            print('Server could not be found.')
-            ada.close()
-            requester.close()
-            raise ConnectionError
-        elif (packet.status_code == 504) or (packet.status_code == '504'):
-            print('Server Timeout')
-            requester.close()
-            requester = requests.Session()
-        else:
-            print(packet.status_code, i, smile)
-            ready_dict[smile] = str(packet.status_code)
-            try:
-                print(packet.json())
-            except:
-                continue
-    requester.close()
-    return ready_dict
Index: dmso_model_dev/cleaner.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/cleaner.py b/dmso_model_dev/cleaner.py
new file mode 100644
--- /dev/null	(date 1717444857588)
+++ b/dmso_model_dev/cleaner.py	(date 1717444857588)
@@ -0,0 +1,72 @@
+import itertools
+import logging
+
+import matplotlib.pyplot as plt
+import pandas as pd
+
+
+def clean_descriptors(dframe: pd.DataFrame, **kwargs):
+    dframe.convert_dtypes(convert_boolean=False)
+    logging.info('Before and after DataFrame shapes.')
+    logging.info(dframe.shape)
+    print(dframe.info())
+    dframe.dropna(axis=0, how='all', inplace=True)
+    dframe.dropna(axis=1, how='all', inplace=True)
+    int_cols = dframe.select_dtypes(include=int)
+    float_cols = dframe.select_dtypes(exclude=int)
+    print(int_cols)
+    logging.info(dframe.shape)
+    print(dframe.info())
+    # zscore, inliers, zmask = variance_measures(dframe[float_cols.columns], **kwargs)
+    zscore, inliers, zmask = variance_measures(dframe, **kwargs)
+    logging.info('Descriptor variance: {}'.format(zscore))
+    print('Descriptor variance: {}'.format(zscore))
+    combo = (inliers & zmask)
+    print(inliers.shape, zmask.shape)
+    # print(dframe.T[inliers].T)
+    pcorr, pmask = correlation_filter(dframe)
+    # pcorr, pmask = correlation_filter(dframe[float_cols.columns])
+    filtered = dframe.drop(columns=dframe.columns[pmask], inplace=True)
+    logging.info('Pearson Correlation Coefficients: {}'.format(pcorr))
+    return filtered
+
+
+def variance_measures(feature_df: pd.DataFrame, outlier_cut=3.0, var_cut=0.01):
+    var_z = feature_df.var()
+    inlier_mask = var_z.abs() < outlier_cut
+    var_mask = var_z.abs() >= var_cut
+    return var_z, inlier_mask, var_mask
+
+
+# Use for unsupervised learning. For supervised, drop least correlated with target.
+def correlation_filter(dframe: pd.DataFrame, pearson_cut=0.95):
+    pearson_df = dframe.corr(method='pearson', numeric_only=True)
+    plt.imshow(pearson_df)
+    plt.show()
+    high_corr = []
+    corr_score = pearson_df.map(lambda t: 1 / (2 - t)).sum(numeric_only=True)
+    icol = range(len(pearson_df.columns))
+    pairlist = [[c1, c2] for (c1, c2) in itertools.combinations(icol, 2)]
+    sort_dict = {}
+    for x, y in pairlist:
+        sort_dict[(x, y)] = corr_score[x] + corr_score[y]
+    # sort_pair = functools.reduce(lambda x, y: corr_score[x] + corr_score[y], sorted(pairlist, reverse=True))
+    # print(sort_dict[(3, 5)])
+    sort_pair = sorted(sort_dict, key=lambda a: sort_dict[a], reverse=True)
+    # print(sort_dict[(3,5)])
+    for tup in sort_pair:
+        c1, c2 = tup
+        if c1 in high_corr or c2 in high_corr:
+            continue
+        elif pearson_df.iat[c1, c2] > pearson_cut:
+            if corr_score[c1] > corr_score[c2]:
+                high_corr.append(c1)
+            else:
+                high_corr.append(c2)
+    print(high_corr)
+    return pearson_df, high_corr
+
+
+def response_corr(features, response, r2_cut=0.1):
+    response_r2 = features.corrwith(response, method='pearson')
+    return response_r2
Index: .idea/compLoiel.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/compLoiel.iml b/.idea/dmso_solubility.iml
rename from .idea/compLoiel.iml
rename to .idea/dmso_solubility.iml
--- a/.idea/compLoiel.iml	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/.idea/dmso_solubility.iml	(date 1720367643849)
@@ -2,9 +2,8 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$">
-      <sourceFolder url="file://$MODULE_DIR$" isTestSource="false" />
-      <sourceFolder url="file://$MODULE_DIR$/kohodequipe" isTestSource="false" />
       <excludeFolder url="file://$MODULE_DIR$/.venv" />
+      <excludeFolder url="file://$MODULE_DIR$/dmso_model_dev/scrap_heap" />
     </content>
     <content url="file://$MODULE_DIR$/../data">
       <sourceFolder url="file://$MODULE_DIR$/../data" isTestSource="false" />
@@ -13,4 +12,7 @@
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
+  <component name="PackageRequirementsSettings">
+    <option name="removeUnused" value="true" />
+  </component>
 </module>
\ No newline at end of file
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"ProjectModuleManager\">\r\n    <modules>\r\n      <module fileurl=\"file://$PROJECT_DIR$/.idea/compLoiel.iml\" filepath=\"$PROJECT_DIR$/.idea/compLoiel.iml\" />\r\n    </modules>\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
--- a/.idea/modules.xml	(revision 74d9d575b42835dcd5708b8a4be1591bd018e9ee)
+++ b/.idea/modules.xml	(date 1713273678454)
@@ -2,7 +2,7 @@
 <project version="4">
   <component name="ProjectModuleManager">
     <modules>
-      <module fileurl="file://$PROJECT_DIR$/.idea/compLoiel.iml" filepath="$PROJECT_DIR$/.idea/compLoiel.iml" />
+      <module fileurl="file://$PROJECT_DIR$/.idea/dmso_solubility.iml" filepath="$PROJECT_DIR$/.idea/dmso_solubility.iml" />
     </modules>
   </component>
 </project>
\ No newline at end of file
Index: dmso_model_dev/SdfReader.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/SdfReader.py b/dmso_model_dev/SdfReader.py
new file mode 100644
--- /dev/null	(date 1717444857373)
+++ b/dmso_model_dev/SdfReader.py	(date 1717444857373)
@@ -0,0 +1,18 @@
+from rdkit.Chem.PandasTools import LoadSDF
+
+
+class SdfReader:
+
+    def __init__(self, filepath):
+        self.filenames = filepath
+        self.mol_list = None
+
+
+
+    def sdf_to_df(self, smiles_col='SMILES', rdmol=None):
+        if type(self.filenames) is list or type(self.filenames) is tuple:
+            return [LoadSDF(f, smilesName=smiles_col, includeFingerprints=True, embedProps=True, molColName=rdmol) for f
+                    in self.filenames]
+        else:
+            return [LoadSDF(self.filenames, includeFingerprints=True, embedProps=True, smilesName=smiles_col,
+                            molColName=rdmol)]
Index: dmso_model_dev/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/__init__.py b/dmso_model_dev/__init__.py
new file mode 100644
--- /dev/null	(date 1718731135664)
+++ b/dmso_model_dev/__init__.py	(date 1718731135664)
@@ -0,0 +1,1 @@
+from . import *
\ No newline at end of file
Index: dmso_model_dev/SubSampleSelector.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/SubSampleSelector.py b/dmso_model_dev/SubSampleSelector.py
new file mode 100644
--- /dev/null	(date 1717444857606)
+++ b/dmso_model_dev/SubSampleSelector.py	(date 1717444857606)
@@ -0,0 +1,87 @@
+import logging
+
+import numpy as np
+import pandas as pd
+
+
+def valid_series(*args):
+    df = pd.concat(args, axis=1).dropna(axis=1, how='any')
+    return [df[f] for f in df.columns]
+
+
+def valid_rows(df, features):
+    """
+    Helper function to avoid having NaNs for selected features.
+    :rtype: pd.DataFrame
+    :param features:
+    :return: DataFrame containing only rows without NaN values.
+    :type df: pd.DataFrame
+    """
+    return df[features].dropna(axis=1, how='any')
+
+
+def nan_score_features(data, target, references, scorer, min_samples, *args, **kwargs):
+    score_dict = dict()
+    val = list()
+    df = data.copy()
+    valids = [valid_series(df[target], df[c]) for c in references]  # Get non-null rows
+    if min_samples < 1.:
+        abs_thresh = min_samples * np.round(df.shape[0])
+    elif min_samples >= 1:
+        abs_thresh = max(min_samples, 2)
+    else:
+        abs_thresh = 2
+    for v in valids:
+        if v[0].shape[0] < abs_thresh:
+            logging.warning('Too few samples in {}'.format(v[1].name))
+            valids.remove(v)
+            continue
+        for w in v:
+            if type(w) is pd.DataFrame and w.shape[1] > 1:
+                w.drop_duplicates(inplace=True)
+                print(w.shape)
+        if all([type(a) is pd.Series or a.shape[1] <= 1 for a in v]):
+            val.append(v)
+    # print(type(valids), valids[0])
+    # val = [v for v in valids if all([len(f.shape) == 1 for f in v])]
+    # duped = [v for v in valids if any([len(f.shape) != 1 for f in v])]
+    # [[o.drop_duplicates(inplace=True) for o in n] for n in duped]
+    # val.extend(duped)
+    for v in val:
+        if len(v) != 2:
+            print(v)
+            continue
+        score_dict[v[1].name] = scorer(X=pd.DataFrame(v[1]), y=v[0], *args, **kwargs)
+    return score_dict
+
+'''
+
+class SubSampleSelector:
+
+    def __init__(self):
+        # Parent input: df, target(s), reference(s), score_func[, subsample] -> new target values[, drop old target(s)]
+        # Score func: func,
+        self.scorer_dict = {
+            'target_col': pd.Index,
+            'reference': pd.Index,
+            'scorer': sklearn.base.BaseEstimator,
+            'subsample': pd.Index
+        }
+
+
+    class FeatureMelter(RelativeFeatures):
+
+        def __init__(self, scoring):
+            self.scoring = scoring
+
+
+        def find_best_predictors(self):
+            return
+
+
+        def create_regressor(func):
+            from sklearn.cross_decomposition import PLSCanonical
+            assert is_regressor(func), '{} is not a regressor'.format(func)
+            regressor = func
+            return regressor
+'''
Index: dmso_model_dev/pipeline_plots.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/pipeline_plots.py b/dmso_model_dev/pipeline_plots.py
new file mode 100644
--- /dev/null	(date 1717444857424)
+++ b/dmso_model_dev/pipeline_plots.py	(date 1717444857424)
@@ -0,0 +1,77 @@
+import pandas as pd
+
+
+def highlighter(df_input):
+    cell_hover = {  # for row hover use <tr> instead of <td>
+        'selector': 'td:hover',
+        'props': [('background-color', '#ffffb3')]
+    }
+    index_names = {
+        'selector': '.index_name',
+        'props': 'font-style: italic; color: darkgrey; font-weight:normal;'
+    }
+    headers = {
+        'selector': 'th:not(.index_name)',
+        'props': 'background-color: #000066; color: white;'
+    }
+    df_input.style.set_table_styles([cell_hover, index_names, headers])
+    df_output = df_input.style.set_table_styles([
+        {'selector': 'th.col_heading', 'props': 'text-align: center;'},
+        {'selector': 'th.col_heading.level0', 'props': 'font-size: 1.5em;'},
+        {'selector': 'td', 'props': 'text-align: center; font-weight: bold;'},
+    ], overwrite=False)
+    return df_output
+
+
+def plot_con_matrix_cv(df, df_std, model_name=None):
+    cm_caption = 'Confusion Matrix for {}'.format(model_name)
+    df_out = df.style.format('{:.2f}').set_table_styles([{
+        'selector': '',
+        'props': 'border-collapse: separate;'
+    }, {
+        'selector': 'caption',
+        'props': 'caption-side: bottom; font-size:1.3em;'
+    }, {
+        'selector': '.index_name',
+        'props': 'font-style: italic; color: darkgrey; font-weight:normal;'
+    }, {
+        'selector': 'th:not(.index_name)',
+        'props': 'background-color: #000066; color: white;'
+    }, {
+        'selector': 'th.col_heading',
+        'props': 'text-align: center;'
+    }, {
+        'selector': 'th.col_heading.level0',
+        'props': 'font-size: 1.5em;'
+    }, {
+        'selector': 'th.col2',
+        'props': 'border-left: 1px solid white;'
+    }, {
+        'selector': '.col2',
+        'props': 'border-left: 1px solid #000066;'
+    }, {
+        'selector': 'td',
+        'props': 'text-align: center; font-weight:bold;'
+    }, {
+        'selector': '.true',
+        'props': 'background-color: #e6ffe6;'
+    }, {
+        'selector': '.false',
+        'props': 'background-color: #ffe6e6;'
+    }, {
+        'selector': '.border-red',
+        'props': 'border: 2px dashed red;'
+    }, {
+        'selector': '.border-green',
+        'props': 'border: 2px dashed green;'
+    }, {
+        'selector': 'td:hover',
+        'props': 'background-color: #ffffb3;'
+    }]) \
+        .set_td_classes(pd.DataFrame([['true', 'false'],
+                                      ['false', 'true']],
+                                     index=df.index, columns=df.columns)) \
+        .set_caption("{}".format(cm_caption))
+    df_out = df_out.background_gradient(axis=None, vmin=0, vmax=1.0, cmap="YlOrRd", gmap=df_std)
+    df_out = df_out.to_html(doctype_html=True)
+    return df_out
Index: dmso_model_dev/feature_tools.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/feature_tools.py b/dmso_model_dev/feature_tools.py
new file mode 100644
--- /dev/null	(date 1721828948769)
+++ b/dmso_model_dev/feature_tools.py	(date 1721828948769)
@@ -0,0 +1,174 @@
+import itertools
+
+import numpy as np
+import pandas as pd
+import rdkit.Chem.Descriptors
+import seaborn
+import seaborn as sns
+from matplotlib import pyplot as plt
+from rdkit.Chem.Descriptors import CalcMolDescriptors
+from sklearn.cluster import AgglomerativeClustering, HDBSCAN
+from sklearn.decomposition import KernelPCA, PCA
+
+from dmso_model_dev.data_handling import padel_categorization
+import plotting_tools
+
+
+def get_rdk_mol(smile_df=None, smile_ser=None, smile_list=None):
+    smile_dict = dict()
+    if smile_df is not None:
+        smile_list = smile_df['SMILES'].to_dict()
+    elif smile_ser is not None:
+        smile_list = smile_ser.to_dict()
+    if smile_list is not None:
+        for smile in smile_list:
+            if smile is not np.nan and smile is not None:
+                print(smile)
+                smile_dict[smile] = rdkit.Chem.MolFromSmiles(smile)
+    else:
+        print('No SMILES input given for RDK Mol generation.')
+        raise IOError
+    return smile_dict
+
+
+def calc_rdk_desc(smile_mol, missing=np.nan, silence=True):
+    if type(smile_mol) is pd.Series:
+        smile_mol = smile_mol.to_dict()
+    desc_dict, nan_list = dict(), list()
+    for combo in smile_mol.items():
+        desc_dict[combo[0]] = CalcMolDescriptors(mol=combo[1], missingVal=missing, silent=silence)
+        if len([a[0] for a in desc_dict.items() if a[1] == np.nan]) > 0:
+            nan_list.append(combo[0])
+    return desc_dict, nan_list
+
+
+def pca_vs_kpca(pca_fit, kpca_fit, **kwargs):
+    fig, axes = plt.subplots(ncols=2)
+    plt.plot(data=pca_fit, ax=axes[0])
+    axes[0].set_title('PCA')
+    plt.plot(data=kpca_fit, ax=axes[1])
+    axes[1].set_title('Kernal PCA')
+    if 'X' in kwargs:
+        plt.suptitle('{} and {}'.format(*kwargs['X']))
+    plt.draw()
+    plt.show()
+    return fig, axes
+
+def sequential_pca_cross_type(full_df, grouped_feats=None):
+    if grouped_feats is None:
+        grouped_feats = padel_categorization.get_desc_vals().groupby(by='Type', axis='rows', dropna=False)
+    for a, b in itertools.combinations(grouped_feats, 2):
+        print(full_df[a[1]]['Type'], full_df[b[1]['Type']])
+        print(a, b)
+        combo: pd.DataFrame = full_df.loc[a[1]['Type']].concat(full_df.loc[b[1]['Type']])
+        combo.dropna(axis=1, inplace=True, copy=True)
+        print('Dropped compounds for NAs:\n', combo.index.difference(grouped_feats[a][0].index.union(grouped_feats[b][0].index)))
+        pca = PCA(n_components=2, whiten=True, random_state=0)
+        kpca = KernelPCA(n_components=2, random_state=0, n_jobs=-1)
+        fpca = pca.fit_transform(combo.to_numpy())
+        fkpca = kpca.fit_transform(combo.to_numpy())
+        print(fkpca.eigenvalues_, fkpca.eigenvectors_)
+        pca_vs_kpca(fpca, fkpca)
+        fig, axes = plt.subplots(ncols=2)
+        seaborn.lineplot(fpca, ax=axes[0])
+        seaborn.lineplot(fkpca, ax=axes[1])
+        plt.draw()
+        plt.show()
+
+# Calculate (mean, median, etc) for each cluster from clustering algorithm.
+def cluster_stats(feat_df, labels):
+    return
+
+
+def nan_clustering(full_df, connectivity=None, agglom=False):
+    from sklearn.manifold import SpectralEmbedding
+    na_df = full_df.isna().astype(int)
+    tdf = na_df.T.copy()
+    # na_df.corr(method='spearman')
+    top_nan = na_df.loc[na_df.sum(axis=1).sort_values(ascending=False).index]
+    #ts = Isomap(n_jobs=-1)
+    #tfit = ts.fit_transform(na_df)
+    #print(ts.kl_divergence_)
+    spec = SpectralEmbedding(n_components=5, random_state=0, n_jobs=-1)
+    fspec = spec.fit_transform(na_df)
+    fig, ax = plt.subplots()
+    sns.scatterplot(data=pd.DataFrame(fspec), alpha=0.1, palette='Reds')
+    plt.loglog()
+    plt.draw()
+    plt.show()
+    if agglom:
+        agg = AgglomerativeClustering(n_clusters=8, distance_threshold=None, metric='euclidean', linkage='ward',
+                                      compute_distances=True)
+        ag = agg.fit_predict(top_nan[:50].to_numpy())
+        labels, counts = np.unique(agg.labels_, return_counts=True)
+        print(agg.distances_)
+        print(sorted(counts))
+        label_counts = zip(labels, counts)
+        feat_labels = pd.Series(data=agg.labels_, index=top_nan[:50].index)
+        sort_ind = feat_labels.argsort(axis='index')
+        plotting_tools.plot_dendrogram(agg, truncate_mode="level", p=10)
+    # exit()
+    feat_cov = na_df[na_df.mean(axis=0).sort_values(ascending=False).index[:100]].corr(method='spearman')
+    top_feats = feat_cov.quantile(q=0.9, axis=0).sort_values(ascending=True).index
+    feat_maxcov = feat_cov.loc[top_feats[:50], top_feats[:50]]
+    feat_mincov = feat_cov.loc[top_feats[-50:], top_feats[-50:]]
+    # feat_mincov = feat_cov[feat_cov.quantile(q=0.1, axis=0).sort_values(ascending=True).index].iloc[0:50, 0:50]
+    print(feat_cov.shape)
+    fig, ax = plt.subplots(ncols=2)
+    sns.heatmap(data=feat_maxcov, square=True, xticklabels=[], yticklabels=feat_maxcov.columns.tolist(), vmin=-1,
+                vmax=1, ax=ax[0], cmap='vlag', cbar_ax=ax[0])
+    sns.heatmap(data=feat_mincov, square=True, xticklabels=[], yticklabels=feat_mincov.columns.tolist(), vmin=-1,
+                vmax=1, ax=ax[1], cmap='vlag')
+    fig.set_layout_engine('constrained')
+    plt.draw()
+    plt.show()
+
+    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.75, metric='l2', linkage='average',
+                                  compute_distances=True, compute_full_tree=True)
+    ag = agg.fit_predict(tdf)
+    labels, counts = np.unique(agg.labels_, return_counts=True)
+    print(sorted(counts))
+    label_counts = zip(labels, counts)
+    feat_labels = pd.Series(data=agg.labels_, index=tdf.index)
+    sort_ind = feat_labels.argsort(axis='index')
+    plotting_tools.plot_dendrogram(agg, truncate_mode="level", p=5)
+    for i in labels:
+        print('\n')
+        print(feat_labels[feat_labels == i].index.tolist(), sep='\n')
+    exit()
+
+    # labels = agg.labels_
+    # wt = ward_tree(X=full_df)
+    # print('Ward tree: ', wt)
+    hdb = HDBSCAN(min_cluster_size=2, n_jobs=-1, cluster_selection_method='leaf', store_centers=None)
+    hdb.fit_predict(na_df)
+    labels = hdb.labels_
+    plotting_tools.plot(na_df, labels=labels, ground_truth=False)
+    plt.show()
+
+
+def normalize_atom_count(data_df: pd.DataFrame):
+    if 'nHeavyAtoms' in data_df.columns.tolist():
+        name_type = 'short'
+        atom_desc_names = ["nAromAtom", "nH", "nB", "nC", "nN", "nO", "nS", "nP", "nF", "nCl", "nBr", "nI", "nX"]
+        norm = 'nHeavyAtom'
+        bond_desc_names = ["nBonds2", "nBondsS", "nBondsS2", "nBondsS3", "nBondsD", "nBondsD2", "nBondsT", "nBondsQ",
+                           "nBondsM"]
+        bond_norm = "nBonds"
+    elif 'Number of heavy atoms (i.e. not hydrogen)' in data_df.columns.tolist():
+        name_type = 'full'
+        atom_desc_names = ["Number of aromatic atoms", "Number of hydrogen atoms", "Number of boron atoms",
+                           "Number of carbon atoms", "Number of nitrogen atoms", "Number of oxygen atoms",
+                           "Number of sulphur atoms", "Number of phosphorus atoms", "Number of fluorine atoms",
+                           "Number of chlorine atoms", "Number of bromine atoms", "Number of iodine atoms",
+                           "Number of halogen atoms"]
+        norm = 'Number of heavy atoms (i.e. not hydrogen)'
+    else:
+        name_type = 'index'
+        if data_df.shape[1] == 1444:
+            norm = 8
+            atom_desc_names = [5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
+        else:
+            raise IndexError
+    for desc_name in atom_desc_names:
+        data_df[desc_name] = data_df[desc_name] / data_df[norm]
Index: dmso_model_dev/get_estimators.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/get_estimators.py b/dmso_model_dev/get_estimators.py
new file mode 100644
--- /dev/null	(date 1718227210873)
+++ b/dmso_model_dev/get_estimators.py	(date 1718227210873)
@@ -0,0 +1,86 @@
+from functools import partial
+
+import pandas as pd
+
+import pipeline_one
+
+def run_eight_ada_rf():
+    whole_data, whole_labels, discrete_data, cont_data = pipeline_one.retrieve_data()
+    disc_cols = discrete_data.columns.tolist()
+    cont_cols = cont_data.columns.tolist()
+    from sklearn.pipeline import Pipeline
+    from sklearn.feature_selection import VarianceThreshold
+    disc_trans = Pipeline(steps=[('d_vary', VarianceThreshold(0.05))])
+    # ('d_univar', GenericUnivariateSelect(
+    #    score_func=partial(mutual_info_classif, discrete_features=True, n_neighbors=3), param=75,
+    #    mode='k_best'))])
+    cont_trans = Pipeline(steps=[  # ('c_scaler', RobustScaler(with_centering=False)),
+        ('c_vary', VarianceThreshold(threshold=0.05))])
+    # ('c_univar', GenericUnivariateSelect(partial(mutual_info_classif, discrete_features=False, n_neighbors=3),
+    #    param=150, mode='k_best'))])
+    from sklearn.compose import ColumnTransformer
+    col_trans = ColumnTransformer(transformers=[
+        ('cont_pipeline', cont_trans, cont_cols),
+        ('disc_pipeline', disc_trans, disc_cols)])
+    from imblearn.ensemble import BalancedRandomForestClassifier
+    entropy = BalancedRandomForestClassifier(bootstrap=True,
+                                             criterion='entropy',
+                                             n_estimators=500,
+                                             replacement=False,
+                                             sampling_strategy='majority', n_jobs=-1)
+    from sklearn.ensemble import AdaBoostClassifier
+    clf_pipe = Pipeline(steps=[('col_trans', col_trans),
+                               # ('ipca', IncrementalPCA()),
+                               ('model',
+                                (AdaBoostClassifier(estimator=entropy, algorithm='SAMME', learning_rate=5)
+                                 ))])
+    return clf_pipe
+
+
+def run_seven_gaussian_mixture():
+    whole_data, whole_labels, discrete_data, cont_data = pipeline_one.retrieve_data()
+    disc_cols = discrete_data.columns.tolist()
+    cont_cols = cont_data.columns.tolist()
+    from sklearn.compose import ColumnTransformer
+    import functools
+    from sklearn.feature_selection import mutual_info_classif
+    from sklearn.feature_selection import GenericUnivariateSelect
+    from sklearn.feature_selection import VarianceThreshold
+    from sklearn.preprocessing import RobustScaler
+
+    whole_data, whole_labels, discrete_data, cont_data = pipeline_one.retrieve_data()
+    disc_cols = discrete_data.columns.tolist()
+    cont_cols = cont_data.columns.tolist()
+    '''
+    col_trans = Pipeline(steps=[['col_trans',
+                                 ColumnTransformer(transformers=[('cont_pipeline',
+                                                                  Pipeline(steps=[('c_scaler',
+                                                                                   RobustScaler()),
+                                                                                  ('c_vary',
+                                                                                   VarianceThreshold(threshold=0.05)),
+                                                                                  ('c_univar',
+                                                                                   GenericUnivariateSelect(
+                                                                                       mode='k_best',
+                                                                                       param=35,
+                                                                                       score_func=functools.partial(
+                                                                                           mutual_info_classif,
+                                                                                           discrete_features=False,
+                                                                                           n_neighbors=3)))])
+                                                                  disc_trans = Pipeline(steps=[
+                                                                  ('d_vary', VarianceThreshold(0.05)),
+                                 ('d_univar', GenericUnivariateSelect(
+                                     score_func=partial(mutual_info_classif, discrete_features=True, n_neighbors=3),
+                                     param=75,
+                                     mode='k_best'))]
+    Pipeline(steps=[('col_trans', col_trans),
+    col_trans = ColumnTransformer(transformers=[
+        ('cont_pipeline', cont_trans, cont_cols),
+        ('disc_pipeline', disc_trans, disc_cols)])
+    ('ipca', IncrementalPCA(n_components=25)),
+    ('model',
+     GaussianMixture(covariance_type='diag',
+                     init_params='k-means++', max_iter=1000,
+                     n_components=2, n_init=6, random_state=0,
+                     warm_start=True))
+                     )'''
+    return
Index: dmso_model_dev/FrameWeight.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/FrameWeight.py b/dmso_model_dev/FrameWeight.py
new file mode 100644
--- /dev/null	(date 1717444857657)
+++ b/dmso_model_dev/FrameWeight.py	(date 1717444857657)
@@ -0,0 +1,22 @@
+import pandas as pd
+
+
+class FrameWeight:
+
+    def __init__(self, weights, row_wts=None, col_wts=None):
+        self.weights = weights
+        self.row_wts = row_wts
+        self.col_wts = col_wts
+
+    def reweight(self, data, index=None):
+        if index is not None and index in self.weights.index:
+            data = pd.Series(data)
+            return data.multiply(self.weights[index])
+        elif data is pd.Series:
+            data.multiply(self.weights[data.index])
+
+    def rw_row(self, data):
+        return data.multiply(self.row_wts[[c for c in data.columns if c in self.row_wts.index]])
+
+    def rw_col(self, data):
+        return data.multiply([c for c in data.index if c in self.col_wts.index])
Index: dmso_model_dev/feature_melter.pyi
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/feature_melter.pyi b/dmso_model_dev/feature_melter.pyi
new file mode 100644
--- /dev/null	(date 1717444857405)
+++ b/dmso_model_dev/feature_melter.pyi	(date 1717444857405)
@@ -0,0 +1,90 @@
+import os.path
+import re
+
+import yaml
+
+smiles_path = "//enamine_data/qsared/"
+
+smiles = list()
+jsmiles = list()
+for j in list(range(17)):
+    i = j + 2
+    temp_file = '{}temp_smiles_{}.txt'.format(smiles_path, i)
+    if os.path.isfile(temp_file):
+        with open(temp_file, 'r') as tf:
+            s = tf.readlines()[0]
+        # print(temps[0].replace("'", '"'))
+        #dics = [x.strip(", 'mol").strip("{") for x in re.findall(pattern="{*, 'mol", string=s)]
+        # print([x.string.split(sep=',') for x in re.finditer(pattern="{*}", string=s)])
+        raw = [x.groups()[0] for x in re.finditer(pattern="\[{(.*)}]", string=s)]
+        print(raw)
+        dub = [w.groups() for w in [re.finditer(" '(*)'", string=x) for x in raw]]
+        print([v for v in dub])
+        #print([w.split("}]") for w in [x.split(", [{") for x in raw]])
+        exit()
+        print([ast.literal_eval(x.string) for x in re.finditer(pattern="{*}", string=s)])
+        # dics = [dict([a for a in x.string.split(sep=',')]) for x in re.finditer(pattern="{*}", string=s)]
+        print(dics)
+        # sdict = s[s.find("{")+1:s.find(", 'mol'")]
+        sics = list()
+        for x in re.finditer(pattern="]*[", string=s):
+            if x == "" or not x:
+                break
+            else:
+                sics.append(x.string.removeprefix("]").removesuffix(", [{"))
+        # sics = [s[g+1: n] for g, n in zip(seg, sen)]
+        # smiles = s[s.find("]")+1:s.find(", [{")]
+        print(dics[0])
+        print(sics)
+        print(pd.DataFrame.from_dict(data=dics))
+        # smiles = dict(ast.literal_eval(s[0]))
+        # print(smiles)
+        # yaml.full_load(temps[0].replace("'", '"'))
+        # json.loads(s=temps[0].replace("'", '"'))
+print(smiles.keys())
+temp_smiles = yaml.full_load(s[0])
+print(temp_smiles)
+temp_smiles.to_pickle('{}reqsar_originals_df.pkl'.format(smiles_path), protocol=3)
+'''
+if open(temp_file, 'r').readable():
+    with open(temp_file, 'r') as tf:
+        temp_smiles = tf.readlines()
+else:
+    with open(temp_file, 'r', encoding='utf-8') as tf:
+        temp_smiles = tf.readlines()
+for j in list(range(17)):
+    i = j + 2
+    temp_file = '{}temp_smiles_{}.txt'.format(smiles_path, i)    
+    start = 0
+    while True:
+        if os.path.isfile(temp_file):
+            if open(temp_file, 'r').readable():
+                with open(temp_file, 'r') as tf:
+                    temp_smiles = tf.readlines()
+            else:
+                with open(temp_file, 'r', encoding='utf-8') as tf:
+                    temp_smiles = tf.readlines()
+            start = temp_smiles.count('id')
+            smiles = pd.read_csv('{}non_qsar_originals{}.csv'.format(smiles_path, i), usecols=[1]).squeeze().tolist()[
+                     start:]
+            if smiles[-1] in temp_smiles[-1:]:
+                print('Smiles complete for {}'.format(i))
+                break
+        print('Starting at compound # {} for \n{}'.format(start + 1, i))
+        break
+        try:
+            new_dict = qsar_readiness.qsar_standardizer_api(smiles, temp_file='{}temp_smiles_{}.txt'.format(smiles_path, i), complete=True, connect_timeout=60, response_timeout=600)
+            break
+        except ConnectionError:
+            print('Connection reset')
+'''
+[{'id': 'DTXSID00160431',
+'cid': 'DTXCID7082922',
+'sid': 'DTXSID00160431',
+'casrn': '137988-05-7',
+'name': 'Benzoic acid, 4-((7-hydroxy-4-oxo-4H-1-benzopyran-3-yl)oxy)-, methyl ester',
+'smiles': 'COC(C1C=CC(OC2C(=O)C3C=CC(O)=CC=3OC=2)=CC=1)=O',
+'canonicalSmiles': 'COC(=O)C1C=CC(=CC=1)OC1=COC2C=C(O)C=CC=2C1=O',
+'inchi': 'InChI=1/C17H12O6/c1-21-17(20)10-2-5-12(6-3-10)23-15-9-22-14-8-11(18)4-7-13(14)16(15)19/h2-9,18H,1H3',
+'inchiKey': 'AOEGZILUCAJRCF-UHFFFAOYNA-N',
+'mol': '\n  -INDIGO-04102423292D\n\n  0  0  0  0  0  0  0  0  0  0  0 V3000\nM  V30 BEGIN CTAB\nM  V30 COUNTS 23 25 0 0 0\nM  V30 BEGIN ATOM\nM  V30 1 C 0.0 0.0 0.0 0\nM  V30 2 O 0.0 0.0 0.0 0\nM  V30 3 C 0.0 0.0 0.0 0\nM  V30 4 O 0.0 0.0 0.0 0\nM  V30 5 C 0.0 0.0 0.0 0\nM  V30 6 C 0.0 0.0 0.0 0\nM  V30 7 C 0.0 0.0 0.0 0\nM  V30 8 C 0.0 0.0 0.0 0\nM  V30 9 O 0.0 0.0 0.0 0\nM  V30 10 C 0.0 0.0 0.0 0\nM  V30 11 C 0.0 0.0 0.0 0\nM  V30 12 O 0.0 0.0 0.0 0\nM  V30 13 C 0.0 0.0 0.0 0\nM  V30 14 C 0.0 0.0 0.0 0\nM  V30 15 C 0.0 0.0 0.0 0\nM  V30 16 O 0.0 0.0 0.0 0\nM  V30 17 C 0.0 0.0 0.0 0\nM  V30 18 C 0.0 0.0 0.0 0\nM  V30 19 C 0.0 0.0 0.0 0\nM  V30 20 C 0.0 0.0 0.0 0\nM  V30 21 O 0.0 0.0 0.0 0\nM  V30 22 C 0.0 0.0 0.0 0\nM  V30 23 C 0.0 0.0 0.0 0\nM  V30 END ATOM\nM  V30 BEGIN BOND\nM  V30 1 1 1 2\nM  V30 2 1 2 3\nM  V30 3 2 3 4\nM  V30 4 1 3 5\nM  V30 5 2 5 6\nM  V30 6 1 6 7\nM  V30 7 2 7 8\nM  V30 8 1 8 9\nM  V30 9 1 9 10\nM  V30 10 2 10 11\nM  V30 11 1 11 12\nM  V30 12 1 12 13\nM  V30 13 2 13 19\nM  V30 14 1 13 14\nM  V30 15 2 14 15\nM  V30 16 1 15 16\nM  V30 17 1 15 17\nM  V30 18 2 17 18\nM  V30 19 1 18 19\nM  V30 20 1 19 20\nM  V30 21 1 20 10\nM  V30 22 2 20 21\nM  V30 23 1 8 22\nM  V30 24 2 22 23\nM  V30 25 1 23 5\nM  V30 END BOND\nM  V30 END CTAB\nM  END\n'}]
\ No newline at end of file
Index: dmso_model_dev/shake.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/shake.py b/dmso_model_dev/shake.py
new file mode 100644
--- /dev/null	(date 1713191571994)
+++ b/dmso_model_dev/shake.py	(date 1713191571994)
@@ -0,0 +1,40 @@
+import ctypes
+import ctypes.wintypes
+import random
+import time
+
+mover = 0x0001
+
+
+class MouseInput(ctypes.Structure):
+    _fields_ = [
+        ('dx', ctypes.wintypes.LONG),
+        ('dy', ctypes.wintypes.LONG),
+        ('mouseData', ctypes.wintypes.DWORD),
+        ('dwFlags', ctypes.wintypes.DWORD),
+        ('time', ctypes.wintypes.DWORD),
+        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),
+    ]
+
+
+def _size():
+    return ctypes.windll.user32.GetSystemMetrics(0), ctypes.windll.user32.GetSystemMetrics(1)
+
+
+def _moveTo(x, y):
+    ctypes.windll.user32.SetCursorPos(x, y)
+
+
+def send_event(ev, x, y, dwData=0):
+    width, height = _size()
+    convertedX = 65536 * x // width + 1
+    convertedY = 65536 * y // height + 1
+    ctypes.windll.user32.mouse_event(ev, ctypes.c_long(convertedX), ctypes.c_long(convertedY), dwData, 0)
+
+
+def shake_it(radius_px, wait_secs=600):
+    time.sleep(wait_secs)
+    x = random.gauss(mu=radius_px, sigma=radius_px/2.)
+    y = random.gauss(mu=radius_px, sigma=radius_px/2.)
+    _moveTo(x, y)
+    return
\ No newline at end of file
Index: dmso_model_dev/pipeline_rescue.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/pipeline_rescue.py b/dmso_model_dev/pipeline_rescue.py
new file mode 100644
--- /dev/null	(date 1718308098241)
+++ b/dmso_model_dev/pipeline_rescue.py	(date 1718308098241)
@@ -0,0 +1,362 @@
+import glob
+import warnings
+
+import joblib
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import qsar_readiness
+from imblearn.ensemble import BalancedRandomForestClassifier
+from imblearn.metrics import sensitivity_score, sensitivity_specificity_support, specificity_score
+from sklearn.experimental import enable_halving_search_cv  # noqa
+from sklearn.feature_selection import VarianceThreshold
+from sklearn.metrics import balanced_accuracy_score, confusion_matrix
+from sklearn.model_selection import LearningCurveDisplay, StratifiedKFold
+from sklearn.tree import DecisionTreeClassifier
+from sklearn.utils import compute_sample_weight
+
+from dmso_model_dev.data_handling import padel_categorization
+import pipeline_one
+from pipeline_one import balanced_scorer
+
+
+def pipe_settings():
+    PROJECT_PATH = "//"
+    np.set_printoptions(precision=3, floatmode='maxprec')
+    pd.set_option('display.precision', 4)
+    pd.set_option('display.max_rows', None)
+    pd.set_option('display.max_columns', None)
+    pd.set_option('display.width', 1000)
+    pd.set_option('display.colheader_justify', 'center')
+    mem = joblib.Memory(location="C:/Users/mmanning/PycharmProjects/data/joblib_cache")
+    warnings.simplefilter(action='ignore', category=FutureWarning)
+    warnings.simplefilter(action='ignore', category=UserWarning)
+    return mem, PROJECT_PATH
+
+
+def load_pipes(pipepath):
+    for pick in glob.glob('{}*/*.pkl'.format(pipepath)):
+        pkl_run = joblib.load(pick)
+        print('\n\n\nPickle Type')
+        print(type(pkl_run))
+
+
+def learning_curve():
+    clf_pipe = joblib.load('{}enamine_data/pipeone_run1.joblib'.format(PROJECT_PATH)).best_estimator_
+    print(clf_pipe)
+    from sklearn.model_selection import StratifiedShuffleSplit
+    common_params = {
+        "train_sizes": np.linspace(0.1, 1.0, 5),
+        "cv": StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0),
+        "scoring": balanced_scorer,
+        "score_type": "both",
+        "n_jobs": 6,
+        "line_kw": {"marker": "o"},
+        "std_display_style": "fill_between",
+        "score_name": "Balanced Accuracy",
+        "error_score": "raise"
+    }
+    fig, ax = plt.subplots()
+    common_params['X'] = whole_data
+    common_params['y'] = whole_labels
+    LearningCurveDisplay.from_estimator(clf_pipe, **common_params, ax=ax)
+    handles, label = ax.get_legend_handles_labels()
+    ax.legend(handles[:2], ["Training Score", "SaltsAndIntermetallicsTest Score"])
+    ax.set_title(f"Learning Curve for Run 4: IPCA B-RF (unboosted)")
+    plt.show()
+
+
+whole_data, whole_labels, discrete_data, cont_data = pipeline_one.retrieve_data(all_feats=True)
+disc_cols = discrete_data.columns.tolist()
+cont_cols = cont_data.columns.tolist()
+
+# test_pipe = BalancedRandomForestClassifier(n_estimators=5000, bootstrap=True, sampling_strategy='all', replacement=False, oob_score=True, n_jobs=7, random_state=0, verbose=1, max_samples=5000, class_weight='balanced_subsample')
+# model_name = 'Majority-Bootstrapped Ada-BRF'
+# run_name = 'run8'
+'''
+clf_pipe = joblib.load('{}enamine_data/pipeone_run8.joblib'.format(PROJECT_PATH)).best_estimator_
+sklearn.utils.estimator_html_repr(clf_pipe)
+clf_list = list()
+train_list, test_list = list(), list()
+ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval, sss_dev, sss_eval = list(), list(), list(), list(), list(), list(), list(), list()
+for train, test in StratifiedKFold(n_splits=5).split(X=whole_data, y=whole_labels):
+    train_list.append(train)
+    test_list.append(test)
+    trimmed_data, trimmed_labels = whole_data.iloc[train], whole_labels[train]
+    test_data, test_labels = whole_data.iloc[test], whole_labels[test]
+    trimmed_pipe = clf_pipe.fit(trimmed_data, y=trimmed_labels)
+    clf_list.append(trimmed_pipe)
+    trimmed_sample_wt = compute_sample_weight(class_weight='balanced', y=trimmed_labels)
+    test_sample_wt = compute_sample_weight(class_weight='balanced', y=test_labels)
+    test_pred = clf_pipe.predict(test_data)
+    trimmed_pred = clf_pipe.predict(trimmed_data)
+    ba_eval.append(balanced_accuracy_score(y_true=test_labels, y_pred=test_pred))
+    ba_dev.append(balanced_accuracy_score(y_true=trimmed_labels, y_pred=trimmed_pred))
+    sen_eval.append(sensitivity_score(y_true=test_labels, y_pred=test_pred, average='weighted'))
+    sen_dev.append(sensitivity_score(y_true=trimmed_labels, y_pred=trimmed_pred, average='weighted'))
+    spec_eval.append(specificity_score(y_true=test_labels, y_pred=test_pred, average='weighted'))
+    spec_dev.append(specificity_score(y_true=trimmed_labels, y_pred=trimmed_pred, average='weighted'))
+    sss_eval.append(sensitivity_specificity_support(y_true=test_labels, y_pred=test_pred, average='weighted'))
+    sss_dev.append(sensitivity_specificity_support(y_true=trimmed_labels, y_pred=trimmed_pred, average='weighted'))
+    # print(ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval, sss_dev, sss_eval)
+score_names = 'Eval Balanced Accuracy Score', 'Dev Balanced Accuracy Score', 'Eval Sensitivity Score', 'Dev Sensitivity Score', 'Eval Specificity Score', 'Dev Specificity Score', 'Eval S-S Support (Weighted)', 'Dev S-S Support (Weighted)',
+columns = pd.MultiIndex.from_product([['Balanced Accuracy', 'Sensitivity', 'Specificity',], ['Dev', 'Eval']], names=['Score', 'Data Set'])
+rows = pd.Index(['CV Run #{}'.format(i+1) for i in list(range(len(ba_dev)))])
+try:
+    score_df = pd.DataFrame(data=[ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval], columns=columns, index=rows)
+except:
+    score_df = pd.DataFrame(data=[ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval], columns=rows, index=columns).T
+support_df = pd.DataFrame(sss_dev, sss_eval)
+score_df = pd.concat([score_df, score_df.mean(axis=0).rename(index='Mean'), score_df.std(axis=1).rename(index='StDev')], axis=0)
+#score_df.pipe(pipeline_plots.highlighter)
+score_df.to_pickle('{}enamine_data/pipeline_score_report_{}.pkl'.format(PROJECT_PATH, run_name))
+print('Dev Set: {}\t\tEval Set: {}'.format(np.unique(whole_labels[test_list[0]][1].tolist(), return_counts=True), np.unique(whole_labels[train_list[0]][1].tolist(), return_counts=True)))
+print(score_df.to_string())
+#scht = score_df.to_html()
+confused_test, confused_train = list(), list()
+for clf, test, train in zip(clf_list, test_list, train_list):
+    confused_test.append(confusion_matrix(whole_labels[test], clf.predict(whole_data.iloc[test])))
+    confused_train.append(confusion_matrix(whole_labels[train], clf.predict(whole_data.iloc[train])))
+ctest = np.stack(confused_test)
+ctrain = np.stack(confused_train)
+cat_axis = 0
+ctestmean = np.mean(ctest, axis=cat_axis)
+cteststd = np.std(ctest, axis=cat_axis)
+ctrainmean = np.mean(ctrain, axis=cat_axis)
+ctrainstd = np.std(ctrain, axis=cat_axis)
+print(ctestmean, cteststd)
+test_df = pd.DataFrame(data=ctestmean, index=['Insoluble', 'Soluble'], columns=['Soluble', 'Insoluble'])
+test_std = pd.DataFrame(data=cteststd, index=['Insoluble', 'Soluble'], columns=['Soluble', 'Insoluble'])
+test_df_styled = pipeline_plots.plot_con_matrix_cv(test_df, df_std=cteststd, model_name=model_name)
+train_df = pd.DataFrame(data=cteststd, index=['Insoluble', 'Soluble'], columns=['Soluble', 'Insoluble'])
+train_std= pd.DataFrame(data=ctrainmean, index=['Insoluble', 'Soluble'], columns=['Soluble', 'Insoluble'])
+train_df_styled = pipeline_plots.plot_con_matrix_cv(train_df, df_std=ctrainstd, model_name=model_name)
+with open('{}enamine_data/pipeline_score_report_{}.csv'.format(PROJECT_PATH, run_name), 'a') as sf:
+    sf.write('{} {}'.format(model_name, run_name))
+    sf.write(score_df.to_string())
+    # sf.write('\n'.join(['Dev Set: {}'.format(np.unique(whole_labels[test_list[ti]], return_counts=True)[1].tolist()) for ti in train_list]))
+    # sf.write('\n'.join(['Dev Confusion Matrices:\n'.format(i) for i in confused_test]))
+    sf.write('Dev-Eval CV Split Labels: {}'.format('\t'.join(['{}-{}'.format(np.unique(whole_labels[ti], return_counts=True)[1].tolist(), np.unique(whole_labels[ti], return_counts=True)[1].tolist()) for ti, te in zip(train_list, test_list)])))
+    sf.write('Dev Confusion Matrices Mean: {}\nDev Confusion Matrices St Dev:\n{}'.format(train_df,  train_std))
+    sf.write('Eval Confusion Matrices Mean: {}\nEval Confusion Matrices St Dev:\n{}'.format(test_df,  test_std))
+    # sf.write('\n'.join(['Eval Confusion Matrices:\n'.format(i) for i in confused_train]))
+    # Debug stats here.
+    # print('Eval Confusion Matrices Mean:\n', test_df, '\n', 'Eval Confusion Matrices St Dev:\n',  test_std)
+import plotly
+# Balanced RF Model
+'''
+
+
+def prune_loop(run_name, model_name, start=0., stop=0.1, step=0.01):
+    score_list = list()
+    for i, alpha in enumerate(np.arange(start, stop, step).tolist()):
+        run = '{}_prune{}'.format(run_name, alpha)
+        model = '{}_prune{}'.format(model_name, alpha)
+        data = nan_data()
+        clfs, score_df = brf(data, run, model, alpha)
+        score_list.append(score_df)
+        print(model)
+        print(score_df)
+
+
+def nan_data(feature_kws=None, whole_tuple=None):
+    if feature_kws is None:
+        feature_kws = ['BCUT', 'HBond', 'Shape', 'hydrogen bond', 'rotatable', 'Count',
+                       'ExtendedTopochemicalAtomDescriptor']
+    if type(whole_tuple) is not iter and type(whole_tuple[0]) is not pd.DataFrame and type(whole_tuple[1]) is not pd.Series:
+        whole_data, whole_labels = pipeline_one.retrieve_data(all_feats=True)
+    else:
+        whole_data, whole_labels = whole_tuple
+    kw_columns, nonkw_columns = padel_categorization.get_padel_containing_indices(keywords=feature_kws, df=whole_data)
+    whole_smiles, small_smiles = qsar_readiness.filter_by_size(whole_data.index.copy().tolist())
+    whole_data = whole_data.drop(index=[c for c in whole_data.index.tolist() if c not in whole_smiles])
+    whole_labels = whole_labels.squeeze().loc[whole_smiles]
+    whole_data, _ = padel_categorization.get_padel_containing_indices(keywords=feature_kws, df=whole_data)
+    whole_data = VarianceThreshold().fit_transform(whole_data)
+    return [whole_data, whole_labels, kw_columns, nonkw_columns]
+
+
+def neighbors(X, y):
+    from sklearn.cluster import FeatureAgglomeration
+    nn = FeatureAgglomeration().fit_transform(X, y)
+    groups = nn.classes_
+    return groups
+
+
+def decision_tree(X, y, sample_weight=None):
+    dtc = DecisionTreeClassifier(max_features='log2', random_state=0, max_depth=4)
+    DecisionTreeClassifier.fit(X, y, sample_weight=sample_weight)
+
+
+def brf(data_set_list, run_name, model_name, ccp_alpha=0, sep_list=None):
+    ind_list, clf_list = list(), list()
+    whole_data, whole_labels = data_set_list
+    mem, PROJECT_PATH = pipe_settings()
+    ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval, sss_dev, sss_eval = list(), list(), list(), list(), list(), list(), list(), list()
+    for train, test in StratifiedKFold(n_splits=5).split(X=whole_data, y=whole_labels):
+        ind_list.append([train, test])
+        trimmed_data, trimmed_labels = whole_data[train], whole_labels[train]
+        test_data, test_labels = whole_data[test], whole_labels[test]
+        # whole_data, whole_labels, discrete_data, cont_data = pipeline_one.retrieve_data()
+        # trimmed_data, trimmed_labels, test_data, test_labels = data_set[0], data_set[1], data_set[2], data_set[3]
+        clf_pipe = BalancedRandomForestClassifier(n_estimators=10000, max_samples=2500, bootstrap=True,
+                                                  sampling_strategy='all', class_weight='balanced_subsample',
+                                                  ccp_alpha=ccp_alpha, oob_score=True, max_features='log2',
+                                                  replacement=False, n_jobs=-1, random_state=0, verbose=5)
+        trimmed_pipe = clf_pipe.fit(trimmed_data, y=trimmed_labels)
+        clf_list.append(clf_pipe)
+        trimmed_sample_wt = compute_sample_weight(class_weight='balanced', y=trimmed_labels)
+        test_sample_wt = compute_sample_weight(class_weight='balanced', y=test_labels)
+        test_pred = clf_pipe.predict(test_data)
+        trimmed_pred = clf_pipe.predict(trimmed_data)
+        ba_eval.append(balanced_accuracy_score(y_true=test_labels, y_pred=test_pred))
+        ba_dev.append(balanced_accuracy_score(y_true=trimmed_labels, y_pred=trimmed_pred))
+        sen_eval.append(sensitivity_score(y_true=test_labels, y_pred=test_pred, average='weighted'))
+        sen_dev.append(sensitivity_score(y_true=trimmed_labels, y_pred=trimmed_pred, average='weighted'))
+        spec_eval.append(specificity_score(y_true=test_labels, y_pred=test_pred, average='weighted'))
+        spec_dev.append(specificity_score(y_true=trimmed_labels, y_pred=trimmed_pred, average='weighted'))
+        sss_eval.append(sensitivity_specificity_support(y_true=test_labels, y_pred=test_pred, labels=test_labels,
+                                                        average='weighted'))
+        sss_dev.append(
+            sensitivity_specificity_support(y_true=trimmed_labels, y_pred=trimmed_pred, labels=trimmed_labels,
+                                            average='weighted'))
+        # print(ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval, sss_dev, sss_eval)
+    joblib.dump(clf_list, '{}{}'.format(PROJECT_PATH, 'enamine_data/pipeone_{}.joblib'.format(run_name), protocol=5))
+    score_names = 'Eval Balanced Accuracy Score', 'Dev Balanced Accuracy Score', 'Eval Sensitivity Score', 'Dev Sensitivity Score', 'Eval Specificity Score', 'Dev Specificity Score', 'Eval S-S Support (Weighted)', 'Dev S-S Support (Weighted)',
+    columns = pd.MultiIndex.from_product([['Balanced Accuracy', 'Sensitivity', 'Specificity', ], ['Dev', 'Eval']],
+                                         names=['Score', 'Data Set'])
+    rows = pd.Index(['CV Run #{}'.format(i + 1) for i in list(range(len(ba_dev)))])
+    try:
+        score_df = pd.DataFrame(data=[ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval], columns=columns,
+                                index=rows)
+    except:
+        score_df = pd.DataFrame(data=[ba_dev, ba_eval, sen_dev, sen_eval, spec_dev, spec_eval], columns=rows,
+                                index=columns).T
+    support_df = pd.DataFrame(sss_dev, sss_eval)
+    print('Support Score: \n{}'.format(support_df))
+    try:
+        score_df = pd.concat([score_df, score_df.mean(axis='rows').rename(index='Mean').T,
+                              score_df.std(axis='rows').rename(index='StDev').T], axis=1)
+        assert score_df.shape[1] == 6
+    except:
+        score_df = score_df
+    # score_df.pipe(pipeline_plots.highlighter)
+    score_df.to_pickle('{}enamine_data/pipeline_score_report_{}.pkl'.format(PROJECT_PATH, run_name))
+    print(score_df.to_string(col_space=5))
+    # scht = score_df.to_html()
+    confused_test, confused_train = list(), list()
+    for clf, ind in zip(clf_list, ind_list):
+        whole_data, whole_labels = data_set_list
+        train, test = ind
+        train_data, test_data, train_labels, test_labels = whole_data[train], whole_data[test], whole_labels[
+            train], whole_labels[test]
+        confused_test.append(confusion_matrix(test_labels, clf.predict(test_data)))
+        confused_train.append(confusion_matrix(train_labels, clf.predict(train_data)))
+    ctest = np.dstack(confused_test)
+    ctrain = np.dstack(confused_train)
+    cat_axis = np.argmax(ctest.shape)
+    ctestmean = np.mean(ctest, axis=cat_axis)
+    cteststd = np.std(ctest, axis=cat_axis)
+    ctrainmean = np.mean(ctrain, axis=cat_axis)
+    ctrainstd = np.std(ctrain, axis=cat_axis)
+    test_df = pd.DataFrame(data=ctestmean, index=['Soluble', 'Insoluble'].reverse(),
+                           columns=['Soluble', 'Insoluble'].reverse())
+    test_std = pd.DataFrame(data=cteststd, index=['Soluble', 'Insoluble'].reverse(),
+                            columns=['Soluble', 'Insoluble'].reverse())
+    # test_df_styled = pipeline_plots.plot_con_matrix_cv(test_df, df_std=cteststd, model_name=model_name)
+    train_df = pd.DataFrame(data=ctrainmean, index=['Soluble', 'Insoluble'].reverse(),
+                            columns=['Soluble', 'Insoluble'].reverse())
+    train_std = pd.DataFrame(data=ctrainstd, index=['Soluble', 'Insoluble'].reverse(),
+                             columns=['Soluble', 'Insoluble'].reverse())
+    # train_df_styled = pipeline_plots.plot_con_matrix_cv(train_df, df_std=ctrainstd, model_name=model_name)
+    with open('{}enamine_data/pipeline_score_report_{}.csv'.format(PROJECT_PATH, run_name), 'a') as sf:
+        sf.write('{}\t{}'.format(model_name, run_name))
+        sf.write(score_df.to_string())
+        # sf.write('Dev-Eval CV Split Labels: \n{}'.format('\t'.join(['{}-{}'.format(np.unique(whole_labels[ti], return_counts=True)[1].tolist(), np.unique(whole_labels[ti], return_counts=True)[1].tolist()) for ti, te in zip(train_list, test_list)])))
+        sf.write('Dev Confusion Matrices Mean: \n{}\nDev Confusion Matrices St Dev:\n{}'.format(train_df, train_std))
+        sf.write('Eval Confusion Matrices Mean: \n{}\nEval Confusion Matrices St Dev:\n{}'.format(test_df, test_std))
+    return clf_list, score_df
+    # Debug stats here.
+    # print('Eval Confusion Matrices Mean:\n', test_df, '\n', 'Eval Confusion Matrices St Dev:\n',  test_std)
+
+
+# Balanced RF Model
+run_name = 'more_features_run1'
+model_name = 'Balanced_RF_10k'
+prune_loop(run_name, model_name)
+exit()
+# PrecisionRecallDisplay.from_estimator(estimator=clf_pipe, X=test_data, y=test_labels, pos_label=clf_pipe.classes_[0])
+# PrecisionRecallDisplay(y_true=trimmed_labels, y_pred=trimmed_pred)
+# ConfusionMatrixDisplay.from_predictions(y_true=test_labels, y_pred=test_pred, labels=test_labels, sample_weight=test_sample_wt)
+# plt.title('Confusion Matrix: Eval Set for AdaBoost RF')
+# plt.show()
+# ConfusionMatrixDisplay.from_predictions(y_true=trimmed_labels, y_pred=trimmed_pred, labels=trimmed_labels)
+# plt.title('Confusion Matrix: Dev Set for AdaBoost RF')
+# plt.show()
+
+mem = joblib.Memory(location="C:/Users/mmanning/PycharmProjects/data/joblib_cache")
+pipepath = "C:/Users/mmanning/PycharmProjects/data/joblib_cache/sklearn/pipeline/_fit_transform_one/"
+# load_pipes(pipepath)
+gau, entree, gintree = pipeline_one.estimators()
+from skopt.utils import load
+
+revived = load('{}enamine_data/pipeone_run8.joblib'.format(PROJECT_PATH))
+clf = revived.best_estimator_
+set_config(display="diagram", skip_parameter_validation=True)
+print(revived.best_estimator_)
+pdd = partial_dependence(estimator=clf, X=trimmed_data, sample_weight=trimmed_sample_wt,
+                         features=trimmed_data.columns.tolist())
+joblib.dump(pdd, '{}{}'.format(PROJECT_PATH, 'enamine_data/pdd_run8.joblib', protocol=5))
+indices_list, predict_list, clf_list, cm_list, cmn_list, train_list = list(), list(), list(), list(), list(), list()
+bac, roc = list(), list()
+for train, test in StratifiedKFold(n_splits=6).split(X=trimmed_data, y=trimmed_labels):
+    indices_list.append([train, test])
+    X_train_set, y_train_set = trimmed_data.iloc[train], trimmed_labels[train]
+    X_test, y_test = trimmed_data.iloc[test], trimmed_labels[test]
+    # cvc = cross_val_predict(estimator=revived.best_estimator_, X=trimmed_data, y=trimmed_labels, groups=trimmed_labels, scoring=balanced_accuracy_score, cv=StratifiedKFold, n_jobs=-1, verbose=10, method='True')
+    # X_train_set, X_test, y_train_set, y_test = pipeline_one.split_data(trimmed_data, trimmed_labels, rstate=0)
+    # X_train_set, X_test, y_train_set, y_test = StratifiedKFold().split(X=trimmed_data, y=trimmed_labels)
+    cv_model = revived.best_estimator_.fit(X_train_set, y=y_train_set)
+    set_config(display="diagram")
+    try:
+        print(cv_model.estimator[0])
+        cv_model.estimator[0].to_html()
+    except:
+        print('.estimator call failed.')
+    clf_list.append(cv_model)
+    y_pred = revived.predict(X_test)
+    y_train_pred = revived.predict(X_train_set)
+    predict_list.append([y_test, y_pred])
+    train_list.append([X_train_set, y_train_set, y_train_pred])
+    class_weight = compute_class_weight('balanced', classes=np.unique(y_test), y=y_test)
+    sample_weight = compute_sample_weight(class_weight='balanced', y=y_test)
+    bal = balanced_accuracy_score(y_true=y_test, y_pred=y_pred, sample_weight=sample_weight)
+    bac.append(bal)
+    ros = roc_auc_score(y_true=y_test, y_score=y_pred, average='micro')
+    cm_list.append(confusion_matrix(y_true=y_test, y_pred=y_pred))
+    cmn_list.append(confusion_matrix(y_true=y_test, y_pred=y_pred, normalize=True))
+    roc.append(ros)
+print('Balanced Accuracy scores: ', bac)
+print('ROC scores: ', roc)
+bac_mean, bac_std = np.mean(bac), np.std(bac)
+roc_mean, roc_std = np.mean(roc), np.std(roc)
+con_mean, con_std = np.mean(cmn_list), np.std(cmn_list)
+print(bac_mean, bac_std, bac)
+print(roc_mean, roc_std, roc)
+print(con_mean, con_std)
+print(cmn_list)
+for true, pred in predict_list:
+    ConfusionMatrixDisplay.from_predictions(y_true=true, y_pred=pred)
+    plt.title('Confusion Matrix: Evaluation Set for #8 (AdaBoost RF)')
+    plt.show()
+for true, pred in train_list:
+    ConfusionMatrixDisplay.from_predictions(y_true=true, y_pred=pred)
+    plt.title('Confusion Matrix: Development Set for #8 (AdaBoost RF)')
+    plt.show()
+exit()
+
+'''
+fig, axs = plt.subplot_mosaic([
+        ['ca', 'cb', 'cc'],
+        ['cd', 'ce', 'cf']])
+'''
Index: dmso_model_dev/FeatureMelter.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dmso_model_dev/FeatureMelter.py b/dmso_model_dev/FeatureMelter.py
new file mode 100644
--- /dev/null	(date 1717444857521)
+++ b/dmso_model_dev/FeatureMelter.py	(date 1717444857521)
@@ -0,0 +1,99 @@
+import logging
+
+import numpy as np
+import pandas as pd
+from sklearn.model_selection import StratifiedGroupKFold
+
+'''
+class FeatureMelter(BaseEstimator, AgglomerationTransform):
+    _parameter_constraints: dict = {
+        "min_samples": [Interval(Real, 0, 1, closed="both")]
+    }
+'''
+
+
+class FeatureMelter():
+
+    def __init__(self, scoring, target=None, references=None, min_samples=None, cv=1):
+        self.cv = cv
+        self.min_samples = min_samples
+        self.scoring = scoring
+        self.target = target
+        self.references = references
+        self.score_dict = dict()
+        self.valid_indices = dict()
+        self.cv_scores = dict()
+
+    def _set_min_samples(self, df):
+        if self.min_samples < 1.:
+            abs_thresh = self.min_samples * np.round(df.shape[0])
+        elif self.min_samples >= 1:
+            abs_thresh = max(self.min_samples, 2)
+        else:
+            abs_thresh = 2
+        return abs_thresh
+
+    def valid_series(self, *args):
+        df = pd.concat(args, axis=1).dropna(axis=1, how='any')
+        return [df[f] for f in df.columns]
+
+    def valid_rows(self, df, features):
+        """
+        Helper function to avoid having NaNs for selected features.
+        :rtype: pd.DataFrame
+        :param features:
+        :return: DataFrame containing only rows without NaN values.
+        :type df: pd.DataFrame
+        """
+        return df[features].dropna(axis=1, how='any')
+
+    def _valid_indices(self, X, *args, **kwargs):
+        val = list()
+        df = X.copy()
+        valids = [self.valid_series(df[self.target], df[c]) for c in self.references]  # Get non-null rows
+
+        for v in valids:
+            if v[0].shape[0] < self._set_min_samples(X):
+                logging.warning('Too few samples in {}'.format(v[1].name))
+                valids.remove(v)
+                continue
+            for w in v:
+                if type(w) is pd.DataFrame and w.shape[1] > 1:
+                    w.drop_duplicates(inplace=True)
+                    print(w.shape)
+            if all([type(a) is pd.Series or a.shape[1] <= 1 for a in v]):
+                val.append(v)
+        # print(type(valids), valids[0])
+        # val = [v for v in valids if all([len(f.shape) == 1 for f in v])]
+        # duped = [v for v in valids if any([len(f.shape) != 1 for f in v])]
+        # [[o.drop_duplicates(inplace=True) for o in n] for n in duped]
+        # val.extend(duped)
+
+    def _fit_single(self, X, y=None, sample_weight=None, *args, **kwargs):
+        if type(X) is np.ndarray:
+            X = pd.DataFrame(data=X)
+        for name, indices in self.valid_indices.items():
+            if type(name) is int:
+                X_i = X.iloc[indices, name].to_frame()
+            else:
+                X_i = X.iloc[indices, name].to_frame()
+            self.score_dict[name] = self.scoring(X=X_i, y=self.target[indices], *args,
+                                                 **kwargs)
+        return self.score_dict
+
+    def score(self):
+        assert len(self.score_dict.items()) > 0
+        return self.score_dict
+
+    def transform(self, X, threshold_score, greater_is_better=True):
+        selected = list()
+        for name, score in self.score_dict.items():
+            if (greater_is_better and score >= threshold_score) or (not greater_is_better and score < threshold_score):
+                selected.append(name)
+        return X[:, tuple(selected)]
+
+    def fit(self, X, y, cv_method=StratifiedGroupKFold, cv=5, class_weights=None):
+        for ref in self.references:
+            self.cv_scores[ref] = list()
+            for train, test in  cv_method(n_splits=cv, ).split(X, y):
+                self.cv_scores[ref].append(self._fit_single(X[train], y[train]))
